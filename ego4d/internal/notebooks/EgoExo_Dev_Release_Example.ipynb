{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 36,128\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d56e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEASE_DIR = \"/checkpoint/miguelmartin/egoexo_data/dev/\"  # NOTE: changeme\n",
    "\n",
    "egoexo = {\n",
    "    \"takes\": os.path.join(RELEASE_DIR, \"takes.json\"),\n",
    "    \"captures\": os.path.join(RELEASE_DIR, \"captures.json\"),\n",
    "    \"physical_setting\": os.path.join(RELEASE_DIR, \"physical_setting.json\"),\n",
    "    \"participants\": os.path.join(RELEASE_DIR, \"participants.json\"),\n",
    "    \"visual_objects\": os.path.join(RELEASE_DIR, \"visual_objects.json\"),\n",
    "}\n",
    "\n",
    "\n",
    "egoexo_pd = {}\n",
    "for k, v in egoexo.items():\n",
    "    egoexo_pd[k] = pd.read_json(open(v))\n",
    "\n",
    "for k, v in egoexo.items():\n",
    "    egoexo[k] = json.load(open(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traj_takes = sum(take[\"capture\"][\"has_trajectory\"] for take in egoexo[\"takes\"])\n",
    "num_valid_traj_takes = sum(\n",
    "    take[\"capture\"][\"has_trajectory\"] and take[\"validated\"]\n",
    "    for take in egoexo[\"takes\"]\n",
    ")\n",
    "num_traj_captures = sum(capture[\"has_trajectory\"] for capture in egoexo[\"captures\"])\n",
    "num_valid_traj_captures = sum(\n",
    "    capture[\"has_trajectory\"] and (capture[\"timesync_validated\"] > 0)\n",
    "    for capture in egoexo[\"captures\"]\n",
    ")\n",
    "\n",
    "takes_df = egoexo_pd[\"takes\"]\n",
    "captures_df = egoexo_pd[\"captures\"]\n",
    "\n",
    "validated_takes_df = takes_df[takes_df.validated > 0]\n",
    "num_validated_takes = (takes_df.validated > 0).sum()\n",
    "num_validated_captures = (captures_df.timesync_validated > 0).sum()\n",
    "\n",
    "assert len(validated_takes_df) == num_validated_takes\n",
    "\n",
    "num_takes = len(takes_df)\n",
    "num_captures = len(captures_df)\n",
    "\n",
    "print(\"Summary Stats\\n-------------\")\n",
    "print(f\"Number of Validated Captures: {num_validated_captures} / {num_captures}\")\n",
    "print(\n",
    "    f\"Number of Captures w/ Trajectory (&validated): {num_traj_captures} (valid={num_valid_traj_captures}) / {num_captures} (valid={num_validated_captures}))\"\n",
    ")\n",
    "print(f\"Number of Validated Takes: {num_validated_takes} / {num_takes}\")\n",
    "print(\n",
    "    f\"Number of Takes w/ Trajectory (&validated): {num_traj_takes} (valid={num_valid_traj_takes}) / {num_takes} (valid={num_validated_takes}))\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of Narrated Takes: {takes_df.is_narrated.sum()} / {num_validated_takes} (total takes={num_takes})\"\n",
    ")\n",
    "print(\n",
    "    f\"Take Hours Validated (All): {(validated_takes_df.duration_sec.sum() / 3600):.4f}hrs ({(takes_df.duration_sec.sum() / 3600):.4f}hrs)\"\n",
    ")\n",
    "print(f\"Mean Take Duration: {(takes_df.duration_sec.mean() / 60):.4f}mins\")\n",
    "print()\n",
    "print(\"Take Scenarios\\n-------------\")\n",
    "for k, v in takes_df.groupby(\"task_name\").count()[\"take_uid\"].to_dict().items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "ps_counts = defaultdict(int)\n",
    "for _, take in takes_df.iterrows():\n",
    "    ps_counts[take[\"physical_setting_uid\"]] += 1\n",
    "\n",
    "print(f\"Number of unique physical settings for takes: {len(ps_counts)}\")\n",
    "print()\n",
    "print(\"Number of takes per physical setting\\n----------------------\")\n",
    "for k, v in ps_counts.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_physical = len(\n",
    "    egoexo_pd[\"captures\"]\n",
    "    .groupby(\"physical_setting_uid\")\n",
    "    .count()[\"capture_uid\"]\n",
    "    .to_dict()\n",
    ")\n",
    "print(f\"Number of all possible physical settings: {num_physical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203416b",
   "metadata": {},
   "source": [
    "# Read Video Data\n",
    "\n",
    "The videos we reccomend to read are the synchronized videos trimmed for each take. You \n",
    "can access the locations of these with `egoexo[\"takes\"]` which corresponds to \n",
    "the `take.json` file from the `RELEASE_DIR`.\n",
    "\n",
    "The synchronized videos are frame-aligned between all cameras, meaning you\n",
    "do not need to refer to metadata in order to obtain a synchronized frame. Just read \n",
    "the same frame index for each associated video of a take.\n",
    "\n",
    "There is additionally collage videos, which can be used for visualization purposes. These are the videos being used to annotate keystep and narrations.\n",
    "\n",
    "Usage of both is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b83984",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_idx = random.randint(0, len(egoexo[\"takes\"]))\n",
    "take = egoexo[\"takes\"][take_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ee531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.research.readers import PyAvReader\n",
    "from ego4d.research.readers import TorchAudioStreamReader\n",
    "\n",
    "VideoReader = PyAvReader\n",
    "# VideoReader = TorchAudioStreamReader  # see: https://pytorch.org/audio/stable/hw_acceleration_tutorial.html\n",
    "gpu_idx = -1 # use >= 0 to use a CUDA GPU (only for TorchAudioStreamReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create readers for each video\n",
    "videos = {}\n",
    "for k, temp in take[\"frame_aligned_videos\"].items():\n",
    "    for stream_id, v in temp.items():\n",
    "        path = v[\"relative_path\"]\n",
    "        local_path = os.path.join(RELEASE_DIR, \"takes\", take[\"root_dir\"], f\"{v['relative_path']}\")\n",
    "        print(path, local_path)\n",
    "        videos[(k, stream_id)] = VideoReader(\n",
    "            local_path,\n",
    "            resize=None,\n",
    "            mean=None,\n",
    "            frame_window_size=1,\n",
    "            stride=1,\n",
    "            gpu_idx=gpu_idx,\n",
    "        )\n",
    "\n",
    "for k, v in videos.items():\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "n_frames = len(videos[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(frames_by_key):\n",
    "    result = {}\n",
    "    for key, frame_indices in tqdm(frames_by_key.items()):\n",
    "        frames = []\n",
    "        reader = videos[key]\n",
    "        for idx in frame_indices:\n",
    "            frames.append(reader[idx])\n",
    "        result[key] = frames\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d02d8",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = random.randint(0, n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_frames({\n",
    "    (\"aria01\", \"rgb\"): [frame_idx],\n",
    "    (\"cam01\", \"0\"): [frame_idx],\n",
    "    (\"cam02\", \"0\"): [frame_idx],\n",
    "    (\"cam03\", \"0\"): [frame_idx],\n",
    "    (\"cam04\", \"0\"): [frame_idx],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86001601",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(frames)\n",
    "f, ax = plt.subplots(N, 1)\n",
    "\n",
    "for idx, key in enumerate(frames.keys()):\n",
    "    img = frames[key][0].squeeze().cpu().numpy()\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].set_title(f\"camera: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collage_frame = videos[(\"collage\", \"0\")][frame_idx]\n",
    "\n",
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "img = collage_frame[0].squeeze().cpu().numpy()\n",
    "ax.imshow(img)\n",
    "_ = ax.set_title(\"Take-Level Collage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede3fbe",
   "metadata": {},
   "source": [
    "# Other Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea34ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(egoexo[\"visual_objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "egoexo[\"visual_objects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "egoexo[\"participants\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
