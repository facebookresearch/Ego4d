{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8032419a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook shows the capabilities of the [MAWS CLIP](https://github.com/facebookresearch/maws) features and how to use them in context with Ego-Exo4D. Each frame of Ego-Exo4D videos been fed into this model.\n",
    "\n",
    "In this notebook it is shown that you can perform zero-shot classification with these features using the take's task labels: obtaining an accuracy of 97.8% top-1 and 99% top-2 accuracy. \n",
    "\n",
    "It is reccomended to read the paper: https://arxiv.org/abs/2303.13496\n",
    "\n",
    "Zero-shot, N-shot and linear probe performance of the model are shown to be effective on a wide-variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ego4d.research.readers import TorchAudioStreamReader, PyAvReader\n",
    "VideoReader = TorchAudioStreamReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a56ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dir = \"/large_experiments/egoexo/v2/\"  # NOTE: changeme\n",
    "\n",
    "egoexo = {\n",
    "    \"takes\": os.path.join(release_dir, \"takes.json\"),\n",
    "    \"captures\": os.path.join(release_dir, \"captures.json\"),\n",
    "    \"physical_setting\": os.path.join(release_dir, \"physical_setting.json\"),\n",
    "    \"participants\": os.path.join(release_dir, \"participants.json\"),\n",
    "    \"visual_objects\": os.path.join(release_dir, \"visual_objects.json\"),\n",
    "    \"splits\": os.path.join(release_dir, \"annotations/splits.json\"),\n",
    "}\n",
    "\n",
    "for k, v in egoexo.items():\n",
    "    egoexo[k] = json.load(open(v))\n",
    "\n",
    "takes = egoexo[\"takes\"]\n",
    "captures = egoexo[\"captures\"]\n",
    "takes_by_uid = {x[\"take_uid\"]: x for x in takes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056ab67-d472-4d6f-a485-093fbd68fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = egoexo[\"splits\"][\"split_to_take_uids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcb16e-1602-403c-a4d2-81a35dca6151",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.research.clep.val import accuracy\n",
    "from ego4d.research.dataset import LabelledFeatureDset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_stream_ids(take):\n",
    "    for cam_id, vs in take[\"frame_aligned_videos\"].items():\n",
    "        if cam_id in (\"best_exo\", \"collage\"):\n",
    "            continue\n",
    "        stream_id = \"0\"\n",
    "        if \"aria\" in cam_id.lower():\n",
    "            stream_id = \"rgb\"\n",
    "        \n",
    "        yield cam_id, stream_id\n",
    "\n",
    "def ego_cam_id_filter(take_uid, cam_id):\n",
    "    return \"aria\" in cam_id.lower()\n",
    "\n",
    "def get_data_for(split):\n",
    "    return [\n",
    "        (\n",
    "            f\"{x['take_uid']}_\" + \"_\".join(cam_stream_id),\n",
    "            {\n",
    "                \"parent_task_id\": x[\"parent_task_id\"] // 1000,\n",
    "                \"parent_task_name\": x[\"parent_task_name\"],\n",
    "                \"take_uid\": x[\"take_uid\"],\n",
    "                \"cam_id\": cam_stream_id[0],\n",
    "                \"stream_id\": cam_stream_id[1],\n",
    "            },\n",
    "        )\n",
    "        for x in egoexo[\"takes\"] if x[\"take_uid\"] in split\n",
    "        for cam_stream_id in get_cam_stream_ids(x)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(loader, topk, all_cams=False, cam_id_filter_fn=None):\n",
    "    incorrect = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cmps = []\n",
    "        for x, y in tqdm(loader):\n",
    "            vfs = x.cuda()\n",
    "            assert vfs.shape[0] == 1\n",
    "            \n",
    "            pred = model_fn(vfs)\n",
    "            cmps.append((pred, y))\n",
    "        \n",
    "        probs_by_take = defaultdict(list)\n",
    "        for logits, y in cmps:\n",
    "            assert len(y[\"take_uid\"]) == 1\n",
    "            probs_by_take[y[\"take_uid\"][0]].append((logits.mean(1), y))\n",
    "        \n",
    "        accs = [0 for x in topk]\n",
    "        n = 0\n",
    "        for take_uid, prob_labels in probs_by_take.items():\n",
    "            _, y = prob_labels[0]\n",
    "            pred_targs = [\n",
    "                (p, py[\"parent_task_id\"])\n",
    "                for (p, py) in prob_labels\n",
    "                if cam_id_filter_fn is None or cam_id_filter_fn(py[\"take_uid\"], py[\"cam_id\"][0])\n",
    "            ]\n",
    "            if len(pred_targs) == 0:\n",
    "                continue\n",
    "            \n",
    "            if all_cams:\n",
    "                pred = torch.stack([x for x, _ in pred_targs]).mean(0) \n",
    "                target = y[\"parent_task_id\"]\n",
    "                pred_targs = [(pred, target)]\n",
    "            \n",
    "            for pred, target in pred_targs:\n",
    "                for i, acc in enumerate(accuracy(pred, target.cuda(), topk=topk)):\n",
    "                    if acc != 1:\n",
    "                        incorrect.append((take_uid, pred.argmax().cpu().item(), target.cpu().item()))\n",
    "                    accs[i] += acc\n",
    "                n += pred.shape[0]\n",
    "\n",
    "    accs = [x/n for x in accs]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_by_topk\": {\n",
    "            topk[i]: acc\n",
    "            for i, acc in enumerate(accs)\n",
    "        },\n",
    "        \"n\": n,\n",
    "        \"incorrect\": incorrect,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdb1de",
   "metadata": {},
   "source": [
    "# Feature Pre-Processing (for perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea300e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.research.dataset import save_features_to_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = os.path.join(release_dir, \"features/maws_clip_2b\")\n",
    "# features_dir = \"/checkpoint/miguelmartin/egoexo_features/maws_clip_2b_public\"\n",
    "features_paths = [x for x in os.listdir(features_dir) if x != \"config.yaml\"]\n",
    "\n",
    "features_by_take_cam = {}\n",
    "for x in features_paths:\n",
    "    take_uid, cam_id, stream_id_pt = x.split(\"_\")\n",
    "    stream_id = stream_id_pt = stream_id_pt.split(\".\")[0]\n",
    "    if take_uid not in features_by_take_cam:\n",
    "        features_by_take_cam[take_uid] = {}\n",
    "    key = (cam_id, stream_id)\n",
    "    features_by_take_cam[take_uid][key] = os.path.join(features_dir, x)\n",
    "\n",
    "features_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a694a-7185-43fd-bb31-2503909902e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/checkpoint/miguelmartin/egoexo_features/maws_clip_2b_public.hdf5\"\n",
    "video_uids = [x.split(\".\")[0] for x in features_paths]\n",
    "feature_hdf5_path = out_path\n",
    "# NOTE: this will take ~50minutes\n",
    "# save_features_to_hdf5(video_uids, feature_dir=features_dir, out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"facebookresearch/maws\", model=\"vit_2b14_xlmr_l_maws_clip\")\n",
    "model = model.eval().half()\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998344c-6795-41a4-a1c6-879ed38f32c5",
   "metadata": {},
   "source": [
    "# Zero-Shot on Task Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4029b8a-7fce-4c3a-b768-686ae134d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_takes = set(splits[\"train\"]) & set(features_by_take_cam.keys())\n",
    "val_takes = set(splits[\"val\"]) & set(features_by_take_cam.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe793b-1aa7-4016-ab14-ed8ee7a5a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(get_cam_stream_ids(egoexo[\"takes\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c72c3-4f7b-4e9c-a56d-65a283ad7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = get_data_for(val_takes)\n",
    "train_data = get_data_for(train_takes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f140c79-baf5-46aa-a70d-9477a8e5040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_labels = [\n",
    "    \"Phone with a QR code\",\n",
    "    \"A person is cooking\",\n",
    "    \"A person is performing Health related activities such as a COVID-19 test or CPR\",\n",
    "    \"A person is at a campsite\",\n",
    "    \"A person is performing repair on a bike\",\n",
    "    \"A person is playing a musical instrument\",\n",
    "    \"A person is playing basketball\",\n",
    "    \"A person is rock climbing\",\n",
    "    \"A person is playing soccer\",\n",
    "    \"A person is dancing\",\n",
    "]\n",
    "txt_emb = model.encode_texts(texts=txt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42462db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example zero-shot inference\n",
    "feature_path = \"000a19fe-776e-4c88-b0c3-2fad016a6025_aria01_rgb.pt\"\n",
    "take_uid = feature_path.split(\"_\")[0]\n",
    "gt_label = takes_by_uid[take_uid][\"parent_task_name\"]\n",
    "feature_path = os.path.join(features_dir, feature_path)\n",
    "xs = torch.load(feature_path)\n",
    "txt_emb = model.encode_texts(texts=txt_labels)\n",
    "logits = model.classify(text_features=txt_emb, image_features=xs.cuda())\n",
    "pred_class = logits.mean(0).argmax()\n",
    "pred_txt = txt_labels[pred_class]\n",
    "\n",
    "print(f\"\"\"\n",
    "Take: {take_uid}\n",
    "Prediction: {pred_txt}\n",
    "GT: {gt_label}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68e111-4aa0-4981-8c64-bc84e7925844",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset = LabelledFeatureDset(feature_hdf5_path, val_data)\n",
    "val_dloader = DataLoader(val_dset, batch_size=1, shuffle=False)\n",
    "\n",
    "train_dset = LabelledFeatureDset(feature_hdf5_path, train_data)\n",
    "train_dloader = DataLoader(train_dset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c3b16-6f39-4215-8ba3-7267be833818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda x: model.classify(text_features=txt_emb, image_features=x)\n",
    "classifier = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cee32-6b13-432d-b852-4821b0fb4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=False, cam_id_filter_fn=None)[\"accuracy_by_topk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45fcf-5c3e-426e-98dc-b029c3543fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=True, cam_id_filter_fn=None)[\"accuracy_by_topk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca841f-6d8f-481f-8af3-2ef760d688d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=True, cam_id_filter_fn=ego_cam_id_filter)[\"accuracy_by_topk\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
