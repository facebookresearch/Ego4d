{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbc87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ego4d.research.readers import TorchAudioStreamReader, PyAvReader\n",
    "VideoReader = TorchAudioStreamReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9950a58-2b22-4965-bcfa-b33081a42460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_stream_ids(take):\n",
    "    for cam_id, vs in take[\"frame_aligned_videos\"].items():\n",
    "        if cam_id in (\"best_exo\", \"collage\"):\n",
    "            continue\n",
    "        stream_id = \"0\"\n",
    "        if \"aria\" in cam_id.lower():\n",
    "            stream_id = \"rgb\"\n",
    "        \n",
    "        yield cam_id, stream_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a56ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEASE_DIR = \"/checkpoint/miguelmartin/egoexo_data/dev\"  # NOTE: changeme\n",
    "\n",
    "egoexo = {\n",
    "    \"takes\": os.path.join(RELEASE_DIR, \"takes.json\"),\n",
    "    \"captures\": os.path.join(RELEASE_DIR, \"captures.json\"),\n",
    "    \"physical_setting\": os.path.join(RELEASE_DIR, \"physical_setting.json\"),\n",
    "    \"participants\": os.path.join(RELEASE_DIR, \"participants.json\"),\n",
    "    \"visual_objects\": os.path.join(RELEASE_DIR, \"visual_objects.json\"),\n",
    "    \"splits\": os.path.join(RELEASE_DIR, \"annotations/splits.json\"),\n",
    "}\n",
    "\n",
    "for k, v in egoexo.items():\n",
    "    egoexo[k] = json.load(open(v))\n",
    "\n",
    "takes = egoexo[\"takes\"]\n",
    "captures = egoexo[\"captures\"]\n",
    "takes_by_uid = {x[\"take_uid\"]: x for x in takes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1056ab67-d472-4d6f-a485-093fbd68fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = egoexo[\"splits\"][\"split_to_take_uids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcb16e-1602-403c-a4d2-81a35dca6151",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30372360-24b9-480e-a86f-740375fcbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.research.dataset import save_ego4d_features_to_hdf5\n",
    "\n",
    "# TODO: do dropped\n",
    "def get_data_for(split):\n",
    "    return [\n",
    "        (\n",
    "            f\"{x['take_uid']}_\" + \"_\".join(cam_stream_id),\n",
    "            {\n",
    "                \"parent_task_id\": x[\"parent_task_id\"] // 1000,\n",
    "                \"parent_task_name\": x[\"parent_task_name\"],\n",
    "                \"take_uid\": x[\"take_uid\"],\n",
    "                \"cam_id\": cam_stream_id[0],\n",
    "                \"stream_id\": cam_stream_id[1],\n",
    "            },\n",
    "        )\n",
    "        for x in egoexo[\"takes\"] if x[\"take_uid\"] in split\n",
    "        for cam_stream_id in get_cam_stream_ids(x)\n",
    "    ]\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import bisect\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from ego4d.research.readers import PyAvReader, StridedReader, TorchAudioStreamReader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class LabelledFeatureDset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple utility class to load features associated with labels. The input this\n",
    "    method requires is as follows:\n",
    "        1. `feature_hdf5_path`: the features transposed to a HDF5 file.\n",
    "            See `save_ego4d_features_to_hdf5`\n",
    "        2. `uid_label_pairs` a list of (uid, label). `label` can be anything\n",
    "            `uid` is a unique id associated to the `feature_hdf5_path` file.\n",
    "        3. `aggr_function` a function to aggregate based off given label\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_hdf5_path: str,\n",
    "        uid_label_pairs: List[Tuple[str, Any]],\n",
    "        aggr_function: Optional[Callable[[torch.Tensor, Any], torch.Tensor]] = None,\n",
    "    ):\n",
    "        self.features = h5py.File(feature_hdf5_path)\n",
    "        self.aggr_function = (\n",
    "            aggr_function\n",
    "            if aggr_function is not None\n",
    "            else lambda x, _: torch.tensor(x[0:]).squeeze()\n",
    "        )\n",
    "        self.uid_label_pairs = uid_label_pairs\n",
    "        f_keys = set(self.features.keys())\n",
    "        l_keys = set(uid for uid, _ in self.uid_label_pairs)\n",
    "        if len(l_keys - f_keys) > 0:\n",
    "            print(f\"WARN: missing {len(l_keys - f_keys)} keys in feature hdf5 path: {feature_hdf5_path}\")\n",
    "            self.uid_label_pairs = [(uid, label) for uid, label in self.uid_label_pairs if uid in f_keys]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.uid_label_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        uid, label = self.uid_label_pairs[idx]\n",
    "        feat = self.aggr_function(self.features[uid], label)\n",
    "        return feat, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998344c-6795-41a4-a1c6-879ed38f32c5",
   "metadata": {},
   "source": [
    "# Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697c64ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18664e6b-14c7-4a39-97e0-fd5af39268fb_aria01_rgb.pt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dir = \"/checkpoint/miguelmartin/egoexo_features/maws_clip_2b_public\"\n",
    "features_paths = [x for x in os.listdir(features_dir) if x != \"config.yaml\"]\n",
    "\n",
    "features_by_take_cam = {}\n",
    "for x in features_paths:\n",
    "    take_uid, cam_id, stream_id_pt = x.split(\"_\")\n",
    "    stream_id = stream_id_pt = stream_id_pt.split(\".\")[0]\n",
    "    if take_uid not in features_by_take_cam:\n",
    "        features_by_take_cam[take_uid] = {}\n",
    "    key = (cam_id, stream_id)\n",
    "    features_by_take_cam[take_uid][key] = os.path.join(features_dir, x)\n",
    "    \n",
    "    \n",
    "features_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45808711-87ea-45ce-8933-e08b4bf44cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('gp03', '0'), ('gp01', '0'), ('aria01', 'rgb'), ('gp04', '0'), ('gp05', '0'), ('gp02', '0'), ('gp06', '0')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_by_take_cam[take_uid].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e87299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7a694a-7185-43fd-bb31-2503909902e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912a0cfb9b6b4e9e8299fd348d8fdd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "video_uid:   0%|          | 0/23929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_path = \"/checkpoint/miguelmartin/egoexo_features/maws_clip_2b_public.hdf5\"\n",
    "video_uids = [x.split(\".\")[0] for x in features_paths]\n",
    "feature_hdf5_path = out_path\n",
    "save_ego4d_features_to_hdf5(video_uids, feature_dir=features_dir, out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4029b8a-7fce-4c3a-b768-686ae134d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_takes = set(splits[\"train\"]) & set(features_by_take_cam.keys())\n",
    "val_takes = set(splits[\"val\"]) & set(features_by_take_cam.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25fe793b-1aa7-4016-ab14-ed8ee7a5a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aria01', 'rgb'),\n",
       " ('cam01', '0'),\n",
       " ('cam02', '0'),\n",
       " ('cam03', '0'),\n",
       " ('cam04', '0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_cam_stream_ids(egoexo[\"takes\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5c72c3-4f7b-4e9c-a56d-65a283ad7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = get_data_for(val_takes)\n",
    "train_data = get_data_for(train_takes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ced151b9-0bf4-4dd0-bf9a-c07ad3b5b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_keys = set(val_dset.features.keys())\n",
    "# l_keys = set(uid for uid, _ in val_dset.uid_label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a218523-9a3f-40f2-affa-45b366de7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.research.clep.val import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc25ca3-3d3f-4a9f-99ed-61af5b789240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maws.model_builder import build_model\n",
    "model = build_model(\"vit_2b14_xlmr_l\", \"maws_clip\")\n",
    "model = model.eval().half()\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f140c79-baf5-46aa-a70d-9477a8e5040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_labels = [\n",
    "    \"Phone with a QR code\",\n",
    "    \"A person is cooking\",\n",
    "    \"A person is performing Health related activities such as a COVID-19 test or CPR\",\n",
    "    \"A person is at a campsite\",\n",
    "    \"A person is performing repair on a bike\",\n",
    "    \"A person is playing a musical instrument\",\n",
    "    \"A person is playing basketball\",\n",
    "    \"A person is rock climbing\",\n",
    "    \"A person is playing soccer\",\n",
    "    \"A person is dancing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c68e111-4aa0-4981-8c64-bc84e7925844",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset = LabelledFeatureDset(feature_hdf5_path, val_data)\n",
    "val_dloader = DataLoader(val_dset, batch_size=1, shuffle=False)\n",
    "\n",
    "train_dset = LabelledFeatureDset(feature_hdf5_path, train_data)\n",
    "train_dloader = DataLoader(train_dset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5daace2c-f5aa-47e4-9d24-4b148eeccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_emb = model.encode_texts(texts=txt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e3c3b16-6f39-4215-8ba3-7267be833818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda x: model.classify(text_features=txt_emb, image_features=x)\n",
    "classifier = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1558ad4-2ec1-4fed-8c96-135f6b393e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(loader, topk, all_cams=False, cam_id_filter_fn=None):\n",
    "    incorrect = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cmps = []\n",
    "        for x, y in tqdm(loader):\n",
    "            vfs = x.cuda()\n",
    "            assert vfs.shape[0] == 1\n",
    "            \n",
    "            pred = model_fn(vfs)\n",
    "            cmps.append((pred, y))\n",
    "        \n",
    "        probs_by_take = defaultdict(list)\n",
    "        for logits, y in cmps:\n",
    "            assert len(y[\"take_uid\"]) == 1\n",
    "            probs_by_take[y[\"take_uid\"][0]].append((logits.mean(1), y))\n",
    "        \n",
    "        accs = [0 for x in topk]\n",
    "        n = 0\n",
    "        for take_uid, prob_labels in probs_by_take.items():\n",
    "            _, y = prob_labels[0]\n",
    "            pred_targs = [\n",
    "                (p, py[\"parent_task_id\"])\n",
    "                for (p, py) in prob_labels\n",
    "                if cam_id_filter_fn is None or cam_id_filter_fn(py[\"take_uid\"], py[\"cam_id\"][0])\n",
    "            ]\n",
    "            if len(pred_targs) == 0:\n",
    "                continue\n",
    "            \n",
    "            if all_cams:\n",
    "                pred = torch.stack([x for x, _ in pred_targs]).mean(0) \n",
    "                target = y[\"parent_task_id\"]\n",
    "                pred_targs = [(pred, target)]\n",
    "            \n",
    "            for pred, target in pred_targs:\n",
    "                for i, acc in enumerate(accuracy(pred, target.cuda(), topk=topk)):\n",
    "                    if acc != 1:\n",
    "                        incorrect.append((take_uid, pred.argmax().cpu().item(), target.cpu().item()))\n",
    "                    accs[i] += acc\n",
    "                n += pred.shape[0]\n",
    "\n",
    "    accs = [x/n for x in accs]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_by_topk\": {\n",
    "            topk[i]: acc\n",
    "            for i, acc in enumerate(accs)\n",
    "        },\n",
    "        \"n\": n,\n",
    "        \"incorrect\": incorrect,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d015891-f5b5-463b-b003-1454484bcc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_cam_id_filter(take_uid, cam_id):\n",
    "    return \"aria\" in cam_id.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e0cee32-6b13-432d-b852-4821b0fb4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3797/3797 [04:18<00:00, 14.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.9399525941532789,\n",
       " 2: 0.9723465894126943,\n",
       " 3: 0.9855148801685542,\n",
       " 5: 0.9971029760337108}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=False, cam_id_filter_fn=None)[\"accuracy_by_topk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45fcf-5c3e-426e-98dc-b029c3543fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 2557/3797 [00:45<00:04, 292.95it/s]"
     ]
    }
   ],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=True, cam_id_filter_fn=None)[\"accuracy_by_topk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca841f-6d8f-481f-8af3-2ef760d688d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification(val_dloader, topk=(1, 2, 3, 5), all_cams=True, cam_id_filter_fn=ego_cam_id_filter)[\"accuracy_by_topk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7efa59-341e-49ff-b1ec-d65355696d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17ed815c-5286-4be0-a947-22cbd26413c6",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b546fc-f92d-4cc7-8207-8e22a3ee8073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b825a95b-0a7d-4b86-b576-67b50612f3e6",
   "metadata": {},
   "source": [
    "# Kepstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dffaf5-072e-4ab0-9a85-142d27514903",
   "metadata": {},
   "outputs": [],
   "source": [
    "keystep_train = json.load(open(\"/large_experiments/egoexo/dataset/annotations/keystep_train.json\"))\n",
    "keystep_val = json.load(open(\"/large_experiments/egoexo/dataset/annotations/keystep_val.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dad03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keystep_train[\"taxonomy\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec405fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae",
   "language": "python",
   "name": "mae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
