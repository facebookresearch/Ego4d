{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c1049a-ea49-4fd4-bce5-9a7097026a3f",
   "metadata": {},
   "source": [
    "# Tutorial 2: Hand and Body Pose in Ego-Exo4D Dataset\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "**Filled notebook:**\n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Tutorial&message=View%20On%20Github&color=lightgrey)](https://github.com/facebookresearch/Ego4d/tree/main/notebooks/egoexo)   \n",
    "**Author:** Xizi Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d88077-503d-4880-874b-4d725740dc8b",
   "metadata": {},
   "source": [
    "3D hand and body pose are two important annotations of the Ego-Exo4D dataset. The figure on the left captures the **full body pose** and surrounding environment context, whereas the figure on the right captures the details of close-by **hand-object** interactions and the camera wearer’s attention. In this tutorial, we provide a step-by-step guide on retrieving the hand and body pose of one example take, and projecting the body pose to exocentric views and the hand pose to egocentric views, then visualizing it on the corresponding frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40187b02-236e-4925-b384-2a5434fd701f",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/hand_body_pose.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907e631-cb4b-4505-948b-99f0ec89a921",
   "metadata": {},
   "source": [
    "### 1. Prerequisites and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f037d895-a9d9-4b33-aa0f-d4a08e769bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import rcParams\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core import data_provider\n",
    "from projectaria_tools.core import calibration\n",
    "from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 16, 32\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import av\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe855784-c83d-4d33-884b-ac94a3a1aee0",
   "metadata": {},
   "source": [
    "Next we define some necessary utility functions for retrieving pose metadata and drawing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03715362-3678-4e7e-8f23-248cb1376ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataframe at target path to csv\n",
    "def load_csv_to_df(filepath: str) -> pd.DataFrame:\n",
    "    with open(filepath, \"r\") as csv_file:\n",
    "        return pd.read_csv(csv_file)\n",
    "\n",
    "# color palette for drawing the keypoints\n",
    "palette = np.array(\n",
    "    [\n",
    "        [255, 128, 0],\n",
    "        [255, 153, 51],\n",
    "        [255, 178, 102],\n",
    "        [230, 230, 0],\n",
    "        [255, 153, 255],\n",
    "        [153, 204, 255],\n",
    "        [255, 102, 255],\n",
    "        [255, 51, 255],\n",
    "        [102, 178, 255],\n",
    "        [51, 153, 255],\n",
    "        [255, 153, 153],\n",
    "        [255, 102, 102],\n",
    "        [255, 51, 51],\n",
    "        [153, 255, 153],\n",
    "        [102, 255, 102],\n",
    "        [51, 255, 51],\n",
    "        [0, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 0, 0],\n",
    "        [255, 255, 255],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# retrieve the body keypoints and skeleton\n",
    "def get_body_metadata():\n",
    "    keypoints_map = [\n",
    "        {\"label\": \"Nose\", \"id\": \"fee3cbd2\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Left-eye\", \"id\": \"ab12de34\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Right-eye\", \"id\": \"7f2g1h6k\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Left-ear\", \"id\": \"mn0pqrst\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Right-ear\", \"id\": \"yz89wx76\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Left-shoulder\", \"id\": \"5a4b3c2d\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Right-shoulder\", \"id\": \"e1f2g3h4\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left-elbow\", \"id\": \"6i7j8k9l\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right-elbow\", \"id\": \"uv0wxy12\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left-wrist\", \"id\": \"3z4ab5cd\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Right-wrist\", \"id\": \"efgh6789\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Left-hip\", \"id\": \"ijklmnop\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Right-hip\", \"id\": \"qrstuvwx\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Left-knee\", \"id\": \"yz012345\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Right-knee\", \"id\": \"6bc7defg\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left-ankle\", \"id\": \"hijk8lmn\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right-ankle\", \"id\": \"opqrs1tu\", \"color\": \"#f564d4\"},\n",
    "    ]\n",
    "\n",
    "    # pyre-ignore\n",
    "    pose_kpt_color = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "    skeleton = [\n",
    "        [16, 14],\n",
    "        [14, 12],\n",
    "        [17, 15],\n",
    "        [15, 13],\n",
    "        [12, 13],\n",
    "        [6, 12],\n",
    "        [7, 13],\n",
    "        [6, 7],\n",
    "        [6, 8],\n",
    "        [7, 9],\n",
    "        [8, 10],\n",
    "        [9, 11],\n",
    "        [2, 3],\n",
    "        [1, 2],\n",
    "        [1, 3],\n",
    "        [2, 4],\n",
    "        [3, 5],\n",
    "        [4, 6],\n",
    "        [5, 7],\n",
    "    ]\n",
    "    return keypoints_map, skeleton, pose_kpt_color\n",
    "\n",
    "\n",
    "# retrieve the hand keypoints and skeleton\n",
    "def get_hands_metadata():\n",
    "    keypoints_map = [\n",
    "        {\"label\": \"Right_Wrist\", \"id\": \"fee3cbd2\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Right_Thumb_1\", \"id\": \"yz012345\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Right_Thumb_2\", \"id\": \"6bc7defg\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Right_Thumb_3\", \"id\": \"hijk8lmn\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Thumb_4\", \"id\": \"opqrs1tu\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Right_Index_1\", \"id\": \"ab12de34\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Right_Index_2\", \"id\": \"7f2g1h6k\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Right_Index_3\", \"id\": \"mn0pqrst\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Right_Index_4\", \"id\": \"9vwxyzab\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Right_Middle_1\", \"id\": \"yz89wx76\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Right_Middle_2\", \"id\": \"5a4b3c2d\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Right_Middle_3\", \"id\": \"e1f2g3h4\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Middle_4\", \"id\": \"cdefgh23\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Ring_1\", \"id\": \"efgh6789\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Right_Ring_2\", \"id\": \"ijklmnop\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Right_Ring_3\", \"id\": \"qrstuvwx\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Right_Ring_4\", \"id\": \"ijkl4567\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right_Pinky_1\", \"id\": \"6i7j8k9l\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right_Pinky_2\", \"id\": \"uv0wxy12\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Right_Pinky_3\", \"id\": \"3z4ab5cd\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Right_Pinky_4\", \"id\": \"mnop8qrs\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Wrist\", \"id\": \"fee3cbd2_left\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Left_Thumb_1\", \"id\": \"yz012345_left\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Left_Thumb_2\", \"id\": \"6bc7defg_left\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left_Thumb_3\", \"id\": \"hijk8lmn_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Thumb_4\", \"id\": \"opqrs1tu_left\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Index_1\", \"id\": \"ab12de34_left\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Left_Index_2\", \"id\": \"7f2g1h6k_left\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Left_Index_3\", \"id\": \"mn0pqrst_left\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Left_Index_4\", \"id\": \"9vwxyzab_left\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left_Middle_1\", \"id\": \"yz89wx76_left\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Left_Middle_2\", \"id\": \"5a4b3c2d_left\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Left_Middle_3\", \"id\": \"e1f2g3h4_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Middle_4\", \"id\": \"cdefgh23_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Ring_1\", \"id\": \"efgh6789_left\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Left_Ring_2\", \"id\": \"ijklmnop_left\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Left_Ring_3\", \"id\": \"qrstuvwx_left\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Left_Ring_4\", \"id\": \"ijkl4567_left\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Left_Pinky_1\", \"id\": \"6i7j8k9l_left\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Left_Pinky_2\", \"id\": \"uv0wxy12_left\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Pinky_3\", \"id\": \"3z4ab5cd_left\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Left_Pinky_4\", \"id\": \"mnop8qrs_left\", \"color\": \"#f564d4\"},\n",
    "    ]\n",
    "\n",
    "    links = {\n",
    "        \"fee3cbd2\": [\"ab12de34\", \"yz89wx76\", \"6i7j8k9l\", \"efgh6789\", \"yz012345\"],\n",
    "        \"ab12de34\": [\"7f2g1h6k\"],\n",
    "        \"7f2g1h6k\": [\"mn0pqrst\"],\n",
    "        \"mn0pqrst\": [\"9vwxyzab\"],\n",
    "        \"yz89wx76\": [\"5a4b3c2d\"],\n",
    "        \"5a4b3c2d\": [\"e1f2g3h4\"],\n",
    "        \"e1f2g3h4\": [\"cdefgh23\"],\n",
    "        \"6i7j8k9l\": [\"uv0wxy12\"],\n",
    "        \"uv0wxy12\": [\"3z4ab5cd\"],\n",
    "        \"3z4ab5cd\": [\"mnop8qrs\"],\n",
    "        \"efgh6789\": [\"ijklmnop\"],\n",
    "        \"ijklmnop\": [\"qrstuvwx\"],\n",
    "        \"qrstuvwx\": [\"ijkl4567\"],\n",
    "        \"yz012345\": [\"6bc7defg\"],\n",
    "        \"6bc7defg\": [\"hijk8lmn\"],\n",
    "        \"hijk8lmn\": [\"opqrs1tu\"],\n",
    "        \"fee3cbd2_left\": [\n",
    "            \"ab12de34_left\",\n",
    "            \"yz89wx76_left\",\n",
    "            \"6i7j8k9l_left\",\n",
    "            \"efgh6789_left\",\n",
    "            \"yz012345_left\",\n",
    "        ],\n",
    "        \"ab12de34_left\": [\"7f2g1h6k_left\"],\n",
    "        \"7f2g1h6k_left\": [\"mn0pqrst_left\"],\n",
    "        \"mn0pqrst_left\": [\"9vwxyzab_left\"],\n",
    "        \"yz89wx76_left\": [\"5a4b3c2d_left\"],\n",
    "        \"5a4b3c2d_left\": [\"e1f2g3h4_left\"],\n",
    "        \"e1f2g3h4_left\": [\"cdefgh23_left\"],\n",
    "        \"6i7j8k9l_left\": [\"uv0wxy12_left\"],\n",
    "        \"uv0wxy12_left\": [\"3z4ab5cd_left\"],\n",
    "        \"3z4ab5cd_left\": [\"mnop8qrs_left\"],\n",
    "        \"efgh6789_left\": [\"ijklmnop_left\"],\n",
    "        \"ijklmnop_left\": [\"qrstuvwx_left\"],\n",
    "        \"qrstuvwx_left\": [\"ijkl4567_left\"],\n",
    "        \"yz012345_left\": [\"6bc7defg_left\"],\n",
    "        \"6bc7defg_left\": [\"hijk8lmn_left\"],\n",
    "        \"hijk8lmn_left\": [\"opqrs1tu_left\"],\n",
    "    }\n",
    "\n",
    "    hand_dict = dict()\n",
    "    for index, kpt in enumerate(keypoints_map):\n",
    "        hand_dict[kpt[\"id\"]] = index + 1\n",
    "\n",
    "    skeleton = list()\n",
    "    for start_point in links:\n",
    "        end_points = links[start_point]\n",
    "        for end_point in end_points:\n",
    "            start_index = hand_dict[start_point]\n",
    "            end_index = hand_dict[end_point]\n",
    "            skeleton.append([start_index, end_index])\n",
    "\n",
    "    klist = [0]\n",
    "    klist.extend([2] * 4)\n",
    "    klist.extend([4] * 4)\n",
    "    klist.extend([6] * 4)\n",
    "    klist.extend([8] * 4)\n",
    "    klist.extend([10] * 4)\n",
    "    klist.extend([0])\n",
    "    klist.extend([2] * 4)\n",
    "    klist.extend([4] * 4)\n",
    "    klist.extend([6] * 4)\n",
    "    klist.extend([8] * 4)\n",
    "    klist.extend([10] * 4)\n",
    "\n",
    "    pose_kpt_color = palette[klist]\n",
    "    return keypoints_map, skeleton, pose_kpt_color\n",
    "    \n",
    "def get_coords(annot):\n",
    "    pts = dict()\n",
    "    for k in annot:\n",
    "        atype = 1\n",
    "        if annot[k][\"placement\"] == \"auto\":\n",
    "            atype = 0\n",
    "        pts[k] = [annot[k][\"x\"], annot[k][\"y\"], atype]\n",
    "    return pts\n",
    "\n",
    "\n",
    "def draw_skeleton(img, all_pts, skeleton):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for item in skeleton:\n",
    "        left_index = item[0] - 1\n",
    "        right_index = item[1] - 1\n",
    "        left_pt = all_pts[left_index]\n",
    "        right_pt = all_pts[right_index]\n",
    "        if len(left_pt) == 0 or len(right_pt) == 0:\n",
    "            continue\n",
    "        draw.line([left_pt, right_pt], fill=\"white\", width=10)\n",
    "\n",
    "\n",
    "def draw_cross(img, x, y, color):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the cross\n",
    "    cross_length = 10  # Half-length of the cross arms\n",
    "    # Calculate the end points of the cross\n",
    "    left_point = (center[0] - cross_length, center[1])\n",
    "    right_point = (center[0] + cross_length, center[1])\n",
    "    top_point = (center[0], center[1] - cross_length)\n",
    "    bottom_point = (center[0], center[1] + cross_length)\n",
    "\n",
    "    # Draw the horizontal line\n",
    "    draw.line([left_point, right_point], fill=color, width=3)\n",
    "    # Draw the vertical line\n",
    "    draw.line([top_point, bottom_point], fill=color, width=3)\n",
    "\n",
    "\n",
    "def draw_circle(img, x, y, color):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the circle\n",
    "    radius = 12  # Radius of the circle\n",
    "\n",
    "    # Calculate the bounding box of the circle\n",
    "    left_up_point = (center[0] - radius, center[1] - radius)\n",
    "    right_down_point = (center[0] + radius, center[1] + radius)\n",
    "\n",
    "    # Draw the circle with a black outline\n",
    "    draw.ellipse(\n",
    "        [left_up_point, right_down_point], outline=(255, 255, 255), fill=color, width=6\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_label(img, x, y, color, label):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default(size=40)\n",
    "    # Circle parameters\n",
    "    center = (x + 20, y - 20)  # Center of the circle\n",
    "    draw.text(center, label, fill=color, font=font)\n",
    "\n",
    "\n",
    "def draw_skeleton_hands(img, all_pts, skeleton, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for item in skeleton:\n",
    "        left_index = item[0] - 1\n",
    "        right_index = item[1] - 1\n",
    "        left_pt = all_pts[left_index]\n",
    "        right_pt = all_pts[right_index]\n",
    "        if len(left_pt) == 0 or len(right_pt) == 0:\n",
    "            continue\n",
    "        draw.line([left_pt, right_pt], fill=\"white\", width=int(ratio * 4))\n",
    "\n",
    "\n",
    "def draw_circle_hands(img, x, y, color, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the circle\n",
    "    radius = int(ratio * 8)  # Radius of the circle\n",
    "\n",
    "    # Calculate the bounding box of the circle\n",
    "    left_up_point = (center[0] - radius, center[1] - radius)\n",
    "    right_down_point = (center[0] + radius, center[1] + radius)\n",
    "\n",
    "    # Draw the circle with a black outline\n",
    "    draw.ellipse(\n",
    "        [left_up_point, right_down_point],\n",
    "        outline=(255, 255, 255),\n",
    "        fill=color,\n",
    "        width=int(ratio * 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_cross_hands(img, x, y, color, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the cross\n",
    "    cross_length = int(ratio * 8)  # Half-length of the cross arms\n",
    "    # Calculate the end points of the cross\n",
    "    left_point = (center[0] - cross_length, center[1])\n",
    "    right_point = (center[0] + cross_length, center[1])\n",
    "    top_point = (center[0], center[1] - cross_length)\n",
    "    bottom_point = (center[0], center[1] + cross_length)\n",
    "\n",
    "    # Draw the horizontal line\n",
    "    draw.line([left_point, right_point], fill=color, width=int(ratio * 4))\n",
    "    # Draw the vertical line\n",
    "    draw.line([top_point, bottom_point], fill=color, width=int(ratio * 4))\n",
    "\n",
    "\n",
    "def show_results(results):\n",
    "    for cam_name in results:\n",
    "        img = results[cam_name]\n",
    "        plt.figure(figsize=(40, 20))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")  # Hide the axes ticks\n",
    "        plt.title(f\"{cam_name}\", fontsize=20)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_viz(\n",
    "    pil_img,\n",
    "    keypoints_map,\n",
    "    ann,\n",
    "    skeleton,\n",
    "    pose_kpt_color,\n",
    "    annot_type=\"body\",\n",
    "    is_aria=False,\n",
    "):\n",
    "    pts = get_coords(ann)\n",
    "    ratio = 1\n",
    "    if is_aria:\n",
    "        ratio = 0.5\n",
    "\n",
    "    all_pts = []\n",
    "    for _, keypoints in enumerate(keypoints_map):\n",
    "        kpname = keypoints[\"label\"].lower()\n",
    "\n",
    "        if kpname in pts:\n",
    "            x, y = pts[kpname][0], pts[kpname][1]\n",
    "            all_pts.append((x, y))\n",
    "        else:\n",
    "            all_pts.append(())\n",
    "\n",
    "    if annot_type == \"body\":\n",
    "        draw_skeleton(pil_img, all_pts, skeleton)\n",
    "    else:\n",
    "        draw_skeleton_hands(pil_img, all_pts, skeleton, ratio)\n",
    "\n",
    "    for index, keypoints in enumerate(keypoints_map):\n",
    "        kpname = keypoints[\"label\"].lower()\n",
    "        if kpname in pts:\n",
    "            x, y, pt_type = pts[kpname][0], pts[kpname][1], pts[kpname][2]\n",
    "            color = tuple(pose_kpt_color[index])\n",
    "            if pt_type == 1:\n",
    "                if annot_type == \"body\":\n",
    "                    draw_circle(pil_img, x, y, color)\n",
    "                else:\n",
    "                    draw_circle_hands(pil_img, x, y, color, ratio)\n",
    "            else:\n",
    "                if annot_type == \"body\":\n",
    "                    draw_cross(pil_img, x, y, color)\n",
    "                else:\n",
    "                    draw_cross_hands(pil_img, x, y, color, ratio)\n",
    "            if annot_type == \"body\":\n",
    "                draw_label(pil_img, x, y, color, kpname)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    return pil_img\n",
    "\n",
    "\n",
    "# Video Utilities\n",
    "def get_frame(video_local_path, frame_idx):\n",
    "    container = av.open(video_local_path)\n",
    "    frame_count = 0\n",
    "    for frame in container.decode(video=0):\n",
    "        if frame_count == frame_idx:\n",
    "            input_img = np.array(frame.to_image())\n",
    "            pil_img = Image.fromarray(input_img)\n",
    "            return pil_img\n",
    "        frame_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e0b70-0eaf-4b91-b3e7-dfa159bf6a97",
   "metadata": {},
   "source": [
    "### 2. Load one sample take and its ego / exo camera's camera calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21547751-86a7-44c9-afdb-2c9af8d7590c",
   "metadata": {},
   "source": [
    "First, let's define the Ego-Exo4D dataset and its annotation location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a8ae18-9270-4abd-80f6-7ba6e236ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dir = \"/datasets01/egoexo4d/v2/\" # replace this with your download folder\n",
    "annotation_dir = os.path.join(release_dir, \"annotations/\")  # annotation folder\n",
    "\n",
    "# load necessary annotation files\n",
    "egoexo = {\n",
    "    \"takes\": os.path.join(release_dir, \"takes.json\"),\n",
    "    \"captures\": os.path.join(release_dir, \"captures.json\")\n",
    "}\n",
    "\n",
    "for k, v in egoexo.items():\n",
    "    egoexo[k] = json.load(open(v))\n",
    "\n",
    "takes = egoexo[\"takes\"]\n",
    "captures = egoexo[\"captures\"]\n",
    "takes_by_uid = {x[\"take_uid\"]: x for x in takes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89797566-8b79-423f-8b12-45cc3bf73438",
   "metadata": {},
   "source": [
    "We then randomly sample one example take of playing piano. We also provide the take uid of some other takes as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9ee34c-e8ac-46ac-a7fa-c22fd7f556aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take_uid = \"0bc47e29-e086-4726-b874-f89671366f06\"  # Violin\n",
    "take_uid = \"23ff1c48-01ea-4d34-a38b-bc96e767b9b9\" #Piano\n",
    "#take_uid = \"02715c86-e30c-4791-92b7-38b488e51aba\"  # Bike\n",
    "\n",
    "take = [take for take in egoexo[\"takes\"] if take[\"take_uid\"] == take_uid]\n",
    "take = take[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ded14-dcbf-4b93-a84f-93352fccaff6",
   "metadata": {},
   "source": [
    "And load the camera intrinsics and extrinsics of the take. **exo_traj_df** reads in the exocentric cameras calibrations in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85de0595-7140-467d-a10e-ea8156d25544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exo cameras:\t ['gp01', 'gp02', 'gp03', 'gp04', 'gp05']\n",
      "ego camera:\t aria01\n"
     ]
    }
   ],
   "source": [
    "# Initialize exo cameras from calibration file\n",
    "traj_dir = os.path.join(release_dir, take[\"root_dir\"], \"trajectory\")\n",
    "exo_traj_path = os.path.join(traj_dir, \"gopro_calibs.csv\")\n",
    "\n",
    "exo_traj_df = load_csv_to_df(exo_traj_path)\n",
    "exo_cam_names = list(exo_traj_df[\"cam_uid\"])\n",
    "ego_cam_names = [x[\"cam_id\"] for x in take[\"capture\"][\"cameras\"] if x[\"is_ego\"] and x[\"cam_id\"].startswith(\"aria\")]\n",
    "all_cams = ego_cam_names + exo_cam_names\n",
    "ego_cam_name = ego_cam_names[0]\n",
    "print(\"exo cameras:\\t\", exo_cam_names)\n",
    "print(\"ego camera:\\t\", ego_cam_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c1a4f-a5e0-44fe-b62d-5b0c6e3eac52",
   "metadata": {},
   "source": [
    "#### 2.1 load exocentric camera calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e20770-c863-4ed9-a79c-c2243bcd572e",
   "metadata": {},
   "source": [
    "The exocentric camera calibrations can be read using Project Aria Machine Perception Services (MPS). We store them with camera uid as keys in **go_pro_proxy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07ff208-4d0c-44c0-ba63-db6cf637b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #StaticCameraCalibration data: 5\n",
      "Camera: gp05 was not localized, ignoring this camera.\n"
     ]
    }
   ],
   "source": [
    "go_pro_proxy = {}\n",
    "static_calibrations = mps.read_static_camera_calibrations(exo_traj_path)\n",
    "for static_calibration in static_calibrations:\n",
    "    # assert the GoPro was correctly localized\n",
    "    if static_calibration.quality != 1.0:\n",
    "        print(f\"Camera: {static_calibration.camera_uid} was not localized, ignoring this camera.\")\n",
    "        continue\n",
    "    proxy = {}\n",
    "    proxy[\"name\"] = static_calibration.camera_uid\n",
    "    proxy[\"pose\"] = static_calibration.transform_world_cam\n",
    "    proxy[\"camera\"] = CameraCalibration(\n",
    "                            static_calibration.camera_uid,\n",
    "                            KANNALA_BRANDT_K3,\n",
    "                            static_calibration.intrinsics,\n",
    "                            static_calibration.transform_world_cam, # probably extrinsics\n",
    "                            static_calibration.width,\n",
    "                            static_calibration.height,\n",
    "                            None,\n",
    "                            math.pi,\n",
    "                            \"\")\n",
    "\n",
    "    go_pro_proxy[static_calibration.camera_uid] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4d054-5a39-4e47-85bd-bac18ee1d17e",
   "metadata": {},
   "source": [
    "#### 2.2 load egocentric camera calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca8e25-2ede-4670-bd56-5ad852485614",
   "metadata": {},
   "source": [
    "We read the egocentric camera intrinsics using VRS and camera extrinsics using MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49966086-1808-4d53-9cd1-55aad1a3a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;000;255m[ProgressLogger][INFO]: 2025-01-10 06:15:48: Opening /datasets01/egoexo4d/v2/takes/upenn_0721_Piano_2_5/aria01.vrs...\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;128;000m[MultiRecordFileReader][DEBUG]: Opened file '/datasets01/egoexo4d/v2/takes/upenn_0721_Piano_2_5/aria01.vrs' and assigned to reader #0\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 211-1/camera-et activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 214-1/camera-rgb activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 231-1/mic activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 247-1/baro0 activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[VrsDataProvider][WARNING]: Unsupported TimeSync mode: APP, ignoring.\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: Fail to activate streamId 286-1\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-1/camera-slam-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-2/camera-slam-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-1/imu-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-2/imu-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1203-1/mag0 activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[MpsDataPathsProvider][WARNING]: Hand tracking folder (/datasets01/egoexo4d/v2/takes/upenn_0721_Piano_2_5/hand_tracking) does not exist in MPS root folder, not loading wrist and palm poses.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "## Configure the VRSDataProvider (interface used to retrieve Trajectory data)\n",
    "ego_exo_project_path = os.path.join(release_dir, 'takes', take['take_name'])\n",
    "\n",
    "aria_dir = os.path.join(release_dir, take[\"root_dir\"])\n",
    "aria_path = os.path.join(aria_dir, f\"{ego_cam_name}.vrs\")\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(aria_path)\n",
    "device_calibration = vrs_data_provider.get_device_calibration()\n",
    "\n",
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\n",
    "rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ef662-d6c8-4244-9a54-cd7c508fd25a",
   "metadata": {},
   "source": [
    "### 3. Load body / hand pose and project it to exocentric views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af38d5a-41e0-4fa0-8764-e9d163ee6642",
   "metadata": {},
   "source": [
    "In this section, we go through the steps of projecting body / hand pose to exocentric views. We first load the annotation file of the pose. The annotation is a dictionary with the frame indices as keys. As an example, we randomly sample the 3D and 2D annotation of one frame. You can switch annotation_type to choose between body and hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625a014a-0775-4097-82f6-dbf2e44cba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation at sampled frame 1061 is dict_keys(['annotation3D', 'annotation2D']).\n"
     ]
    }
   ],
   "source": [
    "annotation_type = \"hand\" # annotation_type should be body or hand.\n",
    "\n",
    "# get body pose annotation folder\n",
    "egopose_ann_dir = os.path.join(\n",
    "    annotation_dir, f\"ego_pose/train/{annotation_type}/annotation\"\n",
    ")\n",
    "# get the annotation file of the sampled take\n",
    "annotation_file_path = os.path.join(egopose_ann_dir, f\"{take_uid}.json\")\n",
    "all_annotations = json.load(open(annotation_file_path))\n",
    "# annotation is a dictionary with frame numbers as keys, we then randomly sample one frame.\n",
    "frame_idx = random.sample(list(all_annotations.keys()), 1)[0]\n",
    "annotation = all_annotations[frame_idx][0]\n",
    "frame_idx = int(frame_idx)\n",
    "print(f\"annotation at sampled frame {frame_idx} is {annotation.keys()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353fc39-fd08-492f-9e47-38887f1ed672",
   "metadata": {},
   "source": [
    "Next we read the corresponding at the sampled frame index from exocentric videos and egocentric video. We store it in a dictionary **videos** with camera name as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59612e8-9a4d-46d3-86b9-56fa0016a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.join(release_dir, take[\"root_dir\"])\n",
    "videos = {}\n",
    "for cam_name in all_cams:\n",
    "    if cam_name in exo_cam_names:\n",
    "        stream_name = '0'\n",
    "    else:\n",
    "        stream_name = 'rgb'\n",
    "        \n",
    "    local_path = os.path.join(base_directory, take['frame_aligned_videos'][cam_name][stream_name]['relative_path'])\n",
    "    container = av.open(local_path) \n",
    "    videos[cam_name] = get_frame(local_path, frame_idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f9924-4275-4ddf-9d78-3e3673946f8f",
   "metadata": {},
   "source": [
    "We retrieve the pose in the format of 3D keypoints from the annotation file. 3D keypoints are world coordinates of the pose. Note that annotation also have **annotation2D** which are 2D keypoints annotated on undistorted frames. We will show this part later in projecting hand pose to egocentric frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fda12c2-95ec-43c4-9547-899cd9eec90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = list(annotation[\"annotation3D\"].keys())\n",
    "annot_3d = {}\n",
    "for part in parts:\n",
    "    keypoint = annotation[\"annotation3D\"][part]\n",
    "    annot_3d[part] = [keypoint['x'], keypoint['y'], keypoint['z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be90887-c98c-48fd-b7ea-2f6f2ecc2602",
   "metadata": {},
   "source": [
    "With exocentric camera calibration, we project the 3D body/hand keypoints to different exocentric views. The process is similar to the reprojection in the gaze tutorial. The body/hand keypoint is first projected from world coordinates to exocentric camera device, then to the exocentric camera image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d6e34b-528f-49af-a356-ed34f9dfe794",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_go_pro_reprojection = {}\n",
    "for go_pro in go_pro_proxy:\n",
    "    if go_pro not in per_go_pro_reprojection.keys():\n",
    "        per_go_pro_reprojection[go_pro] = {}\n",
    "    for part in parts:\n",
    "        pose_vector_in_world = annot_3d[part]\n",
    "        # project the keypoint from world to go_pro device\n",
    "        pose_in_go_pro_world = go_pro_proxy[go_pro][\"pose\"].inverse() @ pose_vector_in_world\n",
    "\n",
    "        # project the keypoint from go_pro device to go_pro image plane\n",
    "        device_projection = go_pro_proxy[go_pro][\"camera\"].project(pose_in_go_pro_world)\n",
    "        if device_projection is None:\n",
    "            continue\n",
    "        else:\n",
    "            per_go_pro_reprojection[go_pro][part] = {'x': device_projection[0], 'y': device_projection[1], 'placement': 'manual'}     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857a798-046a-48e8-ab2b-14e858becd7a",
   "metadata": {},
   "source": [
    "We define the keypoints_map, skeleton and pose_kpt_color for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc403a9-8346-4657-a1be-23a4a0aa5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "if annotation_type == \"body\":\n",
    "    keypoints_map, skeleton, pose_kpt_color = get_body_metadata()\n",
    "else:\n",
    "    keypoints_map, skeleton, pose_kpt_color = get_hands_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7018c971-ea7d-495f-a95a-5973ae755439",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_results = {}\n",
    "for cam_name in per_go_pro_reprojection.keys():\n",
    "    viz_img = get_viz(\n",
    "        videos[cam_name],\n",
    "        keypoints_map,\n",
    "        per_go_pro_reprojection[cam_name],\n",
    "        skeleton,\n",
    "        pose_kpt_color,\n",
    "        annot_type=annotation_type,\n",
    "    )\n",
    "    projection_results[cam_name] = viz_img\n",
    "# uncomment the following line to show the results\n",
    "#show_results(projection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad484fae-2110-40c9-ac17-0a789d66d570",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pose_tutorial-0.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d5be7-698e-4464-b436-4680a364162b",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pose_tutorial-1.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723c58e-132d-44cd-8ec3-fa6ab0001f6b",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pose_tutorial-2.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9a505-4f6e-45e0-bd92-222c811c392c",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pose_tutorial-3.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12581f17-8ae9-4691-bc89-1395f041e66a",
   "metadata": {},
   "source": [
    "### 3. Projecting pose to egocentric view using camera info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0a828-3344-48ee-8af0-42ce9f430fb9",
   "metadata": {},
   "source": [
    "In this section, we further provide instructions projecting hand pose to egocentric view, since hand-object interaction is an important part of egocentric video analysis. The projection includes the following steps:\n",
    "1. retrieve 3D pose of the hand keypoint as world coordinates from annotation\n",
    "2. retrieve camera intrinsics and extrinsics from camera_info\n",
    "3. project the 3D keypoint from world to aria device with camera extrinsics\n",
    "4. project the 3D keypoint from aria device to aria image plane with camera intrinsics\n",
    "5. get the resulted 2D keypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd406905-24d4-46d1-a7a8-0a8974b61bd2",
   "metadata": {},
   "source": [
    "To retrieve the pose of the aria glasses at the given timestamp, we get the sample_timestamp at the frame_idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118dbf3d-3cb8-4991-b374-b361d1dfd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "takes_info = json.load(open(os.path.join(release_dir, \"takes.json\")))\n",
    "\n",
    "# go through all takes to locate the sampled take, i.e., cmu_bike01_5\n",
    "take_name = take[\"take_name\"]\n",
    "import re\n",
    "\n",
    "# get the capture name, and load the timesync.csv\n",
    "capture_name = re.sub(r\"_\\d+$\", \"\", take_name)\n",
    "timesync = pd.read_csv(os.path.join(release_dir, f\"captures/{capture_name}/timesync.csv\"))\n",
    "\n",
    "start_idx = take[\"timesync_start_idx\"]+1\n",
    "end_idx = take[\"timesync_end_idx\"]\n",
    "take_timestamps = []\n",
    "for idx in range(start_idx, end_idx):\n",
    "    take_timestamps.append(int(timesync.iloc[idx][f\"aria01_214-1_capture_timestamp_ns\"]))\n",
    "sample_timestamp = take_timestamps[int(frame_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f563f1-efb6-4187-9159-6248bf149eb2",
   "metadata": {},
   "source": [
    "Then we can use the camera extrinsics at the sample_timestamp, and camera intrinsics to project the pose keypoints to the aria frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a187d8-197c-4999-90bd-c5e13c6aa7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #closed loop trajectory poses records: 646865\n"
     ]
    }
   ],
   "source": [
    "ego_reprojection = {}\n",
    "pose_info = mps_data_provider.get_closed_loop_pose(sample_timestamp)\n",
    "\n",
    "cam = ego_cam_name\n",
    "for part in parts:\n",
    "    pose_vector_in_world = annot_3d[part]\n",
    "\n",
    "    if pose_info:\n",
    "        # transform coordinates from device to world\n",
    "        T_world_device = pose_info.transform_world_device\n",
    "        \n",
    "    T_device_camera = rgb_camera_calibration.get_transform_device_camera()\n",
    "    T_world_camera = T_world_device @ T_device_camera\n",
    "\n",
    "    pose_in_aria_world = T_world_camera.inverse() @ pose_vector_in_world\n",
    "    device_projection = rgb_camera_calibration.project(pose_in_aria_world)\n",
    "\n",
    "    if device_projection is None:\n",
    "        continue\n",
    "    else:\n",
    "        x_coord = device_projection[0]\n",
    "        y_coord = device_projection[1]\n",
    "        ego_reprojection[part] = {'x': x_coord, 'y': y_coord, 'placement': 'auto'}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6958b-6dc0-44e4-b016-924036706502",
   "metadata": {},
   "source": [
    "Now let's visualize the projected hand pose on the aria frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df5c7599-c5bf-449a-b63f-d1f45570524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_local_path = os.path.join(base_directory, take['frame_aligned_videos'][ego_cam_name]['rgb']['relative_path'])  \n",
    "ego_frame = get_frame(ego_local_path, frame_idx) \n",
    "if annotation_type == 'hand':\n",
    "    cam_name = ego_cam_name\n",
    "    ann = ego_reprojection\n",
    "    img = ego_frame\n",
    "    img = img.rotate(90)\n",
    "    image_array = np.asarray(img)\n",
    "    image = Image.fromarray(image_array, \"RGB\")\n",
    "\n",
    "    viz_img = get_viz(image, keypoints_map, ann, skeleton, pose_kpt_color, annot_type=annotation_type, is_aria=True)\n",
    "    # uncomment the following line to show the results\n",
    "    #plt.figure(figsize=(8, 8))\n",
    "    #plt.imshow(viz_img.rotate(270))\n",
    "    #plt.axis(\"off\")  # Hide the axes ticks\n",
    "    #plt.title(f\"{cam_name}\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486d208-63a6-473c-92fe-f7d5c96fa3a6",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pose_tutorial-4.png\" width=600 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af18bf-d01f-42c9-ab69-5822c8cabca1",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0231cc-f029-4776-8be8-6adbc020c551",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed loading takes, its egocentric and exocentric camera calibrations and EgoPose annotations. We also provided a step-by-step guide on projecting 3D hand/body keypoints to different views with the camera calibrations. This can serve as a good starting point to understand EgoPose and utilize it for research in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e0a3f-0e22-40fd-80e3-e1dec4919812",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/facebookresearch/Ego4d)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/facebookresearch/Ego4d/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
