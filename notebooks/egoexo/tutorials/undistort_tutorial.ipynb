{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c1049a-ea49-4fd4-bce5-9a7097026a3f",
   "metadata": {},
   "source": [
    "# Tutorial 3: Undistort frames and overlay annotations\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
    "\n",
    "**Filled notebook:**\n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Tutorial&message=View%20On%20Github&color=lightgrey)](https://github.com/facebookresearch/Ego4d/tree/main/notebooks/egoexo)   \n",
    "**Author:** Xizi Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d88077-503d-4880-874b-4d725740dc8b",
   "metadata": {},
   "source": [
    "Cameras on the Aria glasses and GoPro cameras might have lenses that can cause distortions. For example, cameras on the Aria glasses all have fisheye lenses, and spherical camera model are much better approximations for these glasses. Some of the annotations in the EgoExo4D dataset are done on the undistorted frames. Thus, we need to undistort the frames first, then we could overlay the 2D annotations on the frames. In this tutorial, we provide a step-by-step guide on undistorting the egocentric and exocentric frames, and overlay the annotations on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907e631-cb4b-4505-948b-99f0ec89a921",
   "metadata": {},
   "source": [
    "### 1. Prerequisites and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037d895-a9d9-4b33-aa0f-d4a08e769bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import rcParams\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core import data_provider\n",
    "from projectaria_tools.core import calibration\n",
    "#from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3\n",
    "\n",
    "rcParams[\"figure.figsize\"] = 16, 32\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import av\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85328594-c924-406e-ba2d-19df8a17c1e9",
   "metadata": {},
   "source": [
    "Next we define some necessary utility functions for retrieving pose metadata and drawing functions. These are the same as the utility functions in the pose tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5314b7-bf81-43a9-a046-1a74a857cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataframe at target path to csv\n",
    "def load_csv_to_df(filepath: str) -> pd.DataFrame:\n",
    "    with open(filepath, \"r\") as csv_file:\n",
    "        return pd.read_csv(csv_file)\n",
    "\n",
    "# color palette for drawing the keypoints\n",
    "palette = np.array(\n",
    "    [\n",
    "        [255, 128, 0],\n",
    "        [255, 153, 51],\n",
    "        [255, 178, 102],\n",
    "        [230, 230, 0],\n",
    "        [255, 153, 255],\n",
    "        [153, 204, 255],\n",
    "        [255, 102, 255],\n",
    "        [255, 51, 255],\n",
    "        [102, 178, 255],\n",
    "        [51, 153, 255],\n",
    "        [255, 153, 153],\n",
    "        [255, 102, 102],\n",
    "        [255, 51, 51],\n",
    "        [153, 255, 153],\n",
    "        [102, 255, 102],\n",
    "        [51, 255, 51],\n",
    "        [0, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 0, 0],\n",
    "        [255, 255, 255],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# retrieve the body keypoints and skeleton\n",
    "def get_body_metadata():\n",
    "    keypoints_map = [\n",
    "        {\"label\": \"Nose\", \"id\": \"fee3cbd2\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Left-eye\", \"id\": \"ab12de34\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Right-eye\", \"id\": \"7f2g1h6k\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Left-ear\", \"id\": \"mn0pqrst\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Right-ear\", \"id\": \"yz89wx76\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Left-shoulder\", \"id\": \"5a4b3c2d\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Right-shoulder\", \"id\": \"e1f2g3h4\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left-elbow\", \"id\": \"6i7j8k9l\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right-elbow\", \"id\": \"uv0wxy12\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left-wrist\", \"id\": \"3z4ab5cd\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Right-wrist\", \"id\": \"efgh6789\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Left-hip\", \"id\": \"ijklmnop\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Right-hip\", \"id\": \"qrstuvwx\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Left-knee\", \"id\": \"yz012345\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Right-knee\", \"id\": \"6bc7defg\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left-ankle\", \"id\": \"hijk8lmn\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right-ankle\", \"id\": \"opqrs1tu\", \"color\": \"#f564d4\"},\n",
    "    ]\n",
    "\n",
    "    # pyre-ignore\n",
    "    pose_kpt_color = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "    skeleton = [\n",
    "        [16, 14],\n",
    "        [14, 12],\n",
    "        [17, 15],\n",
    "        [15, 13],\n",
    "        [12, 13],\n",
    "        [6, 12],\n",
    "        [7, 13],\n",
    "        [6, 7],\n",
    "        [6, 8],\n",
    "        [7, 9],\n",
    "        [8, 10],\n",
    "        [9, 11],\n",
    "        [2, 3],\n",
    "        [1, 2],\n",
    "        [1, 3],\n",
    "        [2, 4],\n",
    "        [3, 5],\n",
    "        [4, 6],\n",
    "        [5, 7],\n",
    "    ]\n",
    "    return keypoints_map, skeleton, pose_kpt_color\n",
    "\n",
    "\n",
    "# retrieve the hand keypoints and skeleton\n",
    "def get_hands_metadata():\n",
    "    keypoints_map = [\n",
    "        {\"label\": \"Right_Wrist\", \"id\": \"fee3cbd2\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Right_Thumb_1\", \"id\": \"yz012345\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Right_Thumb_2\", \"id\": \"6bc7defg\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Right_Thumb_3\", \"id\": \"hijk8lmn\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Thumb_4\", \"id\": \"opqrs1tu\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Right_Index_1\", \"id\": \"ab12de34\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Right_Index_2\", \"id\": \"7f2g1h6k\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Right_Index_3\", \"id\": \"mn0pqrst\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Right_Index_4\", \"id\": \"9vwxyzab\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Right_Middle_1\", \"id\": \"yz89wx76\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Right_Middle_2\", \"id\": \"5a4b3c2d\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Right_Middle_3\", \"id\": \"e1f2g3h4\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Middle_4\", \"id\": \"cdefgh23\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Right_Ring_1\", \"id\": \"efgh6789\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Right_Ring_2\", \"id\": \"ijklmnop\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Right_Ring_3\", \"id\": \"qrstuvwx\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Right_Ring_4\", \"id\": \"ijkl4567\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right_Pinky_1\", \"id\": \"6i7j8k9l\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Right_Pinky_2\", \"id\": \"uv0wxy12\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Right_Pinky_3\", \"id\": \"3z4ab5cd\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Right_Pinky_4\", \"id\": \"mnop8qrs\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Wrist\", \"id\": \"fee3cbd2_left\", \"color\": \"#f77189\"},\n",
    "        {\"label\": \"Left_Thumb_1\", \"id\": \"yz012345_left\", \"color\": \"#83f483\"},\n",
    "        {\"label\": \"Left_Thumb_2\", \"id\": \"6bc7defg_left\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left_Thumb_3\", \"id\": \"hijk8lmn_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Thumb_4\", \"id\": \"opqrs1tu_left\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Index_1\", \"id\": \"ab12de34_left\", \"color\": \"#d58c32\"},\n",
    "        {\"label\": \"Left_Index_2\", \"id\": \"7f2g1h6k_left\", \"color\": \"#a4a031\"},\n",
    "        {\"label\": \"Left_Index_3\", \"id\": \"mn0pqrst_left\", \"color\": \"#50b131\"},\n",
    "        {\"label\": \"Left_Index_4\", \"id\": \"9vwxyzab_left\", \"color\": \"#32d58c\"},\n",
    "        {\"label\": \"Left_Middle_1\", \"id\": \"yz89wx76_left\", \"color\": \"#34ae91\"},\n",
    "        {\"label\": \"Left_Middle_2\", \"id\": \"5a4b3c2d_left\", \"color\": \"#37abb5\"},\n",
    "        {\"label\": \"Left_Middle_3\", \"id\": \"e1f2g3h4_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Middle_4\", \"id\": \"cdefgh23_left\", \"color\": \"#3ba3ec\"},\n",
    "        {\"label\": \"Left_Ring_1\", \"id\": \"efgh6789_left\", \"color\": \"#94d14f\"},\n",
    "        {\"label\": \"Left_Ring_2\", \"id\": \"ijklmnop_left\", \"color\": \"#b3d32c\"},\n",
    "        {\"label\": \"Left_Ring_3\", \"id\": \"qrstuvwx_left\", \"color\": \"#f9b530\"},\n",
    "        {\"label\": \"Left_Ring_4\", \"id\": \"ijkl4567_left\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Left_Pinky_1\", \"id\": \"6i7j8k9l_left\", \"color\": \"#bb83f4\"},\n",
    "        {\"label\": \"Left_Pinky_2\", \"id\": \"uv0wxy12_left\", \"color\": \"#f564d4\"},\n",
    "        {\"label\": \"Left_Pinky_3\", \"id\": \"3z4ab5cd_left\", \"color\": \"#2fd4aa\"},\n",
    "        {\"label\": \"Left_Pinky_4\", \"id\": \"mnop8qrs_left\", \"color\": \"#f564d4\"},\n",
    "    ]\n",
    "\n",
    "    links = {\n",
    "        \"fee3cbd2\": [\"ab12de34\", \"yz89wx76\", \"6i7j8k9l\", \"efgh6789\", \"yz012345\"],\n",
    "        \"ab12de34\": [\"7f2g1h6k\"],\n",
    "        \"7f2g1h6k\": [\"mn0pqrst\"],\n",
    "        \"mn0pqrst\": [\"9vwxyzab\"],\n",
    "        \"yz89wx76\": [\"5a4b3c2d\"],\n",
    "        \"5a4b3c2d\": [\"e1f2g3h4\"],\n",
    "        \"e1f2g3h4\": [\"cdefgh23\"],\n",
    "        \"6i7j8k9l\": [\"uv0wxy12\"],\n",
    "        \"uv0wxy12\": [\"3z4ab5cd\"],\n",
    "        \"3z4ab5cd\": [\"mnop8qrs\"],\n",
    "        \"efgh6789\": [\"ijklmnop\"],\n",
    "        \"ijklmnop\": [\"qrstuvwx\"],\n",
    "        \"qrstuvwx\": [\"ijkl4567\"],\n",
    "        \"yz012345\": [\"6bc7defg\"],\n",
    "        \"6bc7defg\": [\"hijk8lmn\"],\n",
    "        \"hijk8lmn\": [\"opqrs1tu\"],\n",
    "        \"fee3cbd2_left\": [\n",
    "            \"ab12de34_left\",\n",
    "            \"yz89wx76_left\",\n",
    "            \"6i7j8k9l_left\",\n",
    "            \"efgh6789_left\",\n",
    "            \"yz012345_left\",\n",
    "        ],\n",
    "        \"ab12de34_left\": [\"7f2g1h6k_left\"],\n",
    "        \"7f2g1h6k_left\": [\"mn0pqrst_left\"],\n",
    "        \"mn0pqrst_left\": [\"9vwxyzab_left\"],\n",
    "        \"yz89wx76_left\": [\"5a4b3c2d_left\"],\n",
    "        \"5a4b3c2d_left\": [\"e1f2g3h4_left\"],\n",
    "        \"e1f2g3h4_left\": [\"cdefgh23_left\"],\n",
    "        \"6i7j8k9l_left\": [\"uv0wxy12_left\"],\n",
    "        \"uv0wxy12_left\": [\"3z4ab5cd_left\"],\n",
    "        \"3z4ab5cd_left\": [\"mnop8qrs_left\"],\n",
    "        \"efgh6789_left\": [\"ijklmnop_left\"],\n",
    "        \"ijklmnop_left\": [\"qrstuvwx_left\"],\n",
    "        \"qrstuvwx_left\": [\"ijkl4567_left\"],\n",
    "        \"yz012345_left\": [\"6bc7defg_left\"],\n",
    "        \"6bc7defg_left\": [\"hijk8lmn_left\"],\n",
    "        \"hijk8lmn_left\": [\"opqrs1tu_left\"],\n",
    "    }\n",
    "\n",
    "    hand_dict = dict()\n",
    "    for index, kpt in enumerate(keypoints_map):\n",
    "        hand_dict[kpt[\"id\"]] = index + 1\n",
    "\n",
    "    skeleton = list()\n",
    "    for start_point in links:\n",
    "        end_points = links[start_point]\n",
    "        for end_point in end_points:\n",
    "            start_index = hand_dict[start_point]\n",
    "            end_index = hand_dict[end_point]\n",
    "            skeleton.append([start_index, end_index])\n",
    "\n",
    "    klist = [0]\n",
    "    klist.extend([2] * 4)\n",
    "    klist.extend([4] * 4)\n",
    "    klist.extend([6] * 4)\n",
    "    klist.extend([8] * 4)\n",
    "    klist.extend([10] * 4)\n",
    "    klist.extend([0])\n",
    "    klist.extend([2] * 4)\n",
    "    klist.extend([4] * 4)\n",
    "    klist.extend([6] * 4)\n",
    "    klist.extend([8] * 4)\n",
    "    klist.extend([10] * 4)\n",
    "\n",
    "    pose_kpt_color = palette[klist]\n",
    "    return keypoints_map, skeleton, pose_kpt_color\n",
    "    \n",
    "def get_coords(annot):\n",
    "    pts = dict()\n",
    "    for k in annot:\n",
    "        atype = 1\n",
    "        if annot[k][\"placement\"] == \"auto\":\n",
    "            atype = 0\n",
    "        pts[k] = [annot[k][\"x\"], annot[k][\"y\"], atype]\n",
    "    return pts\n",
    "\n",
    "\n",
    "def draw_skeleton(img, all_pts, skeleton):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for item in skeleton:\n",
    "        left_index = item[0] - 1\n",
    "        right_index = item[1] - 1\n",
    "        left_pt = all_pts[left_index]\n",
    "        right_pt = all_pts[right_index]\n",
    "        if len(left_pt) == 0 or len(right_pt) == 0:\n",
    "            continue\n",
    "        draw.line([left_pt, right_pt], fill=\"white\", width=10)\n",
    "\n",
    "\n",
    "def draw_cross(img, x, y, color):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the cross\n",
    "    cross_length = 10  # Half-length of the cross arms\n",
    "    # Calculate the end points of the cross\n",
    "    left_point = (center[0] - cross_length, center[1])\n",
    "    right_point = (center[0] + cross_length, center[1])\n",
    "    top_point = (center[0], center[1] - cross_length)\n",
    "    bottom_point = (center[0], center[1] + cross_length)\n",
    "\n",
    "    # Draw the horizontal line\n",
    "    draw.line([left_point, right_point], fill=color, width=3)\n",
    "    # Draw the vertical line\n",
    "    draw.line([top_point, bottom_point], fill=color, width=3)\n",
    "\n",
    "\n",
    "def draw_circle(img, x, y, color):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the circle\n",
    "    radius = 12  # Radius of the circle\n",
    "\n",
    "    # Calculate the bounding box of the circle\n",
    "    left_up_point = (center[0] - radius, center[1] - radius)\n",
    "    right_down_point = (center[0] + radius, center[1] + radius)\n",
    "\n",
    "    # Draw the circle with a black outline\n",
    "    draw.ellipse(\n",
    "        [left_up_point, right_down_point], outline=(255, 255, 255), fill=color, width=6\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_label(img, x, y, color, label):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default(size=40)\n",
    "    # Circle parameters\n",
    "    center = (x + 20, y - 20)  # Center of the circle\n",
    "    draw.text(center, label, fill=color, font=font)\n",
    "\n",
    "\n",
    "def draw_skeleton_hands(img, all_pts, skeleton, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for item in skeleton:\n",
    "        left_index = item[0] - 1\n",
    "        right_index = item[1] - 1\n",
    "        left_pt = all_pts[left_index]\n",
    "        right_pt = all_pts[right_index]\n",
    "        if len(left_pt) == 0 or len(right_pt) == 0:\n",
    "            continue\n",
    "        draw.line([left_pt, right_pt], fill=\"white\", width=int(ratio * 4))\n",
    "\n",
    "\n",
    "def draw_circle_hands(img, x, y, color, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the circle\n",
    "    radius = int(ratio * 8)  # Radius of the circle\n",
    "\n",
    "    # Calculate the bounding box of the circle\n",
    "    left_up_point = (center[0] - radius, center[1] - radius)\n",
    "    right_down_point = (center[0] + radius, center[1] + radius)\n",
    "\n",
    "    # Draw the circle with a black outline\n",
    "    draw.ellipse(\n",
    "        [left_up_point, right_down_point],\n",
    "        outline=(255, 255, 255),\n",
    "        fill=color,\n",
    "        width=int(ratio * 4),\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_cross_hands(img, x, y, color, ratio=1):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # Circle parameters\n",
    "    center = (x, y)  # Center of the cross\n",
    "    cross_length = int(ratio * 8)  # Half-length of the cross arms\n",
    "    # Calculate the end points of the cross\n",
    "    left_point = (center[0] - cross_length, center[1])\n",
    "    right_point = (center[0] + cross_length, center[1])\n",
    "    top_point = (center[0], center[1] - cross_length)\n",
    "    bottom_point = (center[0], center[1] + cross_length)\n",
    "\n",
    "    # Draw the horizontal line\n",
    "    draw.line([left_point, right_point], fill=color, width=int(ratio * 4))\n",
    "    # Draw the vertical line\n",
    "    draw.line([top_point, bottom_point], fill=color, width=int(ratio * 4))\n",
    "\n",
    "\n",
    "def show_results(results):\n",
    "    for cam_name in results:\n",
    "        img = results[cam_name]\n",
    "        plt.figure(figsize=(40, 20))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")  # Hide the axes ticks\n",
    "        plt.title(f\"{cam_name}\", fontsize=20)\n",
    "        plt.savefig(f\"{cam_name}.png\")\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "def get_viz(\n",
    "    pil_img,\n",
    "    keypoints_map,\n",
    "    ann,\n",
    "    skeleton,\n",
    "    pose_kpt_color,\n",
    "    annot_type=\"body\",\n",
    "    is_aria=False,\n",
    "):\n",
    "    pts = get_coords(ann)\n",
    "    ratio = 1\n",
    "    if is_aria:\n",
    "        ratio = 0.5\n",
    "\n",
    "    all_pts = []\n",
    "    for _, keypoints in enumerate(keypoints_map):\n",
    "        kpname = keypoints[\"label\"].lower()\n",
    "\n",
    "        if kpname in pts:\n",
    "            x, y = pts[kpname][0], pts[kpname][1]\n",
    "            all_pts.append((x, y))\n",
    "        else:\n",
    "            all_pts.append(())\n",
    "\n",
    "    if annot_type == \"body\":\n",
    "        draw_skeleton(pil_img, all_pts, skeleton)\n",
    "    else:\n",
    "        draw_skeleton_hands(pil_img, all_pts, skeleton, ratio)\n",
    "\n",
    "    for index, keypoints in enumerate(keypoints_map):\n",
    "        kpname = keypoints[\"label\"].lower()\n",
    "        if kpname in pts:\n",
    "            x, y, pt_type = pts[kpname][0], pts[kpname][1], pts[kpname][2]\n",
    "            color = tuple(pose_kpt_color[index])\n",
    "            if pt_type == 1:\n",
    "                if annot_type == \"body\":\n",
    "                    draw_circle(pil_img, x, y, color)\n",
    "                else:\n",
    "                    draw_circle_hands(pil_img, x, y, color, ratio)\n",
    "            else:\n",
    "                if annot_type == \"body\":\n",
    "                    draw_cross(pil_img, x, y, color)\n",
    "                else:\n",
    "                    draw_cross_hands(pil_img, x, y, color, ratio)\n",
    "            if annot_type == \"body\":\n",
    "                draw_label(pil_img, x, y, color, kpname)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    return pil_img\n",
    "\n",
    "\n",
    "# Video Utilities\n",
    "def get_frame(video_local_path, frame_idx):\n",
    "    container = av.open(video_local_path)\n",
    "    frame_count = 0\n",
    "    for frame in container.decode(video=0):\n",
    "        if frame_count == frame_idx:\n",
    "            input_img = np.array(frame.to_image())\n",
    "            pil_img = Image.fromarray(input_img)\n",
    "            return pil_img\n",
    "        frame_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e0b70-0eaf-4b91-b3e7-dfa159bf6a97",
   "metadata": {},
   "source": [
    "### 2. Undistortion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21547751-86a7-44c9-afdb-2c9af8d7590c",
   "metadata": {},
   "source": [
    "#### 2.1 Aria Undistortion Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14113ae-c627-47b4-9875-5bb6dd7df99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_aria(image_array, provider, sensor_name, focal_length, size):\n",
    "    device_calib = provider.get_device_calibration()\n",
    "    src_calib = device_calib.get_camera_calib(sensor_name)\n",
    "\n",
    "    # create output calibration: a linear model of image size 512x512 and focal length 150\n",
    "    # Invisible pixels are shown as black.\n",
    "    dst_calib = calibration.get_linear_camera_calibration(\n",
    "        size, size, focal_length, sensor_name\n",
    "    )\n",
    "\n",
    "    # distort image\n",
    "    rectified_array = calibration.distort_by_calibration(\n",
    "        image_array, dst_calib, src_calib\n",
    "    )\n",
    "    return (\n",
    "        rectified_array,\n",
    "        dst_calib.get_principal_point(),\n",
    "        dst_calib.get_focal_lengths(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e30c50-9a96-4bf8-a0b7-1acad4189cfa",
   "metadata": {},
   "source": [
    "#### 2.2 GoPro Undistortion Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa81a87-8621-436c-ad0f-305469819505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_exocam(image, intrinsics, distortion_coeffs, dimension=(3840, 2160)):\n",
    "    DIM = dimension\n",
    "    dim2 = None\n",
    "    dim3 = None\n",
    "    balance = 0.8\n",
    "    # Load the distortion parameters\n",
    "    distortion_coeffs = distortion_coeffs\n",
    "    # Load the camera intrinsic parameters\n",
    "    intrinsics = intrinsics\n",
    "\n",
    "    dim1 = image.shape[:2][::-1]  # dim1 is the dimension of input image to un-distort\n",
    "\n",
    "    # Change the calibration dim dynamically (bouldering cam01 and cam04 are verticall for examples)\n",
    "    if DIM[0] != dim1[0]:\n",
    "        DIM = (DIM[1], DIM[0])\n",
    "\n",
    "    assert (\n",
    "        dim1[0] / dim1[1] == DIM[0] / DIM[1]\n",
    "    ), \"Image to undistort needs to have same aspect ratio as the ones used in calibration\"\n",
    "    if not dim2:\n",
    "        dim2 = dim1\n",
    "    if not dim3:\n",
    "        dim3 = dim1\n",
    "    scaled_K = (\n",
    "        intrinsics * dim1[0] / DIM[0]\n",
    "    )  # The values of K is to scale with image dimension.\n",
    "    scaled_K[2][2] = 1.0  # Except that K[2][2] is always 1.0\n",
    "\n",
    "    # This is how scaled_K, dim2 and balance are used to determine the final K used to un-distort image. OpenCV document failed to make this clear!\n",
    "    new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(\n",
    "        scaled_K, distortion_coeffs, dim2, np.eye(3), balance=balance\n",
    "    )\n",
    "    map1, map2 = cv2.fisheye.initUndistortRectifyMap(\n",
    "        scaled_K, distortion_coeffs, np.eye(3), new_K, dim3, cv2.CV_16SC2\n",
    "    )\n",
    "    undistorted_image = cv2.remap(\n",
    "        image,\n",
    "        map1,\n",
    "        map2,\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "    )\n",
    "\n",
    "    return undistorted_image, new_K\n",
    "\n",
    "\n",
    "def get_distortion_and_intrinsics(_raw_camera):\n",
    "    intrinsics = np.array(\n",
    "        [\n",
    "            [_raw_camera[\"intrinsics_0\"], 0, _raw_camera[\"intrinsics_2\"]],\n",
    "            [0, _raw_camera[\"intrinsics_1\"], _raw_camera[\"intrinsics_3\"]],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "    distortion_coeffs = np.array(\n",
    "        [\n",
    "            _raw_camera[\"intrinsics_4\"],\n",
    "            _raw_camera[\"intrinsics_5\"],\n",
    "            _raw_camera[\"intrinsics_6\"],\n",
    "            _raw_camera[\"intrinsics_7\"],\n",
    "        ]\n",
    "    )\n",
    "    return distortion_coeffs, intrinsics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e0499-fa88-46a4-96a2-12c23cd540ad",
   "metadata": {},
   "source": [
    "### 3. Overlay 2D annotations on undistorted frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d451f-4672-40e8-a4cc-17d7566bdbb3",
   "metadata": {},
   "source": [
    "First, we locate the download folder of the EgoExo4D dataset and load necessary annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a8ae18-9270-4abd-80f6-7ba6e236ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dir = \"/datasets01/egoexo4d/v2/\" # replace this with your download folder\n",
    "annotation_dir = os.path.join(release_dir, \"annotations/\")  # annotation folder\n",
    "\n",
    "# load necessary annotation files\n",
    "egoexo = {\n",
    "    \"takes\": os.path.join(release_dir, \"takes.json\"),\n",
    "    \"captures\": os.path.join(release_dir, \"captures.json\")\n",
    "}\n",
    "\n",
    "for k, v in egoexo.items():\n",
    "    egoexo[k] = json.load(open(v))\n",
    "\n",
    "takes = egoexo[\"takes\"]\n",
    "captures = egoexo[\"captures\"]\n",
    "takes_by_uid = {x[\"take_uid\"]: x for x in takes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89797566-8b79-423f-8b12-45cc3bf73438",
   "metadata": {},
   "source": [
    "We then randomly sample one example take of playing violin. We also provide the take uid of some other takes as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9ee34c-e8ac-46ac-a7fa-c22fd7f556aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "take_uid = \"0bc47e29-e086-4726-b874-f89671366f06\"  # Violin\n",
    "#take_uid = \"23ff1c48-01ea-4d34-a38b-bc96e767b9b9\" #Piano\n",
    "#take_uid = \"02715c86-e30c-4791-92b7-38b488e51aba\"  # Bike\n",
    "\n",
    "take = [take for take in egoexo[\"takes\"] if take[\"take_uid\"] == take_uid]\n",
    "take = take[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ded14-dcbf-4b93-a84f-93352fccaff6",
   "metadata": {},
   "source": [
    "And load the camera intrinsics and extrinsics of the take. **exo_traj_df** reads in the exocentric cameras calibrations in csv format. The camera calibrations can be used for frame undistortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85de0595-7140-467d-a10e-ea8156d25544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize exo cameras from calibration file\n",
    "traj_dir = os.path.join(release_dir, take[\"root_dir\"], \"trajectory\")\n",
    "exo_traj_path = os.path.join(traj_dir, \"gopro_calibs.csv\")\n",
    "\n",
    "exo_traj_df = load_csv_to_df(exo_traj_path)\n",
    "exo_cam_names = list(exo_traj_df[\"cam_uid\"])\n",
    "ego_cam_names = [x[\"cam_id\"] for x in take[\"capture\"][\"cameras\"] if x[\"is_ego\"] and x[\"cam_id\"].startswith(\"aria\")]\n",
    "all_cams = ego_cam_names + exo_cam_names\n",
    "ego_cam_name = ego_cam_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51b798-802c-41ac-9d12-a74d02af9371",
   "metadata": {},
   "source": [
    "We also configure the VRS data provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49966086-1808-4d53-9cd1-55aad1a3a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;000;255m[ProgressLogger][INFO]: 2025-01-10 21:56:19: Opening /datasets01/egoexo4d/v2/takes/upenn_0722_Violin_1_3/aria01.vrs...\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;128;000m[MultiRecordFileReader][DEBUG]: Opened file '/datasets01/egoexo4d/v2/takes/upenn_0722_Violin_1_3/aria01.vrs' and assigned to reader #0\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 211-1/camera-et activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 214-1/camera-rgb activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 231-1/mic activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 247-1/baro0 activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[VrsDataProvider][WARNING]: Unsupported TimeSync mode: APP, ignoring.\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: Fail to activate streamId 286-1\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-1/camera-slam-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-2/camera-slam-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-1/imu-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-2/imu-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1203-1/mag0 activated\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "## Configure the VRSDataProvider (interface used to retrieve Trajectory data)\n",
    "ego_exo_project_path = os.path.join(release_dir, 'takes', take['take_name'])\n",
    "\n",
    "aria_dir = os.path.join(release_dir, take[\"root_dir\"])\n",
    "aria_path = os.path.join(aria_dir, f\"{ego_cam_name}.vrs\")\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(aria_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25311161-7e0a-40a3-bba7-7559e2831487",
   "metadata": {},
   "source": [
    "#### 3.1 load 2D hand pose annotation and overlay it on undistorted egocentric frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699a9192-7e3c-45c1-80c1-95fec4ad2671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation at sampled frame 976 is dict_keys(['annotation3D', 'annotation2D']).\n"
     ]
    }
   ],
   "source": [
    "annotation_type = \"hand\" # annotation_type should be body or hand.\n",
    "\n",
    "# get hands meta data\n",
    "keypoints_map, skeleton, pose_kpt_color = get_hands_metadata()\n",
    "\n",
    "# get hand pose annotation folder\n",
    "egopose_ann_dir = os.path.join(\n",
    "    annotation_dir, f\"ego_pose/train/{annotation_type}/annotation\"\n",
    ")\n",
    "# get the annotation file of the sampled take\n",
    "annotation_file_path = os.path.join(egopose_ann_dir, f\"{take_uid}.json\")\n",
    "all_annotations = json.load(open(annotation_file_path))\n",
    "# annotation is a dictionary with frame numbers as keys, we then randomly sample one frame.\n",
    "frame_idx = random.sample(list(all_annotations.keys()), 1)[0]\n",
    "annotation = all_annotations[frame_idx][0]\n",
    "frame_idx = int(frame_idx)\n",
    "print(f\"annotation at sampled frame {frame_idx} is {annotation.keys()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353fc39-fd08-492f-9e47-38887f1ed672",
   "metadata": {},
   "source": [
    "Next we read the corresponding at the sampled frame index from exocentric videos and egocentric video. We store it in a dictionary **videos** with camera name as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b59612e8-9a4d-46d3-86b9-56fa0016a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.join(release_dir, take[\"root_dir\"])\n",
    "videos = {}\n",
    "for cam_name in all_cams:\n",
    "    if cam_name in exo_cam_names:\n",
    "        stream_name = '0'\n",
    "    else:\n",
    "        stream_name = 'rgb'\n",
    "        \n",
    "    local_path = os.path.join(base_directory, take['frame_aligned_videos'][cam_name][stream_name]['relative_path'])  \n",
    "    videos[cam_name] = get_frame(local_path, frame_idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6958b-6dc0-44e4-b016-924036706502",
   "metadata": {},
   "source": [
    "Now let's visualize the 2D annotations on the undistorted aria frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4cd1634-6df8-4fa8-bc2d-e827533e7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_local_path = os.path.join(base_directory, take['frame_aligned_videos'][ego_cam_name]['rgb']['relative_path'])  \n",
    "ego_frame = videos[ego_cam_name]\n",
    "\n",
    "cam_name = ego_cam_name\n",
    "ann = annotation[\"annotation2D\"][ego_cam_name]\n",
    "img = ego_frame\n",
    "img = img.rotate(90)\n",
    "image_array = np.asarray(img)\n",
    "rectified_array, principal_points, focal_lengths = undistort_aria(image_array, vrs_data_provider, \"camera-rgb\", 150, 512)\n",
    "undistorted_frame = Image.fromarray(rectified_array, \"RGB\")\n",
    "\n",
    "viz_img = get_viz(undistorted_frame, keypoints_map, ann, skeleton, pose_kpt_color, annot_type=annotation_type, is_aria=True)\n",
    "# uncomment the following lines to show the result\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(viz_img.rotate(270))\n",
    "# plt.axis(\"off\")  # Hide the axes ticks\n",
    "# plt.title(f\"{cam_name}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd96fe-0e74-43e5-a6e8-ac0d59847ec2",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-aria01.png\" width=400 height=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc851cc7-3584-4a8d-a8f8-aeedc8eec8ea",
   "metadata": {},
   "source": [
    "#### 3.2 load 2D body pose annotation and overlay it on undistorted exocentric frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfa0fd21-feac-4f22-bf65-45c8585b82f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation at sampled frame 976 is dict_keys(['annotation3D', 'annotation2D']).\n"
     ]
    }
   ],
   "source": [
    "annotation_type = \"body\"\n",
    "keypoints_map, skeleton, pose_kpt_color = get_body_metadata()\n",
    "\n",
    "# get body pose annotation folder\n",
    "egopose_ann_dir = os.path.join(\n",
    "    annotation_dir, f\"ego_pose/train/{annotation_type}/annotation\"\n",
    ")\n",
    "# get the annotation file of the sampled take\n",
    "annotation_file_path = os.path.join(egopose_ann_dir, f\"{take_uid}.json\")\n",
    "all_annotations = json.load(open(annotation_file_path))\n",
    "annotation = all_annotations[str(frame_idx)][0]\n",
    "frame_idx = int(frame_idx)\n",
    "print(f\"annotation at sampled frame {frame_idx} is {annotation.keys()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4e08e-72e2-4772-a1d5-28c5b73ba7b6",
   "metadata": {},
   "source": [
    "We load the frames from all exocentric cameras, and overlay the body pose annotation on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e532e08-ba9e-4207-ba0a-6efdcae0dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for cam_name in exo_cam_names:\n",
    "    annot_2d = annotation[\"annotation2D\"]\n",
    "    if cam_name not in annot_2d:\n",
    "        continue\n",
    "    calib_df = exo_traj_df[exo_traj_df.cam_uid == cam_name]\n",
    "    calib_df = calib_df.iloc[0].to_dict()\n",
    "    D, I = get_distortion_and_intrinsics(calib_df)\n",
    "    ann = annot_2d[cam_name]\n",
    "    img = videos[cam_name]\n",
    "    undistorted_frame, new_K_latest = undistort_exocam(np.array(img), I, D)\n",
    "    viz_img = get_viz(\n",
    "        Image.fromarray(undistorted_frame),\n",
    "        keypoints_map,\n",
    "        ann,\n",
    "        skeleton,\n",
    "        pose_kpt_color,\n",
    "        annot_type=annotation_type,\n",
    "    )\n",
    "    results[cam_name] = viz_img\n",
    "# uncomment the following line to display the results    \n",
    "# show_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb6242-5406-4dc7-bfa8-681c00ff115a",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-gp01.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17a079-e34c-480d-9cdb-91b032d428e0",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-gp02.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382cba27-1aa5-48ba-9c3b-9c205fd436ba",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-gp03.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadff9ff-2de3-48da-a2a4-e62de655fec3",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-gp04.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10673f-54b5-411e-aef8-e1642c3eeb49",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/undistort-gp06.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af18bf-d01f-42c9-ab69-5822c8cabca1",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0231cc-f029-4776-8be8-6adbc020c551",
   "metadata": {},
   "source": [
    "In this notebook, we introduce how to undistort the frames from aria and gopro cameras. Instead of projecting 3D pose annotations on the camera frames, we load the 2D hand and body annotations, and overlay them on the undistorted frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e0a3f-0e22-40fd-80e3-e1dec4919812",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/facebookresearch/Ego4d)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/facebookresearch/Ego4d/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
