{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c1049a-ea49-4fd4-bce5-9a7097026a3f",
   "metadata": {},
   "source": [
    "# Tutorial 1: Gaze in Ego-Exo4D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d88077-503d-4880-874b-4d725740dc8b",
   "metadata": {},
   "source": [
    "In this tutorial, we provide an introduction of the gaze data in Ego-Exo4D dataset, and a step-by-step guide on how to project 3D eye gaze to 2D in Egocentric/Exocentric view. \n",
    "\n",
    "Eye gaze is one of the 3D spatial signals provided by Ego-Exo4D dataset, which is pre-computed by Project Aria’s machine perception service (MPS). The gaze direction of the user is estimated as a single outward-facing ray anchored in-between the wearer’s eyes. Left and right eye gaze directions (yaw values) along with the depth at which these gaze directions intersect (translation values) are provided in the dataset. The convergence points and distances are derived from the predicted gaze directions. The combined direction’s yaw is used to populate the yaw field of the EyeGaze object for backwards compatibility. The pitch is common to left, right and combined gaze directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0de40-d7e2-4032-bb0e-1491e18172d1",
   "metadata": {},
   "source": [
    "<img src=\"eye_gaze_032024_model.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b66df-78e0-4e5a-8bbc-4d0fa781487b",
   "metadata": {},
   "source": [
    "Eye gaze data is located in each captures' or takes' eye_gaze folder. You can determine if a take or capture has eye gaze data by using `has_gaze` for a capture in <b>captures.json</b> and `has_trimmed_eye_gaze` for a take in <b>takes.json</b>. If eye gaze is available for the take, it is trimmed (cropped in time) with respect to that take. You will find three Eye Gaze MPS file outputs under the take folder: \n",
    "- `summary.json` - high level report on MPS eye gaze generation\n",
    "- `general_eye_gaze.csv` - based on the standard eye gaze configuration\n",
    "- `personalized_eye_gaze.csv` - only if the recording is made with <a href=\"https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps/eye_gaze_calibration\">in-session Eye Gaze Calibration</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f675b1-d2ac-4104-8a13-afcb38ce8964",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5582cb-23bc-452d-bba4-ea0223d7dbf3",
   "metadata": {},
   "source": [
    "#### Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e0633-f3fd-46bd-9f50-2f3d652a6ed5",
   "metadata": {},
   "source": [
    "Before we get started, we need to install necessary libraries. We will install the Python Package for Project Aria Tools. Please check <a href=\"https://facebookresearch.github.io/projectaria_tools/docs/data_utilities/installation/installation_python\">Installation guide</a> for detailed instructions. We also need to install the rerun library for logging and data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee88d29-3f4e-4c0c-8de1-e137657011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install rerun\n",
    "!pip install rerun-notebook==0.19.0\n",
    "!pip install rerun-sdk==0.19.0\n",
    "!pip install rerun-sdk[notebook]\n",
    "# install project aria tools\n",
    "!cd ~/projectaria_tools_python_env # Replace with your projectaria_tools_python_env folder\n",
    "!python3 -m pip install --upgrade pip\n",
    "!python3 -m pip install projectaria-tools'[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1d79a-7684-467a-a906-93c8bfaf248e",
   "metadata": {},
   "source": [
    "We use Rerun to display temporal and interactive data. The following are some functions that we will use all across the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8dda4a-c15d-4de1-866c-2494210f9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "import numpy as np\n",
    "\n",
    "from projectaria_tools.core.calibration import CameraCalibration, DeviceCalibration\n",
    "from projectaria_tools.core.sophus import SE3\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core.mps.utils import get_gaze_vector_reprojection\n",
    "from projectaria_tools.core import data_provider\n",
    "from projectaria_tools.utils.rerun_helpers import AriaGlassesOutline, ToTransform3D\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def log_aria_glasses(\n",
    "    device_calibration: DeviceCalibration,\n",
    "    label: str,\n",
    "    use_cad_calibration: bool = True\n",
    ") -> None:\n",
    "    ## Plot Project Aria Glasses outline (as lines)\n",
    "    aria_glasses_point_outline = AriaGlassesOutline(\n",
    "        device_calibration, use_cad_calibration\n",
    "    )\n",
    "    rr.log(label, rr.LineStrips3D([aria_glasses_point_outline]), timeless=True)\n",
    "\n",
    "def log_calibration(\n",
    "    camera_calibration: CameraCalibration,\n",
    "    label: str\n",
    ") -> None:\n",
    "    rr.log(\n",
    "        label,\n",
    "        rr.Pinhole(\n",
    "            resolution=[\n",
    "                camera_calibration.get_image_size()[0],\n",
    "                camera_calibration.get_image_size()[1],\n",
    "            ],\n",
    "            focal_length=float(\n",
    "                camera_calibration.get_focal_lengths()[0]\n",
    "            ),\n",
    "        ),\n",
    "        timeless=True,\n",
    "    )\n",
    "\n",
    "def log_pose(\n",
    "    pose: SE3,\n",
    "    label: str,\n",
    "    timeless = False\n",
    ") -> None:\n",
    "    rr.log(\n",
    "        label,\n",
    "        ToTransform3D(pose, False),\n",
    "        timeless = timeless\n",
    "    )\n",
    "\n",
    "def log_image(\n",
    "    image_array : np.array,\n",
    "    label: str,\n",
    "    timeless = False\n",
    ") -> None:\n",
    "    rr.log(label, rr.DisconnectedSpace())\n",
    "    rr.log(label, rr.Image(image_array), timeless = timeless)\n",
    "\n",
    "def log_point_cloud(\n",
    "    point_positions : np.array,\n",
    "    label: str,\n",
    "    timeless: bool = True) -> None:\n",
    "    rr.log(label,rr.Points3D(point_positions, radii=0.001, colors=[200, 200, 200]), timeless=timeless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c54564-eae8-4117-bdc7-54345c620a0e",
   "metadata": {},
   "source": [
    "#### Load one sample take and its corresponding 3D gaze "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060c8b2-7a0d-406e-9b2b-0c409d8b5292",
   "metadata": {},
   "source": [
    "In our tutorial, we will use the following take of bike repair as an example. You need to change `ego_exo_root` to the download directory for the Ego-Exo4D dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08087227-b04c-4be5-8629-47b55af6e247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EgoExo Sequence: /datasets01/egoexo4d/v2/takes/cmu_bike01_5\n"
     ]
    }
   ],
   "source": [
    "ego_exo_root = '/datasets01/egoexo4d/v2/' # Replace with your cli's download directory for Ego-Exo4D\n",
    "take_name = 'cmu_bike01_5'\n",
    "\n",
    "import os\n",
    "ego_exo_project_path = os.path.join(ego_exo_root, 'takes', take_name)\n",
    "print(f'EgoExo Sequence: {ego_exo_project_path}')\n",
    "\n",
    "if not os.path.exists(ego_exo_project_path):\n",
    "    print(\"Please do update your path to a valid EgoExo sequence folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c0d1e-127f-4820-8dde-e694bfcb999e",
   "metadata": {},
   "source": [
    "We retrieve the VRS data including device calibration collected by Aria glasses and plot sensors locations, orientations.\n",
    " - VRS: <a href=\"https://facebookresearch.github.io/vrs/\">VRS</a> is the file format used to store the Project Aria Glasses multimodal data. VRS Data is stored Stream and are identified with a unique StreamId. `VrsDataProvider` enables you to list and retrieve all VRS data and calibration data.\n",
    " - DeviceCalibration: an interface that can be used to retrieve Intrinsics for Image Stream data (i.e Camera data) - `CameraCalibration` and Extrinsics are defined for all sensors - `SE3`.\n",
    "\n",
    "Project Aria glasses use 3D Coordinate Frame Conventions. You can find an overview of these conventions <a href=\"3D Coordinate Frame Conventions\">here</a> where Central Pupil Frame (CPF) and 3D Coordinate frame and system conventions are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e9dbec-b3f9-4122-bcfd-dd241ea0ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRS file path: /datasets01/egoexo4d/v2/takes/cmu_bike01_5/aria01.vrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;000;255m[ProgressLogger][INFO]: 2024-11-19 02:05:39: Opening /datasets01/egoexo4d/v2/takes/cmu_bike01_5/aria01.vrs...\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;128;000m[MultiRecordFileReader][DEBUG]: Opened file '/datasets01/egoexo4d/v2/takes/cmu_bike01_5/aria01.vrs' and assigned to reader #0\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 211-1/camera-et activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 214-1/camera-rgb activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 231-1/mic activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 247-1/baro0 activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[VrsDataProvider][WARNING]: Unsupported TimeSync mode: APP, ignoring.\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: Fail to activate streamId 286-1\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-1/camera-slam-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-2/camera-slam-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-1/imu-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-2/imu-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1203-1/mag0 activated\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from projectaria_tools.core import data_provider\n",
    "from projectaria_tools.utils.rerun_helpers import AriaGlassesOutline, ToTransform3D\n",
    "RERUN_NOTEBOOK_ASSET=\"inline\"\n",
    "###\n",
    "# We are using here the projectaria_tools API for:\n",
    "# - retrieving the DeviceCalibration and the POSE of each sensor\n",
    "# - we are then plotting those POSE onto the Aria glasses outline\n",
    "###\n",
    "\n",
    "##\n",
    "# Retrieve device calibration and plot sensors locations, orientations\n",
    "vrs_file_path = os.path.join(ego_exo_project_path, 'aria01.vrs')\n",
    "print(f\"VRS file path: {vrs_file_path}\")\n",
    "assert os.path.exists(vrs_file_path), \"We are not finding the required vrs file\"\n",
    "\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n",
    "if not vrs_data_provider:\n",
    "    print(\"Couldn't create data vrs_data_provider from vrs file\")\n",
    "    exit(1)\n",
    "\n",
    "device_calibration = vrs_data_provider.get_device_calibration()\n",
    "\n",
    "log_aria_glasses(device_calibration, \"world/device/glasses_outline\")\n",
    "\n",
    "# Plot CPF (Central Pupil Frame coordinate system)\n",
    "T_device_CPF = device_calibration.get_transform_device_cpf()\n",
    "log_pose(T_device_CPF, \"device/CPF_CentralPupilFrame\")\n",
    "\n",
    "# Plot Project Aria Glasses outline (as lines)\n",
    "log_aria_glasses(device_calibration, \"device/glasses_outline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85591d65-6cc0-4e2a-82a0-f8198aa6d58f",
   "metadata": {},
   "source": [
    "We can get RGB and SLAM left and right stream with a given StreamId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372339d0-d4f5-407f-af0a-71cffb62ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "slam_left_stream_id = StreamId(\"1201-1\")\n",
    "slam_right_stream_id = StreamId(\"1201-2\")\n",
    "rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\n",
    "slam_left_stream_label = vrs_data_provider.get_label_from_stream_id(slam_left_stream_id)\n",
    "slam_right_stream_label = vrs_data_provider.get_label_from_stream_id(slam_right_stream_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27895ee3-4a86-4cce-b856-0ab5c1670d2e",
   "metadata": {},
   "source": [
    "We can further retrieve time domain, and image configurations from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6fb146-f135-441f-a186-9b09474303c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamId 214-1, StreamLabel camera-rgb, ImageSize: (1408, 1408)\n"
     ]
    }
   ],
   "source": [
    "# Init rerun api\n",
    "rr.init(\"Aria Data Provider - Retrieve Image Stream data\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Configure option for data retrieval\n",
    "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
    "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
    "\n",
    "# Retrieve Start and End time for the given Sensor Stream Id\n",
    "start_time = vrs_data_provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
    "end_time = vrs_data_provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
    "\n",
    "# FYI, you can retrieve the Image configuration using the following\n",
    "image_config = vrs_data_provider.get_image_configuration(rgb_stream_id)\n",
    "width = image_config.image_width\n",
    "height = image_config.image_height\n",
    "print(f\"StreamId {rgb_stream_id}, StreamLabel {rgb_stream_label}, ImageSize: {width, height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd7404-96c3-4a4a-8de9-428fdeb6e6dc",
   "metadata": {},
   "source": [
    "Now let's visualize the stream from RGB, SLAM left and right using rerun. We sample 10 frames from the whole stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3093351-5434-4dd3-93b8-dd5b89dd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 33.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d4670bd08f453ca2f60e8b65ffe0d3",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_count = 10\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "\n",
    "    # Retrieve the RGB image\n",
    "    image_tuple_rgb = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple_rgb[1].capture_timestamp_ns\n",
    "    \n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    log_image(image_tuple_rgb[0].to_numpy_array(), f\"vrs/{rgb_stream_label}\")\n",
    "\n",
    "    # Retrieving the SLAM images\n",
    "    image_tuple_slam_left = vrs_data_provider.get_image_data_by_time_ns(slam_left_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_left[0].to_numpy_array(), f\"vrs/{slam_left_stream_label}\")\n",
    "\n",
    "    image_tuple_slam_right = vrs_data_provider.get_image_data_by_time_ns(slam_right_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_right[0].to_numpy_array(), f\"vrs/{slam_right_stream_label}\")\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892f8e2-56f8-4589-92dd-d60eed7fb325",
   "metadata": {},
   "source": [
    "Ego Eye Gaze ray at a given timestamp T can be loaded using `mps_data_provider.get_personalized_eyegaze(timestamp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af3ced4-ee95-4376-ad26-5c1e0758a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;000m[MpsDataPathsProvider][WARNING]: Hand tracking folder (/datasets01/egoexo4d/v2/takes/cmu_bike01_5/hand_tracking) does not exist in MPS root folder, not loading wrist and palm poses.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Init rerun api\n",
    "rr.init(\"MPS - Trajectory data\")\n",
    "rec = rr.memory_recording()\n",
    "rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, timeless=True)\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve Trajectory data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "assert mps_data_provider.has_personalized_eyegaze(), \"The sequence does not have Eye Gaze data\"\n",
    "\n",
    "# Log Glasses & calibration linked to the image we want to show\n",
    "log_aria_glasses(device_calibration, \"world/device/glasses_outline\")\n",
    "rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "slam_left_camera_calibration = device_calibration.get_camera_calib(slam_left_stream_label)\n",
    "slam_right_camera_calibration = device_calibration.get_camera_calib(slam_right_stream_label)\n",
    "log_calibration(rgb_camera_calibration, f\"world/device/{rgb_stream_label}\")\n",
    "log_calibration(slam_left_camera_calibration, f\"world/device/{slam_left_stream_label}\")\n",
    "log_calibration(slam_right_camera_calibration, f\"world/device/{slam_right_stream_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d826b5e-c9e2-45e2-9f43-5a06cd43c4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                        | 14/40 [00:00<00:00, 67.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #EyeGazes: 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 72.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748ac0d5011741b49d346f21eae89f16",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "# Eye Gaze data is independent of the Device pose\n",
    "# 1. To display the eye gaze ray, you are applying the right relative transform Device_to_CPF\n",
    "###\n",
    "sample_count = 40\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "    image_tuple = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple[1].capture_timestamp_ns\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "    ##\n",
    "    # Eye Gaze data\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    ##\n",
    "\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    eye_gaze = mps_data_provider.get_personalized_eyegaze(timestamp)\n",
    "\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # Here is how to retrieve the depth of the EyeGaze vector\n",
    "    # depth_m = eye_gaze.depth or 1.0\n",
    "    # But here for display we are using a proxy of 30cm, so you can better see things in context of each other\n",
    "    depth_m = 0.1\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(eye_gaze.yaw, eye_gaze.pitch, depth_m)\n",
    "    gaze_vector_in_cpf = np.nan_to_num(gaze_vector_in_cpf)\n",
    "    # Move EyeGaze vector to CPF coordinate system for visualization and log a 3D ray\n",
    "    rr.log(\n",
    "        \"world/device/eye-gaze\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_device_CPF @ [0, 0, 0]],\n",
    "            vectors=[T_device_CPF @ gaze_vector_in_cpf],\n",
    "            colors=[[255, 0, 255]],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    depth_m = eye_gaze.depth or 1.0\n",
    "    gaze_projection = get_gaze_vector_reprojection(\n",
    "        eye_gaze,\n",
    "        rgb_stream_label,\n",
    "        device_calibration,\n",
    "        rgb_camera_calibration,\n",
    "        depth_m,\n",
    "    )\n",
    "    if gaze_projection is not None:\n",
    "        rr.log(\n",
    "            f\"world/device/{rgb_stream_label}/eye-gaze_projection\",\n",
    "            rr.Points2D(gaze_projection, radii=30, colors=[0,255,0]),\n",
    "        )\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaa0bd-addd-4d4c-8b44-5381da0f03dd",
   "metadata": {},
   "source": [
    "### Projecting eye gaze from 3D to 2D in Egocentric View (CPF frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09297fbd-9227-417c-a2c3-ff7e96c9af0b",
   "metadata": {},
   "source": [
    "We project eye gaze data from egocentric view first. Eye gaze data is represented as a 3D ray with depth (showing the point of user focus). The eye gaze ray starts from the Central Pupil Frame(CPF). The reprojection of eye gaze ray in any Aria Image Stream (RGB, SLAMs) includes several steps:\n",
    " 1. Use the `VrsDataProvider` to retrieve the RGB stream at a given timestamp\n",
    " 2. Use the `MpsDataProvider` to retrieve if an EyeGaze file is available and to retrieve EyeGaze data at a given timestamp\n",
    " 3. Compute the corresponding 3D eye gaze vector and retrieve its depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31c4f47d-a747-46c7-bd6a-5e5a06796d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 62.14it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897ab6d76ded4c42806963b360d88606",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rr.init(\"Eye Gaze - CPF - Image Reprojection\")\n",
    "rec = rr.memory_recording()\n",
    "# Aria coordinate system sets X down, Z in front, Y Left\n",
    "#rr.log(\"device\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n",
    "\n",
    "sample_count = 20\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "\n",
    "    # Retrieving the RGB image\n",
    "    image_tuple_rgb = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple_rgb[1].capture_timestamp_ns\n",
    "    \n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    log_image(image_tuple_rgb[0].to_numpy_array(), f\"device/{rgb_stream_label}\")\n",
    "\n",
    "    ##\n",
    "    # Eye Gaze data\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    ##\n",
    "\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    eye_gaze = mps_data_provider.get_personalized_eyegaze(timestamp)\n",
    "\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # Here is how to retrieve the depth of the EyeGaze vector\n",
    "    # depth_m = eye_gaze.depth or 1.0\n",
    "    # But here for display we are using a proxy of 30cm, so you can better see things in context of each other\n",
    "    depth_m = 0.1\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(\n",
    "        eye_gaze.yaw, eye_gaze.pitch, depth_m\n",
    "    )\n",
    "    gaze_vector_in_cpf = np.nan_to_num(gaze_vector_in_cpf)\n",
    "    # Move EyeGaze vector to CPF coordinate system for visualization\n",
    "    '''\n",
    "    rr.log(\n",
    "        \"device/eye-gaze\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_device_CPF @ [0, 0, 0]],\n",
    "            vectors=[T_device_CPF @ gaze_vector_in_cpf],\n",
    "            colors=[[255, 0, 255]],\n",
    "        ),\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    depth_m = eye_gaze.depth or 1.0\n",
    "\n",
    "    for stream_label in [rgb_stream_label]:\n",
    "        if stream_label is rgb_stream_label:\n",
    "            camera_calibration = rgb_camera_calibration\n",
    "        else:\n",
    "            camera_calibration = None\n",
    "\n",
    "        gaze_projection = get_gaze_vector_reprojection(\n",
    "            eye_gaze,\n",
    "            stream_label,\n",
    "            device_calibration,\n",
    "            camera_calibration,\n",
    "            depth_m,\n",
    "        )\n",
    "        if gaze_projection is not None:\n",
    "            rr.log(\n",
    "                f\"device/{stream_label}/eye-gaze_projection\",\n",
    "                rr.Points2D(gaze_projection, radii=20),\n",
    "            )\n",
    "\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ce9eb-9eee-4456-9ca0-aa947bdd167e",
   "metadata": {},
   "source": [
    "### TODO: Projecting eye gaze from 3D to 2D in multiple Exocentric Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9720f74a-038f-4884-b6ce-c2730e466b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;000m[MpsDataPathsProvider][WARNING]: Hand tracking folder (/datasets01/egoexo4d/v2/takes/cmu_bike01_5/hand_tracking) does not exist in MPS root folder, not loading wrist and palm poses.\u001b[0m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #StaticCameraCalibration data: 4\n",
      "TimeDomain.DEVICE_TIME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████████▊                                                                                                                                                                                                                                | 1/20 [00:00<00:08,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #EyeGazes: 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:09<00:00,  2.18it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9bb87e3534de29809a63da3899d8f",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create black images representing GoPro images\n",
    "\n",
    "import math\n",
    "import torchvision # to read video\n",
    "from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3 # Aria/GoPro Camera Calibration\n",
    "from projectaria_tools.core import mps\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve Trajectory data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Ego_Exo - image reprojection\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "sample_count = 20  # sampling 20 frames from the videos\n",
    "\n",
    "## Loading Exo Static camera calibration data\n",
    "#\n",
    "go_pro_proxy = []\n",
    "static_calibrations = mps.read_static_camera_calibrations(os.path.join(ego_exo_project_path,\"trajectory\",\"gopro_calibs.csv\"))\n",
    "for static_calibration in static_calibrations:\n",
    "    # assert the GoPro was correctly localized\n",
    "    if static_calibration.quality != 1.0:\n",
    "        print(f\"Camera: {static_calibration.camera_uid} was not localized, ignoring this camera.\")\n",
    "        continue\n",
    "    proxy = {}\n",
    "    proxy[\"name\"] = static_calibration.camera_uid\n",
    "    proxy[\"image\"] = zeros = np.zeros((static_calibration.height, static_calibration.width))\n",
    "    proxy[\"images\"] = []\n",
    "    proxy[\"pose\"] = static_calibration.transform_world_cam\n",
    "    proxy[\"camera\"] = CameraCalibration(\n",
    "                            static_calibration.camera_uid,\n",
    "                            KANNALA_BRANDT_K3,\n",
    "                            static_calibration.intrinsics,\n",
    "                            static_calibration.transform_world_cam,\n",
    "                            static_calibration.width,\n",
    "                            static_calibration.height,\n",
    "                            None,\n",
    "                            math.pi,\n",
    "                            \"\")\n",
    "\n",
    "    # Replace proxy image with an image from the gopro video\n",
    "    video_path = os.path.join( ego_exo_project_path ,\"frame_aligned_videos\", static_calibration.camera_uid + \".mp4\")\n",
    "    reader = torchvision.io.VideoReader(video_path, \"video\")\n",
    "    # Grab a frame at the middle of the video\n",
    "    reader_metadata = reader.get_metadata()\n",
    "    sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "    for timestamp in sample_timestamps: \n",
    "        reader.seek(timestamp)\n",
    "        frame = next(reader)['data'][0].numpy()\n",
    "        proxy[\"images\"].append(frame)\n",
    "        \n",
    "#        log_image(proxy[\"image\"], f\"image/{proxy['name']}\")\n",
    "\n",
    "    go_pro_proxy.append(proxy)\n",
    "\n",
    "\n",
    "per_go_pro_reprojection = {}\n",
    "# Sample the camera trajectory and reproject it on the GoPro images\n",
    "\n",
    "\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "\n",
    "    # Retrieving the RGB image\n",
    "    image_tuple_rgb = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple_rgb[1].capture_timestamp_ns\n",
    "    \n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    for go_pro in go_pro_proxy:\n",
    "        \n",
    "        log_image(image, f\"device/{go_pro['name']}\")\n",
    "        \n",
    "    log_image(image_tuple_rgb[0].to_numpy_array(), f\"device/{rgb_stream_label}\")\n",
    "\n",
    "    ##\n",
    "    # Eye Gaze data\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    ##\n",
    "\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    eye_gaze = mps_data_provider.get_personalized_eyegaze(timestamp)\n",
    "\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # Here is how to retrieve the depth of the EyeGaze vector\n",
    "    # depth_m = eye_gaze.depth or 1.0\n",
    "    # But here for display we are using a proxy of 30cm, so you can better see things in context of each other\n",
    "    depth_m = 0.1\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(\n",
    "        eye_gaze.yaw, eye_gaze.pitch, depth_m\n",
    "    )\n",
    "    gaze_vector_in_cpf = np.nan_to_num(gaze_vector_in_cpf)\n",
    "    # Move EyeGaze vector to CPF coordinate system for visualization\n",
    "    '''\n",
    "    rr.log(\n",
    "        \"device/eye-gaze\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_device_CPF @ [0, 0, 0]],\n",
    "            vectors=[T_device_CPF @ gaze_vector_in_cpf],\n",
    "            colors=[[255, 0, 255]],\n",
    "        ),\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    depth_m = eye_gaze.depth or 1.0\n",
    "\n",
    "    for stream_label in [rgb_stream_label]:\n",
    "        if stream_label is rgb_stream_label:\n",
    "            camera_calibration = rgb_camera_calibration\n",
    "        else:\n",
    "            camera_calibration = None\n",
    "\n",
    "        gaze_projection = get_gaze_vector_reprojection(\n",
    "            eye_gaze,\n",
    "            stream_label,\n",
    "            device_calibration,\n",
    "            camera_calibration,\n",
    "            depth_m,\n",
    "        )\n",
    "        if gaze_projection is not None:\n",
    "            rr.log(\n",
    "                f\"device/{stream_label}/eye-gaze_projection\",\n",
    "                rr.Points2D(gaze_projection, radii=20),\n",
    "            )\n",
    "\n",
    "\n",
    "# Showing the rerun window\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0a3ae-5d33-4da6-8b81-aa115f951a1a",
   "metadata": {},
   "source": [
    "### Loading pre-computed 2D eye gaze on the egocentric frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a3b0c-ae63-40b7-846b-a4fb321b2b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
