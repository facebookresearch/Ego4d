{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta, Inc. and its affiliates. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "* Clone this repo: `git clone https://github.com/facebookresearch/Ego4d.git`\n",
    "* Download checkpoint/models binaries, place in this directory: [https://dl.fbaipublicfiles.com/ego4d/moments_cvpr_binaries.tgz](https://dl.fbaipublicfiles.com/ego4d/moments_cvpr_binaries.tgz)\n",
    "* Download annoations via the [CLI](cli.ego4d-data.org): `ego4d --output_directory=\"~/ego4d_data\" --datasets annotations video_540ss --metadata`\n",
    "    * For the miniset (i.e. 1 machine), you can choose to download only the minisets via moments_mini_train_uids.csv/moments_mini_val_uids.csv files:\n",
    "        `ego4d -y --output_directory ~/ego4d_data/ --datasets video_540ss --video_uid_file ~/path/to/moments_mini_val_uids.csv`\n",
    "* conda create environment: `conda env create -n moments_cvpr -â€“file conda-env.yaml`\n",
    "    * Alternatively, using your own environment, install the latest pytorchvideo: `pip install git+https://github.com/facebookresearch/pytorchvideo.git`\n",
    "* Fire up jupyter/vscode/etc here: `jupyter notebook MomentsWorkshop.ipynb`\n",
    "\n",
    "### Questions:\n",
    "* Docs: [docs.ego4d-data.org](docs.ego4d-data.org)\n",
    "* Forum: [discuss.ego4d-data.org](discuss.ego4d-data.org)\n",
    "* Email Me Directly: eugenebyrne@fb.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from iopath.common.file_io import g_pathmgr\n",
    "from pytorchvideo.data.clip_sampling import ClipInfo, ClipSampler\n",
    "\n",
    "# Either use directly from PTV or use the example below\n",
    "# from pytorchvideo.data.ego4d import Ego4dMomentsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs - Update first section as relevant\n",
    "\n",
    "# Repository path where the notebook (and downloaded binaries) are loaded\n",
    "notebook_path = os.path.expanduser(\"~/source/ego4d/notebooks/moments_cvpr/\")\n",
    "# Path to the downloaded ego4d data via the CLI above, local or network\n",
    "data_path = os.path.expanduser(\"~/ego4d_data/\")\n",
    "# Place checkpoints at data_path or otherwise?\n",
    "checkpoint_path = os.path.join(data_path, \"checkpoints/\")\n",
    "\n",
    "# Confirming we've updated paths accordingly\n",
    "assert os.path.isfile(os.path.join(notebook_path, \"MomentsWorkshop.ipynb\")), f\"Inputs paths improperly configured - not found: {notebook_path}\"\n",
    "\n",
    "@dataclass\n",
    "class Ego4dMomentsTrainerParams:\n",
    "    annotation_path_train: str = os.path.join(data_path, \"v1/annotations/moments_train.json\")\n",
    "    annotation_path_val: str = os.path.join(data_path, \"v1/annotations/moments_val.json\")\n",
    "    # Primary Ego4D JSON\n",
    "    metadata_path: str = os.path.join(data_path, \"ego4d.json\")\n",
    "    # Assuming downsampled videos\n",
    "    video_path: str = os.path.join(data_path, \"v1/video_540ss/\")\n",
    "    num_nodes: int = 1\n",
    "    num_gpus_per_node: int = 0\n",
    "    num_workers: int = 0\n",
    "    batch_size: int = 8\n",
    "    use_video: bool = True\n",
    "    use_audio: bool = False\n",
    "    use_imu: bool = False\n",
    "    # window_size: int\n",
    "    # n_frames: int\n",
    "    learning_rate: float = 0.0001\n",
    "    learning_rate_milestones: List[int] = field(default_factory=lambda: [5, 10, 15])\n",
    "    checkpoint_dir: str = checkpoint_path\n",
    "    every_n_train_steps: int = 117\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    # Testing Purposes Only! Set to -1\n",
    "    max_epochs: int = 1\n",
    "    # Testing Purposes Only! Set to -1\n",
    "    max_steps: int = 100\n",
    "    image_model_type: str = \"resnet18\"\n",
    "    accumulate_grad_batches: int = 1\n",
    "    label_mapping_file: Optional[str] = \"moments_label_ids.json\"\n",
    "    # Recommended! Use the video_540ss downsampled videos\n",
    "    downsampled: bool = True\n",
    "    # A miniset (20 train, 10 val) provided for test training\n",
    "    miniset: bool = True\n",
    "\n",
    "inputs = Ego4dMomentsTrainerParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "log: logging.Logger = logging.getLogger(\"Ego4dMoments\")\n",
    "# To see more verbose logging, uncomment below (i.e. print all INFO -> console)\n",
    "# log.setLevel(logging.INFO)\n",
    "# sh = logging.StreamHandler(sys.stdout)\n",
    "# sh.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s \\t[%(filename)s.%(funcName)s:%(lineno)d]\", datefmt=\"%y%m%d %H:%M:%S\"))\n",
    "# log.addHandler(sh)\n",
    "\n",
    "def check_window_len(\n",
    "    s_time: float, e_time: float, w_len: float, video_dur: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Constrain/slide the give time window to `w_len` size and the video/clip length.\n",
    "    \"\"\"\n",
    "    # adjust to match w_len\n",
    "    interval = e_time - s_time\n",
    "    if abs(interval - w_len) > 0.001:\n",
    "        delta = w_len - (e_time - s_time)\n",
    "        s_time = s_time - (delta / 2)\n",
    "        e_time = e_time + (delta / 2)\n",
    "        if s_time < 0:\n",
    "            e_time += -s_time\n",
    "            s_time = 0\n",
    "    if video_dur:\n",
    "        if e_time > video_dur:\n",
    "            overlap = e_time - video_dur\n",
    "            assert s_time >= overlap, \"Incompatible w_len / video_dur\"\n",
    "            s_time -= overlap\n",
    "            e_time -= overlap\n",
    "            log.info(\n",
    "                f\"check_window_len: video overlap ({overlap}) adjusted -> ({s_time:.2f}, {e_time:.2f}) video: {video_dur}\"  # noqa\n",
    "            )\n",
    "    if abs((e_time - s_time) - w_len) > 0.01:\n",
    "        log.error(\n",
    "            f\"check_window_len: invalid time interval: {s_time}, {e_time}\",\n",
    "            stack_info=True,\n",
    "        )\n",
    "    return s_time, e_time\n",
    "\n",
    "class MomentsClipSampler(ClipSampler):\n",
    "    \"\"\"\n",
    "    ClipSampler for Ego4d moments. Will return a fixed `window_sec` window\n",
    "    around the given annotation, shifting where relevant to account for the end\n",
    "    of the clip/video.\n",
    "\n",
    "    clip_start/clip_end is added to the annotation dict to facilitate future lookups.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_sec: float = 0) -> None:\n",
    "        self.window_sec = window_sec\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        last_clip_end_time: float,\n",
    "        video_duration: float,\n",
    "        annotation: Dict[str, Any],\n",
    "    ) -> ClipInfo:\n",
    "        assert (\n",
    "            last_clip_end_time is None or last_clip_end_time <= video_duration\n",
    "        ), f\"last_clip_end_time ({last_clip_end_time}) > video_duration ({video_duration})\"\n",
    "        start = annotation[\"label_video_start_sec\"]\n",
    "        end = annotation[\"label_video_end_sec\"]\n",
    "        if video_duration is not None and end > video_duration:\n",
    "            log.error(f\"Invalid video_duration/end_sec: {video_duration} / {end}\")\n",
    "            # If it's small, proceed anyway\n",
    "            if end > video_duration + 0.1:\n",
    "                raise Exception(\n",
    "                    f\"Invalid video_duration/end_sec: {video_duration} / {end} ({annotation['video_name']})\"  # noqa\n",
    "                )\n",
    "        assert end >= start, f\"end < start: {end:.2f} / {start:.2f}\"\n",
    "        if self.window_sec > 0:\n",
    "            s, e = check_window_len(start, end, self.window_sec, video_duration)\n",
    "            if s != start or e != end:\n",
    "                start = s\n",
    "                end = e\n",
    "        annotation[\"clip_start\"] = start\n",
    "        annotation[\"clip_end\"] = end\n",
    "        return ClipInfo(start, end, 0, 0, True)\n",
    "\n",
    "def get_label_id_map(label_id_map_path: str) -> Dict[str, int]:\n",
    "    label_name_id_map: Dict[str, int]\n",
    "\n",
    "    try:\n",
    "        with g_pathmgr.open(label_id_map_path, \"r\") as f:\n",
    "            label_json = json.load(f)\n",
    "            # Verify?\n",
    "            return label_json\n",
    "    except Exception:\n",
    "        raise FileNotFoundError(f\"{label_id_map_path} must be a valid label id json\")\n",
    "\n",
    "class Ego4dImuDataBase(ABC):\n",
    "    \"\"\"\n",
    "    Base class placeholder for Ego4d IMU data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, basepath: str):\n",
    "        self.basepath = basepath\n",
    "\n",
    "    @abstractmethod\n",
    "    def has_imu(self, video_uid: str) -> bool:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_imu_sample(\n",
    "        self, video_uid: str, video_start: float, video_end: float\n",
    "    ) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "def get_video_uids(path):\n",
    "    with g_pathmgr.open(path, \"r\") as f:\n",
    "        return set([x for x in f.read().split('\\n') if x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabeledVideoDataset (Provided only for context)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type\n",
    "\n",
    "import torch.utils.data\n",
    "from pytorchvideo.data.clip_sampling import ClipSampler\n",
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "\n",
    "from pytorchvideo.data.labeled_video_paths import LabeledVideoPaths\n",
    "from pytorchvideo.data.utils import MultiProcessSampler\n",
    "\n",
    "\n",
    "logger = log\n",
    "\n",
    "\n",
    "class LabeledVideoDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    LabeledVideoDataset handles the storage, loading, decoding and clip sampling for a\n",
    "    video dataset. It assumes each video is stored as either an encoded video\n",
    "    (e.g. mp4, avi) or a frame video (e.g. a folder of jpg, or png)\n",
    "    \"\"\"\n",
    "\n",
    "    _MAX_CONSECUTIVE_FAILURES = 10\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labeled_video_paths: List[Tuple[str, Optional[dict]]],\n",
    "        clip_sampler: ClipSampler,\n",
    "        video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "        transform: Optional[Callable[[dict], Any]] = None,\n",
    "        decode_audio: bool = True,\n",
    "        decode_video: bool = True,\n",
    "        decoder: str = \"pyav\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labeled_video_paths (List[Tuple[str, Optional[dict]]]): List containing\n",
    "                    video file paths and associated labels. If video paths are a folder\n",
    "                    it's interpreted as a frame video, otherwise it must be an encoded\n",
    "                    video.\n",
    "\n",
    "            clip_sampler (ClipSampler): Defines how clips should be sampled from each\n",
    "                video. See the clip sampling documentation for more information.\n",
    "\n",
    "            video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal\n",
    "                video container. This defines the order videos are decoded and,\n",
    "                if necessary, the distributed split.\n",
    "\n",
    "            transform (Callable): This callable is evaluated on the clip output before\n",
    "                the clip is returned. It can be used for user defined preprocessing and\n",
    "                augmentations on the clips. The clip output format is described in __next__().\n",
    "\n",
    "            decode_audio (bool): If True, decode audio from video.\n",
    "\n",
    "            decode_video (bool): If True, decode video frames from a video container.\n",
    "\n",
    "            decoder (str): Defines what type of decoder used to decode a video. Not used for\n",
    "                frame videos.\n",
    "        \"\"\"\n",
    "        self._decode_audio = decode_audio\n",
    "        self._decode_video = decode_video\n",
    "        self._transform = transform\n",
    "        self._clip_sampler = clip_sampler\n",
    "        self._labeled_videos = labeled_video_paths\n",
    "        self._decoder = decoder\n",
    "\n",
    "        # If a RandomSampler is used we need to pass in a custom random generator that\n",
    "        # ensures all PyTorch multiprocess workers have the same random seed.\n",
    "        self._video_random_generator = None\n",
    "        if video_sampler == torch.utils.data.RandomSampler:\n",
    "            self._video_random_generator = torch.Generator()\n",
    "            self._video_sampler = video_sampler(\n",
    "                self._labeled_videos, generator=self._video_random_generator\n",
    "            )\n",
    "        else:\n",
    "            self._video_sampler = video_sampler(self._labeled_videos)\n",
    "\n",
    "        self._video_sampler_iter = None  # Initialized on first call to self.__next__()\n",
    "\n",
    "        # Depending on the clip sampler type, we may want to sample multiple clips\n",
    "        # from one video. In that case, we keep the store video, label and previous sampled\n",
    "        # clip time in these variables.\n",
    "        self._loaded_video_label = None\n",
    "        self._loaded_clip = None\n",
    "        self._last_clip_end_time = None\n",
    "        self.video_path_handler = VideoPathHandler()\n",
    "\n",
    "    @property\n",
    "    def video_sampler(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            The video sampler that defines video sample order. Note that you'll need to\n",
    "            use this property to set the epoch for a torch.utils.data.DistributedSampler.\n",
    "        \"\"\"\n",
    "        return self._video_sampler\n",
    "\n",
    "    @property\n",
    "    def num_videos(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Number of videos in dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_sampler)\n",
    "\n",
    "    def __next__(self) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the next clip based on the clip sampling strategy and video sampler.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the following format.\n",
    "\n",
    "            .. code-block:: text\n",
    "\n",
    "                {\n",
    "                    'video': <video_tensor>,\n",
    "                    'label': <index_label>,\n",
    "                    'video_label': <index_label>\n",
    "                    'video_index': <video_index>,\n",
    "                    'clip_index': <clip_index>,\n",
    "                    'aug_index': <aug_index>,\n",
    "                }\n",
    "        \"\"\"\n",
    "        if not self._video_sampler_iter:\n",
    "            # Setup MultiProcessSampler here - after PyTorch DataLoader workers are spawned.\n",
    "            self._video_sampler_iter = iter(MultiProcessSampler(self._video_sampler))\n",
    "\n",
    "        video_id = None\n",
    "        for i_try in range(self._MAX_CONSECUTIVE_FAILURES):\n",
    "            # Reuse previously stored video if there are still clips to be sampled from\n",
    "            # the last loaded video.\n",
    "            if self._loaded_video_label:\n",
    "                video, info_dict, video_index = self._loaded_video_label\n",
    "            else:\n",
    "                video_index = next(self._video_sampler_iter)\n",
    "                try:\n",
    "                    video_path, info_dict = self._labeled_videos[video_index]\n",
    "                    video_id = info_dict.get(\"video_name\")\n",
    "                    # TODO: Repeatedly called?\n",
    "                    # print(f\"Video: {video_path}\")\n",
    "                    video = self.video_path_handler.video_from_path(\n",
    "                        video_path,\n",
    "                        decode_audio=self._decode_audio,\n",
    "                        # decode_video=self._decode_video,\n",
    "                        decoder=self._decoder,\n",
    "                    )\n",
    "                    self._loaded_video_label = (video, info_dict, video_index)\n",
    "                except Exception as e:\n",
    "                    logger.warn(\n",
    "                        \"Failed to load video with error: {}; trial {}; id: {}\".format(\n",
    "                            e,\n",
    "                            i_try,\n",
    "                            video_id,\n",
    "                        )\n",
    "                    )\n",
    "                    logger.exception(\"Video load exception\")\n",
    "                    continue\n",
    "\n",
    "            video_id = video.name\n",
    "\n",
    "            (\n",
    "                clip_start,\n",
    "                clip_end,\n",
    "                clip_index,\n",
    "                aug_index,\n",
    "                is_last_clip,\n",
    "            ) = self._clip_sampler(self._last_clip_end_time, video.duration, info_dict)\n",
    "\n",
    "            if isinstance(clip_start, list):  # multi-clip in each sample\n",
    "\n",
    "                # Only load the clips once and reuse previously stored clips if there are multiple\n",
    "                # views for augmentations to perform on the same clips.\n",
    "                if aug_index[0] == 0:\n",
    "                    self._loaded_clip = {}\n",
    "                    loaded_clip_list = []\n",
    "                    for i in range(len(clip_start)):\n",
    "                        clip_dict = video.get_clip(clip_start[i], clip_end[i])\n",
    "                        if clip_dict is None or clip_dict[\"video\"] is None:\n",
    "                            self._loaded_clip = None\n",
    "                            break\n",
    "                        loaded_clip_list.append(clip_dict)\n",
    "\n",
    "                    if self._loaded_clip is not None:\n",
    "                        for key in loaded_clip_list[0].keys():\n",
    "                            self._loaded_clip[key] = [x[key] for x in loaded_clip_list]\n",
    "\n",
    "            else:  # single clip case\n",
    "\n",
    "                # Only load the clip once and reuse previously stored clip if there are multiple\n",
    "                # views for augmentations to perform on the same clip.\n",
    "                if aug_index == 0:\n",
    "                    self._loaded_clip = video.get_clip(clip_start, clip_end)\n",
    "\n",
    "            self._last_clip_end_time = clip_end\n",
    "\n",
    "            video_is_null = (\n",
    "                self._loaded_clip is None or self._loaded_clip[\"video\"] is None\n",
    "            )\n",
    "            if (\n",
    "                is_last_clip[-1] if isinstance(is_last_clip, list) else is_last_clip\n",
    "            ) or video_is_null:\n",
    "                # Close the loaded encoded video and reset the last sampled clip time ready\n",
    "                # to sample a new video on the next iteration.\n",
    "                self._loaded_video_label[0].close()\n",
    "                self._loaded_video_label = None\n",
    "                self._last_clip_end_time = None\n",
    "                self._clip_sampler.reset()\n",
    "\n",
    "                # Force garbage collection to release video container immediately\n",
    "                # otherwise memory can spike.\n",
    "                gc.collect()\n",
    "\n",
    "                if video_is_null:\n",
    "                    logger.warn(\n",
    "                        \"Failed to load clip {}; trial {}\".format(video.name, i_try)\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            frames = self._loaded_clip[\"video\"]\n",
    "            audio_samples = self._loaded_clip[\"audio\"]\n",
    "            sample_dict = {\n",
    "                \"video\": frames,\n",
    "                \"video_name\": video.name,\n",
    "                \"video_index\": video_index,\n",
    "                \"clip_index\": clip_index,\n",
    "                \"aug_index\": aug_index,\n",
    "                **info_dict,\n",
    "                **({\"audio\": audio_samples} if audio_samples is not None else {}),\n",
    "            }\n",
    "            if self._transform is not None:\n",
    "                sample_dict = self._transform(sample_dict)\n",
    "\n",
    "                # User can force dataset to continue by returning None in transform.\n",
    "                if sample_dict is None:\n",
    "                    logger.info(\"LVD: Sample Bypass: {video.name}\")\n",
    "                    continue\n",
    "\n",
    "            return sample_dict\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"a Failed to load video after {self._MAX_CONSECUTIVE_FAILURES} retries. id: {video_id}\"  # noqa\n",
    "            )\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._video_sampler_iter = None  # Reset video sampler\n",
    "\n",
    "        # If we're in a PyTorch DataLoader multiprocessing context, we need to use the\n",
    "        # same seed for each worker's RandomSampler generator. The workers at each\n",
    "        # __iter__ call are created from the unique value: worker_info.seed - worker_info.id,\n",
    "        # which we can use for this seed.\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if self._video_random_generator is not None and worker_info is not None:\n",
    "            base_seed = worker_info.seed - worker_info.id\n",
    "            self._video_random_generator.manual_seed(base_seed)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def labeled_video_dataset(\n",
    "    data_path: str,\n",
    "    clip_sampler: ClipSampler,\n",
    "    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,\n",
    "    video_path_prefix: str = \"\",\n",
    "    decode_audio: bool = True,\n",
    "    decoder: str = \"pyav\",\n",
    ") -> LabeledVideoDataset:\n",
    "    \"\"\"\n",
    "    A helper function to create ``LabeledVideoDataset`` object for Ucf101 and Kinetics datasets.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data. The path type defines how the data\n",
    "            should be read:\n",
    "\n",
    "            * For a file path, the file is read and each line is parsed into a\n",
    "              video path and label.\n",
    "            * For a directory, the directory structure defines the classes\n",
    "              (i.e. each subdirectory is a class).\n",
    "\n",
    "        clip_sampler (ClipSampler): Defines how clips should be sampled from each\n",
    "                video. See the clip sampling documentation for more information.\n",
    "\n",
    "        video_sampler (Type[torch.utils.data.Sampler]): Sampler for the internal\n",
    "                video container. This defines the order videos are decoded and,\n",
    "                if necessary, the distributed split.\n",
    "\n",
    "        transform (Callable): This callable is evaluated on the clip output before\n",
    "                the clip is returned. It can be used for user defined preprocessing and\n",
    "                augmentations to the clips. See the ``LabeledVideoDataset`` class for clip\n",
    "                output format.\n",
    "\n",
    "        video_path_prefix (str): Path to root directory with the videos that are\n",
    "                loaded in ``LabeledVideoDataset``. All the video paths before loading\n",
    "                are prefixed with this path.\n",
    "\n",
    "        decode_audio (bool): If True, also decode audio from video.\n",
    "\n",
    "        decoder (str): Defines what type of decoder used to decode a video.\n",
    "\n",
    "    \"\"\"\n",
    "    labeled_video_paths = LabeledVideoPaths.from_path(data_path)\n",
    "    labeled_video_paths.path_prefix = video_path_prefix\n",
    "    dataset = LabeledVideoDataset(\n",
    "        labeled_video_paths,\n",
    "        clip_sampler,\n",
    "        video_sampler,\n",
    "        transform,\n",
    "        decode_audio=decode_audio,\n",
    "        decoder=decoder,\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (Provided for context)\n",
    "\n",
    "import json\n",
    "from bisect import bisect_left\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch.utils.data\n",
    "import torchaudio\n",
    "\n",
    "from iopath.common.file_io import g_pathmgr\n",
    "# from pytorchvideo.data import LabeledVideoDataset\n",
    "from pytorchvideo.data.clip_sampling import ClipSampler\n",
    "# from pytorchvideo.data.ego4d.utils import (\n",
    "#     Ego4dImuDataBase,\n",
    "#     get_label_id_map,\n",
    "#     MomentsClipSampler,\n",
    "# )\n",
    "from pytorchvideo.data.video import VideoPathHandler\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Div255,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    ShortSideScale,\n",
    ")\n",
    "from torchvision.transforms import CenterCrop, Compose, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "class Ego4dMomentsDataset(LabeledVideoDataset):\n",
    "    \"\"\"\n",
    "    Ego4d video/audio/imu dataset for the moments benchmark:\n",
    "    `<https://ego4d-data.org/docs/benchmarks/episodic-memory/>`\n",
    "\n",
    "    This dataset handles the parsing of frames, loading and clip sampling for the\n",
    "    videos.\n",
    "\n",
    "    IO utilizing :code:`iopath.common.file_io.PathManager` to support\n",
    "    non-local storage uri's.\n",
    "    \"\"\"\n",
    "\n",
    "    VIDEO_FPS = 30\n",
    "    AUDIO_FPS = 48000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotation_path: str,\n",
    "        metadata_path: str,\n",
    "        split: Optional[str] = None,\n",
    "        decode_audio: bool = True,\n",
    "        imu: bool = False,\n",
    "        clip_sampler: Optional[ClipSampler] = None,\n",
    "        video_sampler: Type[\n",
    "            torch.utils.data.Sampler\n",
    "        ] = torch.utils.data.SequentialSampler,\n",
    "        transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,\n",
    "        decoder: str = \"pyav\",\n",
    "        filtered_labels: Optional[List[str]] = None,\n",
    "        window_sec: int = 10,\n",
    "        audio_transform_type: str = \"melspectrogram\",\n",
    "        imu_path: str = None,\n",
    "        label_id_map: Optional[Dict[str, int]] = None,\n",
    "        label_id_map_path: Optional[str] = None,\n",
    "        video_path_override: Optional[Callable[[str], str]] = None,\n",
    "        video_path_handler: Optional[VideoPathHandler] = None,\n",
    "        eligible_video_uids: Optional[Set[str]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation_path (str):\n",
    "                Path or URI to Ego4d moments annotations json (ego4d.json). Download via:\n",
    "                `<https://github.com/facebookresearch/Ego4d/blob/main/ego4d/cli/README.md>`\n",
    "\n",
    "            metadata_path (str):\n",
    "                Path or URI to primary Ego4d metadata json (moments.json). Download via:\n",
    "                `<https://github.com/facebookresearch/Ego4d/blob/main/ego4d/cli/README.md>`\n",
    "\n",
    "            split (Optional[str]): train/val/test\n",
    "\n",
    "            decode_audio (bool): If True, decode audio from video.\n",
    "\n",
    "            imu (bool): If True, load IMU data.\n",
    "\n",
    "            clip_sampler (ClipSampler):\n",
    "                A standard PTV ClipSampler. By default, if not specified, `MomentsClipSampler`\n",
    "\n",
    "            video_sampler (VideoSampler):\n",
    "                A standard PTV VideoSampler.\n",
    "\n",
    "            transform (Optional[Callable[[Dict[str, Any]], Any]]):\n",
    "                This callable is evaluated on the clip output before the clip is returned.\n",
    "                It can be used for user-defined preprocessing and augmentations to the clips.\n",
    "\n",
    "                    The clip input is a dictionary with the following format:\n",
    "                        {{\n",
    "                            'video': <video_tensor>,\n",
    "                            'audio': <audio_tensor>,\n",
    "                            'imu': <imu_tensor>,\n",
    "                            'start_time': <float>,\n",
    "                            'stop_time': <float>\n",
    "                        }}\n",
    "\n",
    "                If transform is None, the raw clip output in the above format is\n",
    "                returned unmodified.\n",
    "\n",
    "            decoder (str): Defines what type of decoder used to decode a video within\n",
    "                `LabeledVideoDataset`.\n",
    "\n",
    "            filtered_labels (List[str]):\n",
    "                Optional list of moments labels to filter samples for training.\n",
    "\n",
    "            window_sec (int): minimum window size in s\n",
    "\n",
    "            audio_transform_type: melspectrogram / spectrogram / mfcc\n",
    "\n",
    "            imu_path (Optional[str]):\n",
    "                Path to the ego4d IMU csv file.  Required if imu=True.\n",
    "\n",
    "            label_id_map / label_id_map_path:\n",
    "                A map of moments labels to consistent integer ids.  If specified as a path\n",
    "                we expect a vanilla .json dict[str, int].  Exactly one must be specified.\n",
    "\n",
    "            video_path_override ((str) -> str):\n",
    "                An override for video paths, given the video_uid, to support downsampled/etc\n",
    "                videos.\n",
    "\n",
    "            video_path_handler (VideoPathHandler):\n",
    "                Primarily provided as an override for `CachedVideoPathHandler`\n",
    "\n",
    "        Example Usage:\n",
    "            Ego4dMomentsDataset(\n",
    "                annotation_path=\"~/ego4d_data/v1/annotations/moments.json\",\n",
    "                metadata_path=\"~/ego4d_data/v1/ego4d.json\",\n",
    "                split=\"train\",\n",
    "                decode_audio=True,\n",
    "                imu=False,\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        assert annotation_path\n",
    "        assert metadata_path\n",
    "        assert split in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"test\",\n",
    "        ], f\"Split '{split}' not supported for ego4d\"\n",
    "        self.split: str = split\n",
    "        self.decode_audio = decode_audio\n",
    "        self.training: bool = split == \"train\"\n",
    "        self.window_sec = window_sec\n",
    "        self._transform_source = transform\n",
    "        self.audio_transform_type = audio_transform_type\n",
    "        assert (label_id_map is not None) ^ (\n",
    "            label_id_map_path is not None\n",
    "        ), f\"Either label_id_map or label_id_map_path required ({label_id_map_path} / {label_id_map})\"  # noqa\n",
    "\n",
    "        self.video_means = (0.45, 0.45, 0.45)\n",
    "        self.video_stds = (0.225, 0.225, 0.225)\n",
    "        self.video_crop_size = 224\n",
    "        self.video_min_short_side_scale = 256\n",
    "        self.video_max_short_side_scale = 320\n",
    "\n",
    "        try:\n",
    "            with g_pathmgr.open(metadata_path, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "        except Exception:\n",
    "            raise FileNotFoundError(\n",
    "                f\"{metadata_path} must be a valid metadata json for Ego4D\"\n",
    "            )\n",
    "\n",
    "        self.video_metadata_map: Dict[str, Any] = {\n",
    "            x[\"video_uid\"]: x for x in metadata[\"videos\"]\n",
    "        }\n",
    "\n",
    "        if not g_pathmgr.isfile(annotation_path):\n",
    "            raise FileNotFoundError(f\"{annotation_path} not found.\")\n",
    "\n",
    "        try:\n",
    "            with g_pathmgr.open(annotation_path, \"r\") as f:\n",
    "                moments_annotations = json.load(f)\n",
    "        except Exception:\n",
    "            raise FileNotFoundError(f\"{annotation_path} must be json for Ego4D dataset\")\n",
    "\n",
    "        self.label_name_id_map: Dict[str, int]\n",
    "        if label_id_map:\n",
    "            self.label_name_id_map = label_id_map\n",
    "        else:\n",
    "            self.label_name_id_map = get_label_id_map(label_id_map_path)\n",
    "            assert self.label_name_id_map\n",
    "\n",
    "        self.num_classes: int = len(self.label_name_id_map)\n",
    "        log.info(f\"Label Classes: {self.num_classes}\")\n",
    "\n",
    "        self.imu_data: Optional[Ego4dImuDataBase] = None\n",
    "        if imu:\n",
    "            assert imu_path, \"imu_path not provided\"\n",
    "            self.imu_data = Ego4dImuData(imu_path)\n",
    "\n",
    "        video_uids = set()\n",
    "        clip_uids = set()\n",
    "        clip_video_map = {}\n",
    "        labels = set()\n",
    "        labels_bypassed = set()\n",
    "        cnt_samples_bypassed = 0\n",
    "        cnt_samples_bypassed_labels = 0\n",
    "        samples = []\n",
    "\n",
    "        for vid in moments_annotations[\"videos\"]:\n",
    "            video_uid = vid[\"video_uid\"]\n",
    "            video_uids.add(video_uid)\n",
    "            vsplit = vid[\"split\"]\n",
    "            if split and vsplit != split:\n",
    "                continue\n",
    "            # If IMU, filter videos without IMU\n",
    "            if self.imu_data and not self.imu_data.has_imu(video_uid):\n",
    "                continue\n",
    "            if eligible_video_uids and video_uid not in eligible_video_uids:\n",
    "                continue\n",
    "            for clip in vid[\"clips\"]:\n",
    "                clip_uid = clip[\"clip_uid\"]\n",
    "                clip_uids.add(clip_uid)\n",
    "                clip_video_map[clip_uid] = video_uid\n",
    "                clip_start_sec = clip[\"video_start_sec\"]\n",
    "                clip_end_sec = clip[\"video_end_sec\"]\n",
    "                for vann in clip[\"annotations\"]:\n",
    "                    for lann in vann[\"labels\"]:\n",
    "                        label = lann[\"label\"]\n",
    "                        labels.add(label)\n",
    "                        start = lann[\"start_time\"]\n",
    "                        end = lann[\"end_time\"]\n",
    "                        # remove sample with same timestamp\n",
    "                        if start == end:\n",
    "                            continue\n",
    "                        start_video = lann[\"video_start_time\"]\n",
    "                        end_video = lann[\"video_end_time\"]\n",
    "                        assert end_video >= start_video\n",
    "\n",
    "                        if abs(start_video - (clip_start_sec + start)) > 0.5:\n",
    "                            log.debug(\n",
    "                                f\"Suspect clip/video start mismatch: clip: {clip_start_sec:.2f} + {start:.2f} video: {start_video:.2f}\"  # noqa\n",
    "                            )\n",
    "\n",
    "                        # filter annotation base on the existing label map\n",
    "                        if filtered_labels and label not in filtered_labels:\n",
    "                            cnt_samples_bypassed += 1\n",
    "                            labels_bypassed.add(label)\n",
    "                            continue\n",
    "                        metadata = self.video_metadata_map[video_uid]\n",
    "\n",
    "                        if metadata.get(\"is_stereo\"):\n",
    "                            cnt_samples_bypassed += 1\n",
    "                            continue\n",
    "\n",
    "                        if video_path_override:\n",
    "                            video_path = video_path_override(video_uid)\n",
    "                        else:\n",
    "                            video_path = metadata[\"manifold_path\"]\n",
    "                        if not video_path:\n",
    "                            cnt_samples_bypassed += 1\n",
    "                            log.error(\"Bypassing invalid video_path: {video_uid}\")\n",
    "                            continue\n",
    "\n",
    "                        sample = {\n",
    "                            \"clip_uid\": clip_uid,\n",
    "                            \"video_uid\": video_uid,\n",
    "                            \"duration\": metadata[\"duration_sec\"],\n",
    "                            \"clip_video_start_sec\": clip_start_sec,\n",
    "                            \"clip_video_end_sec\": clip_end_sec,\n",
    "                            \"labels\": [label],\n",
    "                            \"label_video_start_sec\": start_video,\n",
    "                            \"label_video_end_sec\": end_video,\n",
    "                            \"video_path\": video_path,\n",
    "                        }\n",
    "                        assert (\n",
    "                            sample[\"label_video_end_sec\"]\n",
    "                            > sample[\"label_video_start_sec\"]\n",
    "                        )\n",
    "\n",
    "                        if self.label_name_id_map:\n",
    "                            if label in self.label_name_id_map:\n",
    "                                sample[\"labels_id\"] = self.label_name_id_map[label]\n",
    "                            else:\n",
    "                                cnt_samples_bypassed_labels += 1\n",
    "                                continue\n",
    "                        else:\n",
    "                            log.error(\"Missing label_name_id_map\")\n",
    "                        samples.append(sample)\n",
    "\n",
    "        self.cnt_samples: int = len(samples)\n",
    "\n",
    "        log.info(\n",
    "            f\"Loaded {self.cnt_samples} samples. Bypass: {cnt_samples_bypassed} Label Lookup Bypass: {cnt_samples_bypassed_labels}\"  # noqa\n",
    "        )\n",
    "        print(\n",
    "            f\"Loaded {self.cnt_samples} samples. Bypass: {cnt_samples_bypassed} Label Lookup Bypass: {cnt_samples_bypassed_labels}\"  # noqa\n",
    "        )\n",
    "\n",
    "        for sample in samples:\n",
    "            assert \"labels_id\" in sample, f\"init: Sample missing labels_id: {sample}\"\n",
    "\n",
    "        if not clip_sampler:\n",
    "            clip_sampler = MomentsClipSampler(self.window_sec)\n",
    "\n",
    "        super().__init__(\n",
    "            [(x[\"video_path\"], x) for x in samples],\n",
    "            clip_sampler,\n",
    "            video_sampler,\n",
    "            transform=self._transform_mm,\n",
    "            decode_audio=decode_audio,\n",
    "            decoder=decoder,\n",
    "        )\n",
    "\n",
    "        if video_path_handler:\n",
    "            self.video_path_handler = video_path_handler\n",
    "\n",
    "    def check_IMU(self, input_dict: Dict[str, Any]) -> bool:\n",
    "        if (\n",
    "            len(input_dict[\"imu\"][\"signal\"].shape) != 2\n",
    "            or input_dict[\"imu\"][\"signal\"].shape[0] == 0\n",
    "            or input_dict[\"imu\"][\"signal\"].shape[0] < 200\n",
    "            or input_dict[\"imu\"][\"signal\"].shape[1] != 6\n",
    "        ):\n",
    "            log.warning(f\"Problematic Sample: {input_dict}\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _transform_mm(self, sample_dict: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        log.debug(\"_transform_mm\")\n",
    "        with profiler.record_function(\"_transform_mm\"):\n",
    "            video_uid = sample_dict[\"video_uid\"]\n",
    "            assert video_uid\n",
    "\n",
    "            assert sample_dict[\"video\"] is not None\n",
    "            assert (\n",
    "                \"labels_id\" in sample_dict\n",
    "            ), f\"Sample missing labels_id: {sample_dict}\"\n",
    "\n",
    "            video = sample_dict[\"video\"]\n",
    "\n",
    "            expected = int(self.VIDEO_FPS * self.window_sec)\n",
    "            actual = video.size(1)\n",
    "            if expected != actual:\n",
    "                log.error(\n",
    "                    f\"video size mismatch: actual: {actual} expected: {expected} video: {video.size()} uid: {video_uid}\",  # noqa\n",
    "                    stack_info=True,\n",
    "                )\n",
    "                return None\n",
    "\n",
    "            start = sample_dict[\"clip_start\"]\n",
    "            end = sample_dict[\"clip_end\"]\n",
    "            assert start >= 0 and end >= start\n",
    "\n",
    "            if abs((end - start) - self.window_sec) > 0.01:\n",
    "                log.warning(f\"Invalid IMU time window: ({start}, {end})\")\n",
    "\n",
    "            if self.imu_data:\n",
    "                sample_dict[\"imu\"] = self.imu_data.get_imu_sample(\n",
    "                    video_uid,\n",
    "                    start,\n",
    "                    end,\n",
    "                )\n",
    "                if self.check_IMU(sample_dict):\n",
    "                    log.warning(f\"Bad IMU sample: ignoring: {video_uid}\")\n",
    "                    return None\n",
    "\n",
    "            sample_dict = self._video_transform()(sample_dict)\n",
    "\n",
    "            if self.decode_audio:\n",
    "                audio_fps = self.AUDIO_FPS\n",
    "                sample_dict[\"audio\"] = self._preproc_audio(\n",
    "                    sample_dict[\"audio\"], audio_fps\n",
    "                )\n",
    "\n",
    "            labels = sample_dict[\"labels\"]\n",
    "            one_hot = self.convert_one_hot(labels)\n",
    "            sample_dict[\"labels_onehot\"] = one_hot\n",
    "\n",
    "            if self._transform_source:\n",
    "                sample_dict = self._transform_source(sample_dict)\n",
    "\n",
    "            lcnt = sum(one_hot)\n",
    "\n",
    "            log.info(\n",
    "                f\"Sample ({sample_dict['video_name']}): \"\n",
    "                f\"({sample_dict['clip_start']:.2f}, {sample_dict['clip_end']:.2f}) \"\n",
    "                f\" {sample_dict['labels_id']} | {sample_dict['labels']} | {lcnt}\"\n",
    "            )\n",
    "\n",
    "            return sample_dict\n",
    "\n",
    "    # pyre-ignore\n",
    "    def _video_transform(self):\n",
    "        \"\"\"\n",
    "        This function contains example transforms using both PyTorchVideo and\n",
    "        TorchVision in the same callable. For 'train' model, we use augmentations (prepended\n",
    "        with 'Random'), for 'val' we use the respective deterministic function\n",
    "        \"\"\"\n",
    "\n",
    "        assert (\n",
    "            self.video_means\n",
    "            and self.video_stds\n",
    "            and self.video_min_short_side_scale > 0\n",
    "            and self.video_crop_size > 0\n",
    "        )\n",
    "\n",
    "        video_transforms = ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                # pyre-fixme\n",
    "                [Div255(), Normalize(self.video_means, self.video_stds)]\n",
    "                + [  # pyre-fixme\n",
    "                    RandomShortSideScale(\n",
    "                        min_size=self.video_min_short_side_scale,\n",
    "                        max_size=self.video_max_short_side_scale,\n",
    "                    ),\n",
    "                    RandomCrop(self.video_crop_size),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "                if self.training\n",
    "                else [\n",
    "                    ShortSideScale(self.video_min_short_side_scale),\n",
    "                    CenterCrop(self.video_crop_size),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        return Compose([video_transforms])\n",
    "\n",
    "    def signal_transform(self, type: str = \"spectrogram\", sample_rate: int = 48000):\n",
    "        if type == \"spectrogram\":\n",
    "            n_fft = 1024\n",
    "            win_length = None\n",
    "            hop_length = 512\n",
    "\n",
    "            transform = torchaudio.transforms.Spectrogram(\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                center=True,\n",
    "                pad_mode=\"reflect\",\n",
    "                power=2.0,\n",
    "            )\n",
    "        elif type == \"melspectrogram\":\n",
    "            n_fft = 1024\n",
    "            win_length = None\n",
    "            hop_length = 512\n",
    "            n_mels = 64\n",
    "\n",
    "            transform = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                center=True,\n",
    "                pad_mode=\"reflect\",\n",
    "                power=2.0,\n",
    "                norm=\"slaney\",\n",
    "                onesided=True,\n",
    "                n_mels=n_mels,\n",
    "                mel_scale=\"htk\",\n",
    "            )\n",
    "        elif type == \"mfcc\":\n",
    "            n_fft = 2048\n",
    "            win_length = None\n",
    "            hop_length = 512\n",
    "            n_mels = 256\n",
    "            n_mfcc = 256\n",
    "\n",
    "            transform = torchaudio.transforms.MFCC(\n",
    "                sample_rate=sample_rate,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"mel_scale\": \"htk\",\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(type)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def _preproc_audio(self, audio, audio_fps) -> Dict[str, Any]:\n",
    "        # convert stero to mono\n",
    "        # https://github.com/pytorch/audio/issues/363\n",
    "        waveform_mono = torch.mean(audio, dim=0, keepdim=True)\n",
    "        return {\n",
    "            \"signal\": waveform_mono,\n",
    "            \"spectrogram\": self.signal_transform(\n",
    "                type=self.audio_transform_type,\n",
    "                sample_rate=audio_fps,\n",
    "            )(waveform_mono),\n",
    "            \"sampling_rate\": audio_fps,\n",
    "        }\n",
    "\n",
    "    def convert_one_hot(self, label_list: List[str]) -> List[int]:\n",
    "        f_label_list = [x for x in label_list if x in self.label_name_id_map.keys()]\n",
    "        assert len(f_label_list) == len(label_list), f\"invalid filter {len(label_list)} -> {len(f_label_list)}: {label_list}\"\n",
    "        label_list = f_label_list\n",
    "        one_hot = [0 for _ in range(self.num_classes)]\n",
    "        for lab in label_list:\n",
    "            one_hot[self.label_name_id_map[lab]] = 1\n",
    "        assert sum(one_hot) == len(label_list)\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "def create_ego4d_moments_dataset(\n",
    "    annotation_path: str,\n",
    "    metadata_path: str = inputs.metadata_path,\n",
    "    imu_path: str = \"~/ego4d_data/imu/\",  # Not publicly available\n",
    "    label_id_map: Dict[str, int] = None,\n",
    "    split: Optional[str] = None,\n",
    "    decode_audio: bool = True,\n",
    "    imu: bool = False,\n",
    "    eligible_video_uids: Set[str] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    return Ego4dMomentsDataset(\n",
    "        annotation_path=annotation_path,\n",
    "        metadata_path=metadata_path,\n",
    "        imu_path=imu_path,\n",
    "        label_id_map=label_id_map,\n",
    "        split=split,\n",
    "        decode_audio=decode_audio,\n",
    "        imu=imu,\n",
    "        eligible_video_uids=eligible_video_uids,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "def collate_fn_mm_moments(data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    log.info(\"collate_fn_mm_moments\")\n",
    "    input_tensor_IMU = []\n",
    "    input_tensor_VIDEO = []\n",
    "    input_tensor_AUDIO = []\n",
    "    input_tensor_SPECTRO = []\n",
    "    len_list_imu = []\n",
    "    len_list_spectrogram = []\n",
    "    len_list_audio = []\n",
    "    labels = []  # labels as class names\n",
    "    labels_onehot = []  # labels as one hot vectors\n",
    "    for d in data:\n",
    "        if \"video\" in d:\n",
    "            input_tensor_VIDEO.append(d[\"video\"])\n",
    "        if \"spectrogram\" in d:\n",
    "            input_tensor_SPECTRO.append(d[\"spectrogram\"])\n",
    "            len_list_spectrogram.append(d[\"spectrogram\"].size(2))\n",
    "        if \"audio\" in d:\n",
    "            input_tensor_AUDIO.append(d[\"audio\"])\n",
    "            len_list_audio.append(d[\"audio\"].size(1))\n",
    "        if \"imu\" in d:\n",
    "            input_tensor_IMU.append(d[\"imu\"])\n",
    "            len_list_imu.append(d[\"imu\"].size(1))\n",
    "        labels.append(d[\"labels\"])\n",
    "        labels_onehot.append(d[\"labels_onehot\"])\n",
    "\n",
    "    dict_output = {}\n",
    "    dict_output[\"labels\"] = labels\n",
    "    dict_output[\"labels_onehot\"] = torch.tensor(labels_onehot).float()\n",
    "    if input_tensor_IMU:\n",
    "        min_len = min(len_list_imu)\n",
    "        input_tensor_IMU = [t[:, :min_len] for t in input_tensor_IMU]\n",
    "        dict_output[\"imu\"] = torch.stack(input_tensor_IMU)\n",
    "    if input_tensor_AUDIO:\n",
    "        min_len = min(len_list_audio)\n",
    "        input_tensor_AUDIO = [t[:, :min_len] for t in input_tensor_AUDIO]\n",
    "        dict_output[\"audio\"] = torch.stack(input_tensor_AUDIO)\n",
    "    if input_tensor_VIDEO:\n",
    "        dict_output[\"video\"] = torch.stack(input_tensor_VIDEO)\n",
    "    if input_tensor_SPECTRO:\n",
    "        min_len = min(len_list_spectrogram)\n",
    "        input_tensor_SPECTRO = [t[:, :, :min_len] for t in input_tensor_SPECTRO]\n",
    "        dict_output[\"spectrogram\"] = torch.stack(input_tensor_SPECTRO)\n",
    "    return dict_output\n",
    "\n",
    "class Ego4dMomentsDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    LightningDataModule for the Ego4d Moments dataset.  Practically a wrapper around\n",
    "    `Ego4dMomentsDataset`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_id_map: Dict[str, int] = None,\n",
    "        batch_size: int = 8,\n",
    "        num_workers: int = 8,\n",
    "        train_transforms=None,\n",
    "        val_transforms=None,\n",
    "        test_transforms=None,\n",
    "        dims=None,\n",
    "        video_path_override: Optional[Callable[[str], str]] = None,\n",
    "        eligible_video_uids: Set[str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(train_transforms, val_transforms, test_transforms, dims)\n",
    "\n",
    "        self.batch_size: int = batch_size\n",
    "        self.num_workers: int = num_workers\n",
    "        self.video_path_override = video_path_override\n",
    "        self.eligible_video_uids = eligible_video_uids\n",
    "\n",
    "        self.label_name_id_map = label_id_map\n",
    "        assert self.label_name_id_map, \"Failed to load label_name_id_map\"\n",
    "        self.num_classes: int = len(self.label_name_id_map)\n",
    "        assert self.num_classes > 0\n",
    "\n",
    "        log.info(f\"Ego4dMomentsDataModule: num_classes: {self.num_classes}\")\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        dataset_train = create_ego4d_moments_dataset(\n",
    "            annotation_path=inputs.annotation_path_train,\n",
    "            split=\"train\",\n",
    "            decode_audio=False,\n",
    "            imu=False,\n",
    "            label_id_map=self.label_name_id_map,\n",
    "            label_id_map_path=None,\n",
    "            video_path_override=self.video_path_override,\n",
    "            eligible_video_uids=self.eligible_video_uids,\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset_train,\n",
    "            collate_fn=collate_fn_mm_moments,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        dataset_train = create_ego4d_moments_dataset(\n",
    "            annotation_path=inputs.annotation_path_val,\n",
    "            split=\"val\",\n",
    "            decode_audio=False,\n",
    "            imu=False,\n",
    "            label_id_map=self.label_name_id_map,\n",
    "            label_id_map_path=None,\n",
    "            video_path_override=self.video_path_override,\n",
    "            eligible_video_uids=self.eligible_video_uids,\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset_train,\n",
    "            collate_fn=collate_fn_mm_moments,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LFMM Model\n",
    "\n",
    "MODEL_PATH_RESNET = os.path.join(notebook_path, \"resnet18-f37072fd.pth\")\n",
    "MODEL_PATH_R2p1d = os.path.join(notebook_path, \"r2plus1d_18-91a641e6.pth\")\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from iopath.common.file_io import PathManager\n",
    "from torchmetrics import AveragePrecision\n",
    "\n",
    "logger = log\n",
    "\n",
    "\n",
    "def get_image_encoder(image_model_type: str) -> Tuple[nn.Module, int]:\n",
    "    \"\"\"\n",
    "    Helper function to return a given image encoder.  For each, we strip out\n",
    "    the last classification layer, where necessary, so that the final\n",
    "    embedding layer is return.  To that end, we return a tuple of the image\n",
    "    encoder (e.g., `nn.Module`) along with the corresponding embedding dims.\n",
    "    \"\"\"\n",
    "    image_encoder = None\n",
    "    embedding_dims = 0\n",
    "\n",
    "    if image_model_type == \"r2+1d\":\n",
    "        # Loading checkpoint from manifold b/c outside internet connection\n",
    "        # usually not available.\n",
    "        image_encoder = torchvision.models.video.r2plus1d_18(pretrained=False)\n",
    "        with g_pathmgr.open(\n",
    "            MODEL_PATH_R2p1d,\n",
    "            \"rb\",\n",
    "        ) as f:\n",
    "            previous_state = torch.load(f, map_location=lambda storage, loc: storage)\n",
    "            image_encoder.load_state_dict(previous_state)\n",
    "\n",
    "        # Cut out the classifier head to expose the 512-D embeddings\n",
    "        image_encoder.fc = nn.Identity()\n",
    "        embedding_dims = 512\n",
    "    elif image_model_type == \"resnet18\":\n",
    "        # Loading checkpoint from manifold b/c outside internet connection\n",
    "        # usually not available.\n",
    "        image_encoder = torchvision.models.resnet18(pretrained=False)\n",
    "        with g_pathmgr.open(\n",
    "            MODEL_PATH_RESNET,\n",
    "            \"rb\",\n",
    "        ) as f:\n",
    "            previous_state = torch.load(f, map_location=lambda storage, loc: storage)\n",
    "            image_encoder.load_state_dict(previous_state)\n",
    "\n",
    "        # Cut out the classifier head to expose the 512-D embeddings\n",
    "        image_encoder.fc = nn.Identity()\n",
    "        embedding_dims = 512\n",
    "\n",
    "    return (image_encoder, embedding_dims)\n",
    "\n",
    "\n",
    "class LFMM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        image_model_type: str = \"resnet18\",\n",
    "        IMU: bool = False,\n",
    "        VIDEO: bool = True,\n",
    "        AUDIO: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Late-fusion based multimodal model.  Given a sequence of image frames and a\n",
    "        corresponding IMU buffer, take only the last image frame and encode and combine\n",
    "        with the IMU model.  We combine at the logits layer (averaging).\n",
    "\n",
    "        Choices for the image encoder are ARNet (not pre-trained) and\n",
    "        Resnet18 (pre-trained on ImageNet).  Revisit this for better pre-trained\n",
    "        image encoder options.\n",
    "\n",
    "        :TODO: fuse logits or last embedding layers (via concat)?\n",
    "        :TODO: add audio stream\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.IMU = IMU\n",
    "        self.VIDEO = VIDEO\n",
    "        self.AUDIO = AUDIO\n",
    "        assert self.IMU or self.VIDEO or self.AUDIO\n",
    "\n",
    "        self.embedding_dims = 0\n",
    "\n",
    "        # Setup the image encoder\n",
    "        if self.VIDEO:\n",
    "            self.image_encoder, image_embedding_dims = get_image_encoder(\n",
    "                image_model_type\n",
    "            )\n",
    "            self.embedding_dims += image_embedding_dims\n",
    "\n",
    "        if self.IMU:\n",
    "            # Setup the IMU encoder.  Add a 32-D embedding output:\n",
    "            # TODO: self.imu_encoder = MCNN1DAttentionPooling(32, None).net\n",
    "            self.embedding_dims += 32\n",
    "\n",
    "        if self.AUDIO:\n",
    "            # Setup the Audio encoder\n",
    "            # TODO: Replace\n",
    "            # self.audio_encoder = AudioClassifierModifiedVGG(\n",
    "            #     None,\n",
    "            #     num_classes,\n",
    "            #     pretrained_checkpoint=\"manifold://fai4ar_supar/tree/checkpoints/mobilenet_v3_small-047dcff4.pth\",\n",
    "            # ).net\n",
    "\n",
    "            # Cut out the classifier head to expose the 1024-D embeddings\n",
    "            self.audio_encoder.classifier[3] = nn.Identity()\n",
    "            self.embedding_dims += 1024\n",
    "\n",
    "        # Then the final classifier head on top of the concatenated embeddings\n",
    "        self.clf = nn.Linear(self.embedding_dims, num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        imu: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs -\n",
    "            images: [bsz, channels=3, <num_frames?>, height, width] tensor of images\n",
    "                --> num_frames optional.  If not given then its just the last frame of the sequence.\n",
    "            imu: [bsz, channels=6, length]\n",
    "            audio: [bsz, channels=3, height, width] Tensor of spectrograms\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        if self.VIDEO:\n",
    "            # Run the image encoder\n",
    "            embeddings.append(self.image_encoder(images))\n",
    "\n",
    "        if self.IMU:\n",
    "            # Run the IMU encoder\n",
    "            embeddings.append(self.imu_encoder(imu))\n",
    "\n",
    "        if self.AUDIO:\n",
    "            # Run the audio encoder\n",
    "            embeddings.append(self.audio_encoder(audio))\n",
    "\n",
    "        embeddings = torch.cat(embeddings, 1)\n",
    "        logits = self.clf(embeddings)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LFMMModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        lr: float,\n",
    "        lr_milestones: List[int],\n",
    "        image_model_type: str = \"resnet18\",\n",
    "        save_checkpoint_dir: str = None,\n",
    "        IMU: bool = False,\n",
    "        VIDEO: bool = True,\n",
    "        AUDIO: bool = False,\n",
    "        label_id_map: Optional[Dict[str, int]] = None,\n",
    "        label_mapping_file: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.IMU = IMU\n",
    "        self.VIDEO = VIDEO\n",
    "        self.AUDIO = AUDIO\n",
    "        self.num_classes = num_classes\n",
    "        self.image_model_type = image_model_type\n",
    "\n",
    "        self.label_mapping = None\n",
    "        try:\n",
    "            ids = set()\n",
    "            if label_id_map is not None:\n",
    "                self.label_mapping = {}\n",
    "                for class_name, id in label_id_map.items():\n",
    "                    self.label_mapping[id] = class_name\n",
    "                    ids.add(id)\n",
    "                assert len(label_id_map) == num_classes, f\"label_id_map invalid: {len(label_id_map)} != {num_classes}\"\n",
    "            elif label_mapping_file is not None:\n",
    "                with g_pathmgr.open(label_mapping_file, \"r\") as f:\n",
    "                    class_mapping = json.load(f)\n",
    "                self.label_mapping = {}\n",
    "                for class_name, metadata in class_mapping.items():\n",
    "                    self.label_mapping[metadata[\"index\"]] = class_name\n",
    "                    ids.add(metadata[\"index\"])\n",
    "            if not self.label_mapping:\n",
    "                logger.error(\"LFMM: No Label Mapping Provided!\")\n",
    "            else:\n",
    "                max_id = max(ids)\n",
    "                if max_id != len(ids) - 1 or len(ids) != len(self.label_mapping) or num_classes != len(self.label_mapping):\n",
    "                    logger.error(f\"Error: LFMM: Label->Id Inconsistency: Labels: {len(self.label_mapping)} num_classes: {num_classes} Ids: {len(ids)} Max: {max_id} Min: {min(ids)}\")  # noqa\n",
    "                else:\n",
    "                    logger.info(f\"LFMM: Valid Labels: {len(self.label_mapping)} num_classes: {num_classes} Max: {max_id} Min: {min(ids)} ({self.VIDEO}|{self.AUDIO}|{self.IMU})\")\n",
    "        except Exception as exc:\n",
    "            logger.error(\n",
    "                f\"Error opening label mapping file ({label_mapping_file}): {exc}\"\n",
    "            )\n",
    "            self.label_mapping = None\n",
    "            raise Exception(f\"Error opening label mapping file ({label_mapping_file}): {exc}\")\n",
    "\n",
    "        self.model = LFMM(\n",
    "            num_classes,\n",
    "            image_model_type=image_model_type,\n",
    "            IMU=self.IMU,\n",
    "            VIDEO=self.VIDEO,\n",
    "            AUDIO=self.AUDIO,\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_milestones = lr_milestones\n",
    "        self.save_checkpoint_dir = save_checkpoint_dir\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        imu: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        return self.model(images, imu, audio)\n",
    "\n",
    "    def one_step(self, batch, batch_idx):\n",
    "        if self.VIDEO:\n",
    "            # Size: [bsz, 3, num_frames, height, width] - e.g. [12, 3, 20, 112, 112]\n",
    "            if self.image_model_type == \"r2+1d\":\n",
    "                # Use the entire video clip tensor\n",
    "                video = batch[\"video\"]  # [bsz, 3, num_frames, height, width]\n",
    "            else:\n",
    "                # Strip out the last image frames from the video clips\n",
    "                video = batch[\"video\"]\n",
    "                video = video[:, :, -1]  # [bsz, 3, height, width]\n",
    "        else:\n",
    "            video = torch.tensor(0)\n",
    "\n",
    "        if self.IMU:\n",
    "            # Size [bsz, 6, length] - e.g. [12, 6, 966]\n",
    "            imu = batch[\"imu\"]  # [bsz, 6, length]\n",
    "        else:\n",
    "            imu = torch.tensor(0)\n",
    "\n",
    "        if self.AUDIO:\n",
    "            # Size: [bsz, 3, height, width] - e.g. [12, 3, 201, 401]\n",
    "            audio = batch[\"spectrogram\"]  # [bsz, 3, height, width]\n",
    "        else:\n",
    "            audio = torch.tensor(0)\n",
    "\n",
    "        output = self.forward(video, imu, audio)\n",
    "        target = batch[\"labels_onehot\"]\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        video shape: [bsz, 3, num_frames, height, width]\n",
    "        imu shape: [bsz, 6, length]\n",
    "        audio shape: [bsz, 3, num_frames, height, width]\n",
    "        \"\"\"\n",
    "\n",
    "        loss, _ = self.one_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        video shape: [bsz, 3, num_frames, height, width]\n",
    "        imu shape: [bsz, 6, length]\n",
    "        audio shape: [bsz, 3, num_frames, height, width]\n",
    "        \"\"\"\n",
    "\n",
    "        loss, output = self.one_step(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss, \"output\": output, \"label\": batch[\"labels_onehot\"]}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        video shape: [bsz, 3, num_frames, height, width]\n",
    "        imu shape: [bsz, 6, length]\n",
    "        audio shape: [bsz, 3, num_frames, height, width]\n",
    "        \"\"\"\n",
    "\n",
    "        loss, output = self.one_step(batch, batch_idx)\n",
    "        self.log(\"test/loss\", loss, prog_bar=True)\n",
    "        return {\"loss\": loss, \"output\": output, \"label\": batch[\"labels_onehot\"]}\n",
    "\n",
    "    def validation_step_end(self, batch_parts):\n",
    "        \"\"\"\n",
    "        Accumulate the outputs across the devices, for a single mini-batch step.\n",
    "        \"\"\"\n",
    "        losses = batch_parts[\"loss\"]\n",
    "        outputs = batch_parts[\"output\"]\n",
    "        label = batch_parts[\"label\"]\n",
    "\n",
    "        return {\n",
    "            \"loss\": torch.mean(torch.Tensor(losses)),\n",
    "            \"output\": outputs,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "    def test_step_end(self, batch_parts):\n",
    "        return self.validation_step_end(batch_parts)\n",
    "\n",
    "    def evaluation_epoch_end(self, step_outputs, prefix):\n",
    "        \"\"\"\n",
    "        Accumulate the AP and mAP metrics across all samples and devices.\n",
    "        \"\"\"\n",
    "\n",
    "        mean_loss = torch.mean(torch.Tensor([v[\"loss\"] for v in step_outputs]))\n",
    "        self.log(\n",
    "            \"\"\"{prefix}/total_loss\"\"\".format(prefix=prefix),\n",
    "            mean_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        outputs = torch.cat([v[\"output\"] for v in step_outputs], 0)\n",
    "        label = torch.cat([v[\"label\"] for v in step_outputs], 0).long()\n",
    "\n",
    "        outputs = torch.nn.functional.sigmoid(outputs)\n",
    "        average_precision = AveragePrecision(pos_label=1)  # do this per class\n",
    "\n",
    "        per_class_ap = torch.Tensor(\n",
    "            [\n",
    "                average_precision(outputs[:, class_idx], label[:, class_idx])\n",
    "                for class_idx in range(self.num_classes)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nonzeo_per_class_ap = torch.Tensor(\n",
    "            [\n",
    "                average_precision(outputs[:, class_idx], label[:, class_idx])\n",
    "                for class_idx in range(self.num_classes)\n",
    "                if torch.sum(label[:, class_idx]) > 0\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Replace NaN with 0\n",
    "        # per_class_ap[per_class_ap != per_class_ap] = 0\n",
    "\n",
    "        mAP = torch.mean(nonzeo_per_class_ap)\n",
    "        self.log(\n",
    "            \"\"\"{prefix}/mAP\"\"\".format(prefix=prefix),\n",
    "            mAP,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        per_class_ap_map = {}\n",
    "        if self.label_mapping is not None:\n",
    "            # Log out per-class AP metrics\n",
    "            for label_index, ap in enumerate(per_class_ap):\n",
    "                per_class_ap_map[self.label_mapping[label_index]] = ap\n",
    "\n",
    "                self.log(\n",
    "                    \"\"\"{prefix}/AP [{class_name}]\"\"\".format(\n",
    "                        prefix=prefix, class_name=self.label_mapping[label_index]\n",
    "                    ),\n",
    "                    ap,\n",
    "                    prog_bar=False,\n",
    "                )\n",
    "\n",
    "        return {\n",
    "            \"\"\"{prefix}/total_loss\"\"\".format(prefix=prefix): mean_loss,\n",
    "            \"\"\"{prefix}/mAP\"\"\".format(prefix=prefix): mAP,\n",
    "            \"\"\"{prefix}/per_class_ap\"\"\".format(prefix=prefix): per_class_ap_map,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        return self.evaluation_epoch_end(validation_step_outputs, \"val\")\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        final_results = self.evaluation_epoch_end(test_step_outputs, \"test\")\n",
    "\n",
    "        per_class_ap_map = final_results[\"test/per_class_ap\"]\n",
    "\n",
    "        # Save final results to CSV file before returning\n",
    "        out_file = os.path.join(self.save_checkpoint_dir, \"test_per_class_ap.csv\")\n",
    "        with g_pathmgr.open(out_file, \"w\") as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=\",\")\n",
    "\n",
    "            class_names = sorted(per_class_ap_map.keys())\n",
    "            csvwriter.writerow(class_names)\n",
    "\n",
    "            scores = []\n",
    "            for class_name in class_names:\n",
    "                scores.append(per_class_ap_map[class_name].item())\n",
    "            csvwriter.writerow(scores)\n",
    "        logger.info(f\"Saved final per-class AP results to: {out_file}\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=self.lr_milestones, gamma=0.1\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_path_override(x):\n",
    "    path = os.path.join(inputs.video_path, f\"{x}.mp4\")\n",
    "    return path\n",
    "\n",
    "moments_label_id_map = get_label_id_map(inputs.label_mapping_file)\n",
    "eligible_uids = None\n",
    "if inputs.miniset:\n",
    "    train_uids = get_video_uids(\"moments_mini_train_uids.csv\")\n",
    "    val_uids = get_video_uids(\"moments_mini_val_uids.csv\")\n",
    "    eligible_uids = train_uids | val_uids\n",
    "\n",
    "data = Ego4dMomentsDataModule(\n",
    "    moments_label_id_map,\n",
    "    batch_size=inputs.batch_size,\n",
    "    num_workers=inputs.num_workers,\n",
    "    video_path_override=video_path_override,\n",
    "    eligible_video_uids=eligible_uids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = LFMMModule(\n",
    "    num_classes=data.num_classes,\n",
    "    lr=inputs.learning_rate,\n",
    "    lr_milestones=inputs.learning_rate_milestones,\n",
    "    image_model_type=inputs.image_model_type,\n",
    "    save_checkpoint_dir=inputs.checkpoint_dir,\n",
    "    IMU=inputs.use_imu,\n",
    "    VIDEO=inputs.use_video,\n",
    "    AUDIO=inputs.use_audio,\n",
    "    label_id_map=moments_label_id_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning Setup\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=inputs.checkpoint_dir,\n",
    "    # every_n_train_steps=inputs.every_n_train_steps,\n",
    "    verbose=True,\n",
    "    monitor=\"val/mAP\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    num_nodes=inputs.num_nodes,\n",
    "    gpus=inputs.num_gpus_per_node,\n",
    "    # If using multiple GPU/node, you'll want to use DDP (but not in a notebook)\n",
    "    # strategy=pl.strategies.DDPStrategy(find_unused_parameters=False),\n",
    "    # logger=pl_logger,\n",
    "    callbacks=[checkpoint],\n",
    "    max_epochs=inputs.max_epochs,\n",
    "    max_steps=inputs.max_steps,\n",
    "    resume_from_checkpoint=inputs.resume_from_checkpoint,\n",
    "    accumulate_grad_batches=inputs.accumulate_grad_batches,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | LFMM | 11.2 M\n",
      "-------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.860    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 19it [19:15, 60.80s/it, loss=0.545, v_num=1]\n",
      "Loaded 275 samples. Bypass: 0 Label Lookup Bypass: 61\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  6.53s/it]Val Net: out: -10287.2421875 label: 16\n",
      "Loaded 891 samples. Bypass: 0 Label Lookup Bypass: 173                     \n",
      "Epoch 0: : 81it [51:39, 50.00s/it, loss=0.088, v_num=1] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugenebyrne/opt/anaconda3/envs/pty/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/mAP')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('val/mAP', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 0, global step 100: 'val/mAP' was not in top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 81it [51:39, 50.00s/it, loss=0.088, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275 samples. Bypass: 0 Label Lookup Bypass: 61\n",
      "Validation DataLoader 0: : 16it [04:08, 15.55s/it]\n",
      "Validation: 35it [09:34,  9.51s/it]Val Net: out: -700311.0 label: 275\n",
      "Validation: 35it [09:34, 16.42s/it]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "                      Validate metric                                               DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      " val/AP [\"cut_/_chop_/_slice_a_vegetable,_fruit,_or_meat\"]                      0.06919191777706146\n",
      "val/AP [\"put_on_safety_equipment_(e.g._gloves,_helmet,_safet                            nan\n",
      "                        y_goggles)\"]\n",
      "   val/AP [arrange_/_organize_clothes_in_closet/dresser]                       0.0036496350076049566\n",
      "        val/AP [arrange_/_organize_items_in_fridge]                                     nan\n",
      "          val/AP [arrange_/_organize_other_items]                               0.021315567195415497\n",
      "val/AP [browse_through_groceries_or_food_items_on_rack_/_she                    0.008678412064909935\n",
      "                            lf]\n",
      "          val/AP [clean_/_sweep_floor_with_broom]                                       nan\n",
      "      val/AP [clean_/_wipe_a_table_or_kitchen_counter]                          0.02984566241502762\n",
      "          val/AP [clean_/_wipe_kitchen_appliance]                               0.01630164310336113\n",
      "       val/AP [clean_/_wipe_other_surface_or_object]                            0.042458780109882355\n",
      "        val/AP [collect_/_rake_dry_leaves_on_ground]                                    nan\n",
      "         val/AP [converse_/_interact_with_someone]                              0.21875594556331635\n",
      "         val/AP [cut_/_trim_grass_with_a_lawnmower]                                     nan\n",
      "         val/AP [cut_/_trim_grass_with_other_tools]                                     nan\n",
      "                     val/AP [cut_dough]                                         0.09049328416585922\n",
      "      val/AP [cut_open_a_package_(e.g._with_scissors)]                                  nan\n",
      "             val/AP [cut_other_item_using_tool]                                         nan\n",
      "val/AP [cut_thread_/_paper_/_cardboard_using_scissors_/_knif                            nan\n",
      "                        e_/_cutter]\n",
      "                  val/AP [cut_tree_branch]                                              nan\n",
      "   val/AP [dig_or_till_the_soil_with_a_hoe_or_other_tool]                       0.05821036547422409\n",
      "      val/AP [drill_into_wall_/_wood_/_floor_/_metal]                                   nan\n",
      "                  val/AP [drink_beverage]                                               nan\n",
      "                  val/AP [drive_a_vehicle]                                      0.023238131776452065\n",
      "                    val/AP [eat_a_snack]                                                nan\n",
      "            val/AP [enter_a_supermarket_/_shop]                                 0.012589821591973305\n",
      "             val/AP [exit_a_supermarket_/_shop]                                 0.008472247049212456\n",
      "    val/AP [fill_a_pot_/_bottle_/_container_with_water]                         0.01665305346250534\n",
      "                  val/AP [fix_other_item]                                       0.003703703638166189\n",
      "                    val/AP [fix_wiring]                                                 nan\n",
      "               val/AP [fold_clothes_/_sheets]                                   0.008928571827709675\n",
      "        val/AP [hang_clothes_in_closet_/_on_hangers]                                    nan\n",
      "                val/AP [hang_clothes_to_dry]                                            nan\n",
      "val/AP [harvest_vegetables_/_fruits_/_crops_from_plants_on_t                            nan\n",
      "                         he_ground]\n",
      "  val/AP [harvest_vegetables_/_fruits_/_crops_from_trees]                               nan\n",
      "        val/AP [interact_or_play_with_pet_/_animal]                             0.014925372786819935\n",
      "              val/AP [iron_clothes_or_sheets]                                           nan\n",
      "          val/AP [knead_/_shape_/_roll-out_dough]                               0.19066734611988068\n",
      "            val/AP [load/unload_the_dishwasher]                                         nan\n",
      "     val/AP [load_/_unload_a_washing_machine_or_dryer]                                  nan\n",
      "          val/AP [operate_a_dough_mixing_machine]                                       nan\n",
      "   val/AP [pack_food_items_/_groceries_into_bags_/_boxes]                       0.026255950331687927\n",
      "  val/AP [pack_soil_into_the_ground_or_a_pot_/_container]                               nan\n",
      "         val/AP [paint_using_paint_brush_/_roller]                              0.011792302131652832\n",
      "              val/AP [pay_at_billing_counter]                                   0.005235602147877216\n",
      "             val/AP [peel_a_fruit_or_vegetable]                                 0.010526316240429878\n",
      "           val/AP [place_items_in_shopping_cart]                                0.029847580939531326\n",
      "    val/AP [plant_seeds_/_plants_/_flowers_into_ground]                                 nan\n",
      "                 val/AP [play_a_video_game]                                             nan\n",
      "val/AP [put_away_(or_take_out)_dishes_/_utensils_in_storage]                    0.02652333863079548\n",
      "  val/AP [put_away_(or_take_out)_food_items_in_the_fridge]                              nan\n",
      "   val/AP [put_away_(or_take_out)_ingredients_in_storage]                               nan\n",
      "          val/AP [put_food_into_the_oven_to_bake]                                       nan\n",
      "    val/AP [read_a_book_/_magazine_/_shopping_list_etc.]                        0.022886833176016808\n",
      "             val/AP [remove_food_from_the_oven]                                         nan\n",
      "             val/AP [remove_weeds_from_ground]                                          nan\n",
      "              val/AP [serve_food_onto_a_plate]                                 0.0074136704206466675\n",
      "           val/AP [stir_/_mix_food_while_cooking]                               0.06203007698059082\n",
      "val/AP [stir_/_mix_ingredients_in_a_bowl_or_pan_(before_cook                    0.02373933047056198\n",
      "                           ing)]\n",
      "             val/AP [taste_food_while_cooking]                                          nan\n",
      "     val/AP [throw_away_trash_/_put_trash_in_trash_can]                         0.018948446959257126\n",
      "       val/AP [tie_up_branches_/_plants_with_string]                                    nan\n",
      "              val/AP [trim_hedges_or_branches]                                          nan\n",
      "         val/AP [turn-on_/_light_the_stove_burner]                               0.4682539701461792\n",
      "              val/AP [use_a_laptop_/_computer]                                          nan\n",
      "           val/AP [use_a_vacuum_cleaner_to_clean]                                       nan\n",
      "                     val/AP [use_phone]                                          0.0538051538169384\n",
      "val/AP [use_the_microwave_/_warm_something_in_the_microwave]                    0.01123595517128706\n",
      "         val/AP [walk_down_stairs_/_walk_up_stairs]                             0.01264280267059803\n",
      "      val/AP [wash_dishes_/_utensils_/_bakeware_etc.]                           0.02852940931916237\n",
      "                    val/AP [wash_hands]                                         0.08985372632741928\n",
      "        val/AP [wash_vegetable_/_fruit_/_food_item]                                     nan\n",
      "                 val/AP [watch_television]                                       0.0920586958527565\n",
      "            val/AP [water_soil_/_plants_/_crops]                                0.017617180943489075\n",
      "  val/AP [weigh_food_/_ingredient_using_a_weighing_scale]                               nan\n",
      "           val/AP [write_notes_in_a_paper_/_book]                                       nan\n",
      "                          val/loss                                               0.5302677750587463\n",
      "                          val/mAP                                               0.05019664019346237\n",
      "                       val/total_loss                                            0.5298373103141785\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugenebyrne/opt/anaconda3/envs/pty/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/eugenebyrne/opt/anaconda3/envs/pty/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val/loss': 0.5302677750587463,\n",
       "  'val/total_loss': 0.5298373103141785,\n",
       "  'val/mAP': 0.05019664019346237,\n",
       "  'val/AP [serve_food_onto_a_plate]': 0.0074136704206466675,\n",
       "  'val/AP [converse_/_interact_with_someone]': 0.21875594556331635,\n",
       "  'val/AP [use_phone]': 0.0538051538169384,\n",
       "  'val/AP [clean_/_wipe_a_table_or_kitchen_counter]': 0.02984566241502762,\n",
       "  'val/AP [plant_seeds_/_plants_/_flowers_into_ground]': nan,\n",
       "  'val/AP [tie_up_branches_/_plants_with_string]': nan,\n",
       "  'val/AP [cut_tree_branch]': nan,\n",
       "  'val/AP [harvest_vegetables_/_fruits_/_crops_from_trees]': nan,\n",
       "  'val/AP [remove_weeds_from_ground]': nan,\n",
       "  'val/AP [cut_other_item_using_tool]': nan,\n",
       "  'val/AP [throw_away_trash_/_put_trash_in_trash_can]': 0.018948446959257126,\n",
       "  'val/AP [water_soil_/_plants_/_crops]': 0.017617180943489075,\n",
       "  'val/AP [wash_hands]': 0.08985372632741928,\n",
       "  'val/AP [turn-on_/_light_the_stove_burner]': 0.4682539701461792,\n",
       "  'val/AP [trim_hedges_or_branches]': nan,\n",
       "  'val/AP [harvest_vegetables_/_fruits_/_crops_from_plants_on_the_ground]': nan,\n",
       "  'val/AP [put_away_(or_take_out)_food_items_in_the_fridge]': nan,\n",
       "  'val/AP [put_food_into_the_oven_to_bake]': nan,\n",
       "  'val/AP [remove_food_from_the_oven]': nan,\n",
       "  'val/AP [load/unload_the_dishwasher]': nan,\n",
       "  'val/AP [wash_vegetable_/_fruit_/_food_item]': nan,\n",
       "  'val/AP [stir_/_mix_food_while_cooking]': 0.06203007698059082,\n",
       "  'val/AP [use_the_microwave_/_warm_something_in_the_microwave]': 0.01123595517128706,\n",
       "  'val/AP [fill_a_pot_/_bottle_/_container_with_water]': 0.01665305346250534,\n",
       "  'val/AP [\"cut_/_chop_/_slice_a_vegetable,_fruit,_or_meat\"]': 0.06919191777706146,\n",
       "  'val/AP [taste_food_while_cooking]': nan,\n",
       "  'val/AP [clean_/_wipe_other_surface_or_object]': 0.042458780109882355,\n",
       "  'val/AP [knead_/_shape_/_roll-out_dough]': 0.19066734611988068,\n",
       "  'val/AP [put_away_(or_take_out)_dishes_/_utensils_in_storage]': 0.02652333863079548,\n",
       "  'val/AP [put_away_(or_take_out)_ingredients_in_storage]': nan,\n",
       "  'val/AP [browse_through_groceries_or_food_items_on_rack_/_shelf]': 0.008678412064909935,\n",
       "  'val/AP [peel_a_fruit_or_vegetable]': 0.010526316240429878,\n",
       "  'val/AP [cut_dough]': 0.09049328416585922,\n",
       "  'val/AP [cut_open_a_package_(e.g._with_scissors)]': nan,\n",
       "  'val/AP [fold_clothes_/_sheets]': 0.008928571827709675,\n",
       "  'val/AP [iron_clothes_or_sheets]': nan,\n",
       "  'val/AP [drink_beverage]': nan,\n",
       "  'val/AP [load_/_unload_a_washing_machine_or_dryer]': nan,\n",
       "  'val/AP [arrange_/_organize_other_items]': 0.021315567195415497,\n",
       "  'val/AP [arrange_/_organize_clothes_in_closet/dresser]': 0.0036496350076049566,\n",
       "  'val/AP [drill_into_wall_/_wood_/_floor_/_metal]': nan,\n",
       "  'val/AP [hang_clothes_in_closet_/_on_hangers]': nan,\n",
       "  'val/AP [clean_/_sweep_floor_with_broom]': nan,\n",
       "  'val/AP [use_a_laptop_/_computer]': nan,\n",
       "  'val/AP [walk_down_stairs_/_walk_up_stairs]': 0.01264280267059803,\n",
       "  'val/AP [read_a_book_/_magazine_/_shopping_list_etc.]': 0.022886833176016808,\n",
       "  'val/AP [watch_television]': 0.0920586958527565,\n",
       "  'val/AP [wash_dishes_/_utensils_/_bakeware_etc.]': 0.02852940931916237,\n",
       "  'val/AP [\"put_on_safety_equipment_(e.g._gloves,_helmet,_safety_goggles)\"]': nan,\n",
       "  'val/AP [paint_using_paint_brush_/_roller]': 0.011792302131652832,\n",
       "  'val/AP [cut_thread_/_paper_/_cardboard_using_scissors_/_knife_/_cutter]': nan,\n",
       "  'val/AP [drive_a_vehicle]': 0.023238131776452065,\n",
       "  'val/AP [eat_a_snack]': nan,\n",
       "  'val/AP [fix_wiring]': nan,\n",
       "  'val/AP [fix_other_item]': 0.003703703638166189,\n",
       "  'val/AP [operate_a_dough_mixing_machine]': nan,\n",
       "  'val/AP [use_a_vacuum_cleaner_to_clean]': nan,\n",
       "  'val/AP [cut_/_trim_grass_with_other_tools]': nan,\n",
       "  'val/AP [cut_/_trim_grass_with_a_lawnmower]': nan,\n",
       "  'val/AP [pack_food_items_/_groceries_into_bags_/_boxes]': 0.026255950331687927,\n",
       "  'val/AP [arrange_/_organize_items_in_fridge]': nan,\n",
       "  'val/AP [clean_/_wipe_kitchen_appliance]': 0.01630164310336113,\n",
       "  'val/AP [stir_/_mix_ingredients_in_a_bowl_or_pan_(before_cooking)]': 0.02373933047056198,\n",
       "  'val/AP [hang_clothes_to_dry]': nan,\n",
       "  'val/AP [dig_or_till_the_soil_with_a_hoe_or_other_tool]': 0.05821036547422409,\n",
       "  'val/AP [exit_a_supermarket_/_shop]': 0.008472247049212456,\n",
       "  'val/AP [write_notes_in_a_paper_/_book]': nan,\n",
       "  'val/AP [enter_a_supermarket_/_shop]': 0.012589821591973305,\n",
       "  'val/AP [interact_or_play_with_pet_/_animal]': 0.014925372786819935,\n",
       "  'val/AP [collect_/_rake_dry_leaves_on_ground]': nan,\n",
       "  'val/AP [weigh_food_/_ingredient_using_a_weighing_scale]': nan,\n",
       "  'val/AP [play_a_video_game]': nan,\n",
       "  'val/AP [pack_soil_into_the_ground_or_a_pot_/_container]': nan,\n",
       "  'val/AP [pay_at_billing_counter]': 0.005235602147877216,\n",
       "  'val/AP [place_items_in_shopping_cart]': 0.029847580939531326}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.validate(model, data, None, verbose=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.04237818717956543\n"
     ]
    }
   ],
   "source": [
    "print(f\"mAP: {metrics[0]['val/mAP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI download for val downsampled videos only: \n",
    "# !python -m ego4d.cli.cli -y --output_directory ~/ego4d_data/ --datasets video_540ss --video_uid_file ~/path/to/moments_mini_val_uids.csv\n",
    "\n",
    "# Using the checkpoint from the binaries file\n",
    "model = LFMMModule.load_from_checkpoint(\"moments.ckpt\", label_mapping_file=None, label_id_map=moments_label_id_map)\n",
    "\n",
    "metrics = trainer.validate(model, data, None, verbose=True)\n",
    "print(f\"mAP: {metrics[0]['val/mAP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moments Benchmark Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/EGO4D/episodic-memory/blob/main/MQ/README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pty')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c574544ce5cc1d9f9623ab1947dc715541b83a6da903e9852c06cbd8d44db24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
